#    IPSUR: Introduction to Probability and Statistics\\ Using R
#    Copyright (C) 2014 G. Jay Kerns
#
#    This program is free software: you can redistribute it and/or modify
#    it under the terms of the GNU General Public License as published by
#    the Free Software Foundation, either version 3 of the License, or
#    (at your option) any later version.
#
#    This program is distributed in the hope that it will be useful,
#    but WITHOUT ANY WARRANTY; without even the implied warranty of
#    MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
#    GNU General Public License for more details.
#
#    You should have received a copy of the GNU General Public License
#    along with this program.  If not, see <http://www.gnu.org/licenses/>.

#+TITLE:     Introduction to Probability and Statistics Using R
#+AUTHOR:    G. Jay Kerns
#+EMAIL:     gkerns@ysu.edu
#+LANGUAGE:  en
#+OPTIONS: ':nil *:t -:t ::t <:t H:5 \n:nil ^:{} arch:headline
#+OPTIONS: author:t c:nil creator:comment d:nil date:t e:t email:nil
#+OPTIONS: f:nil inline:t num:t p:nil pri:nil stat:t tags:nil
#+OPTIONS: tasks:t tex:t timestamp:t toc:nil todo:t |:t
#+SELECT_TAGS: 
#+PROPERTY: session *R*
#+PROPERTY: exports results
#+PROPERTY: results value raw
#+PROPERTY: cache no
#+LaTeX_CLASS: scrbook
#+LaTeX_CLASS_OPTIONS: [captions=tableheading]
#+LaTeX_CLASS_OPTIONS: [10pt,english,twoside]
#+LaTeX_HEADER: \input{include/preamble}
#+LATEX: \input{include/frontmatter}
#+LATEX: \input{include/preface-second}
#+CREATOR: Emacs 24.3.1 (Org mode 8.0.7)

* An Introduction to Probability and Statistics                     :introps:
:PROPERTIES:
:tangle: R/01-introps.R
:END:
#+LaTeX: \pagenumbering{arabic}

#+LaTeX: \noindent 
This chapter has proved to be the hardest to write, by far. The
trouble is that there is so much to say -- and so many people have
already said it so much better than I could. When I get something I
like I will release it here.

In the meantime, there is a lot of information already available to a
person with an Internet connection. I recommend to start at Wikipedia,
which is not a flawless resource but it has the main ideas with links
to reputable sources.

In my lectures I usually tell stories about Fisher, Galton, Gauss,
Laplace, Quetelet, and the Chevalier de Mere.

** Probability

The common folklore is that probability has been around for millennia
but did not gain the attention of mathematicians until approximately
1654 when the Chevalier de Mere had a question regarding the fair
division of a game's payoff to the two players, supposing the game had
to end prematurely.

** Statistics

Statistics concerns data; their collection, analysis, and
interpretation. In this book we distinguish between two types of
statistics: descriptive and inferential.

Descriptive statistics concerns the summarization of data. We have a
data set and we would like to describe the data set in multiple
ways. Usually this entails calculating numbers from the data, called
descriptive measures, such as percentages, sums, averages, and so
forth.

Inferential statistics does more. There is an inference associated
with the data set, a conclusion drawn about the population from which
the data originated.

I would like to mention that there are two schools of thought of
statistics: frequentist and bayesian. The difference between the
schools is related to how the two groups interpret the underlying
probability (see Section [[#sec-Interpreting-Probabilities]]). The frequentist
school gained a lot of ground among statisticians due in large part to
the work of Fisher, Neyman, and Pearson in the early twentieth
century. That dominance lasted until inexpensive computing power
became widely available; nowadays the bayesian school is garnering
more attention and at an increasing rate.

This book is devoted mostly to the frequentist viewpoint because that
is how I was trained, with the conspicuous exception of Sections
[[#sec-Bayes-Rule]] and [[#sec-Conditional-Distributions]]. I plan to add more
bayesian material in later editions of this book.

#+LaTeX: \newpage{}

** Exercises
#+LaTeX: \setcounter{thm}{0}

#+INCLUDE: "include/prelim.R" src R

* Data Description                                                 :datadesc:
:PROPERTIES:
:tangle: R/03-datadesc.R
:CUSTOM_ID: cha-Describing-Data-Distributions
:END:

#+BEGIN_SRC R :exports none :eval never
#    IPSUR: Introduction to Probability and Statistics Using R
#    Copyright (C) 2014  G. Jay Kerns
#
#    Chapter: Data Description
#
#    This file is part of IPSUR.
#
#    IPSUR is free software: you can redistribute it and/or modify
#    it under the terms of the GNU General Public License as published by
#    the Free Software Foundation, either version 3 of the License, or
#    (at your option) any later version.
#
#    IPSUR is distributed in the hope that it will be useful,
#    but WITHOUT ANY WARRANTY; without even the implied warranty of
#    MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
#    GNU General Public License for more details.
#
#    You should have received a copy of the GNU General Public License
#    along with IPSUR.  If not, see <http://www.gnu.org/licenses/>.
#+END_SRC

#+BEGIN_SRC R :exports none :eval no-export
# This chapter's package dependencies
library(aplpack)
library(qcc)
library(e1071)
library(lattice)
library(ggplot2)
#+END_SRC

#+LaTeX: \noindent 
In this chapter we introduce the different types of data that a
statistician is likely to encounter, and in each subsection we give
some examples of how to display the data of that particular type. Once
we see how to display data distributions, we next introduce the basic
properties of data distributions. We qualitatively explore several
data sets. Once that we have intuitive properties of data sets, we
next discuss how we may numerically measure and describe those
properties with descriptive statistics.

 *What do I want them to know?*

- different data types, such as quantitative versus qualitative,
  nominal versus ordinal, and discrete versus continuous
- basic graphical displays for assorted data types, and some of their
  (dis)advantages
- fundamental properties of data distributions, including center,
  spread, shape, and crazy observations
- methods to describe data (visually/numerically) with respect to the
  properties, and how the methods differ depending on the data type
- all of the above in the context of grouped data, and in particular,
  the concept of a factor

** Types of Data
:PROPERTIES:
:CUSTOM_ID: sec-Types-of-Data
:END: 

Loosely speaking, a datum is any piece of collected information, and a
data set is a collection of data related to each other in some way. We
will categorize data into five types and describe each in turn:

- Quantitative :: data associated with a measurement of some quantity
                  on an observational unit,
- Qualitative :: data associated with some quality or property of an
                 observational unit,
- Logical :: data which represent true or false and play an important
             role later,
- Missing :: data which should be there but are not, and
- Other types :: everything else under the sun.

In each subsection we look at some examples of the type in question
and introduce methods to display them.

*** Quantitative data
:PROPERTIES:
:CUSTOM_ID: sub-Quantitative-Data
:END:

Quantitative data are any data that measure or are associated with a
measurement of the quantity of something. They invariably assume
numerical values. Quantitative data can be further subdivided into two
categories.
- /Discrete data/ take values in a finite or countably infinite set of
  numbers, that is, all possible values could (at least in principle)
  be written down in an ordered list. Examples include: counts, number
  of arrivals, or number of successes. They are often represented by
  integers, say, 0, 1, 2, /etc/.
- /Continuous data/ take values in an interval of numbers. These are
  also known as scale data, interval data, or measurement
  data. Examples include: height, weight, length, time,
  /etc/. Continuous data are often characterized by fractions or
  decimals: 3.82, 7.0001, 4 \(\frac{5}{8}\), /etc/.

Note that the distinction between discrete and continuous data is not
always clear-cut. Sometimes it is convenient to treat data as if they
were continuous, even though strictly speaking they are not
continuous. See the examples.

#+ATTR_LATEX: :options [\textbf{Annual Precipitation in US Cities}]
# +BEGIN_exampletoo
The vector =precip= @@latex:\index{Data sets!precip@\texttt{precip}}@@
contains average amount of rainfall (in inches) for each of 70 cities
in the United States and Puerto Rico. Let us take a look at the data:

#+BEGIN_SRC R :exports both :results output pp  
str(precip)
#+END_SRC

#+RESULTS:
:  Named num [1:70] 67 54.7 7 48.5 14 17.2 20.7 13 43.4 40.2 ...
:  - attr(*, "names")= chr [1:70] "Mobile" "Juneau" "Phoenix" "Little Rock" ...

#+BEGIN_SRC R :exports both :results output pp  
precip[1:4]
#+END_SRC

#+RESULTS:
:      Mobile      Juneau     Phoenix Little Rock 
:        67.0        54.7         7.0        48.5

The output shows that =precip= is a numeric vector which has been
/named/, that is, each value has a name associated with it (which can
be set with the =names= @@latex:\index{names@\texttt{names}}@@ function). These
are quantitative continuous data.
# +END_exampletoo

#+ATTR_LATEX: :options [\textbf{Lengths of Major North American Rivers}]
# +BEGIN_exampletoo
The U.S. Geological Survey recorded the lengths (in miles) of several
rivers in North America. They are stored in the vector =rivers=
@@latex:\index{Data sets!rivers@\texttt{rivers}}@@ in the =datasets=
package \cite{datasets} (which ships with base \(\mathsf{R}\)). See
=?rivers=. Let us take a look at the data with the =str=
@@latex:\index{str@\texttt{str}}@@ function.

#+BEGIN_SRC R :exports both :results output pp  
str(rivers)
#+END_SRC

#+RESULTS:
:  num [1:141] 735 320 325 392 524 ...

The output says that =rivers= is a numeric vector of length 141, and
the first few values are 735, 320, 325, /etc/. These data are
definitely quantitative and it appears that the measurements have been
rounded to the nearest mile. Thus, strictly speaking, these are
discrete data. But we will find it convenient later to take data like
these to be continuous for some of our statistical procedures.

# +END_exampletoo

#+ATTR_LATEX: :options [\textbf{Yearly Numbers of Important Discoveries}]
# +BEGIN_exampletoo
The vector =discoveries= @@latex:\index{Data
sets!discoveries@\texttt{discoveries}}@@ contains numbers of "great"
inventions/discoveries in each year from 1860 to 1959, as reported by
the 1975 World Almanac. Let us take a look at the data:

#+BEGIN_SRC R :exports both :results output pp  
str(discoveries)
#+END_SRC

#+RESULTS:
:  Time-Series [1:100] from 1860 to 1959: 5 3 0 2 0 3 2 3 6 1 ...

# +END_exampletoo

The output is telling us that =discoveries= is a /time series/ (see
Section [[#sub-other-data-types]] for more) of length 100. The entries are
integers, and since they represent counts this is a good example of
discrete quantitative data. We will take a closer look in the
following sections.

*** Displaying Quantitative Data
:PROPERTIES:
:CUSTOM_ID: sub-Displaying-Quantitative-Data
:END:

One of the first things to do when confronted by quantitative data (or
any data, for that matter) is to make some sort of visual display to
gain some insight into the data's structure. There are almost as many
display types from which to choose as there are data sets to plot. We
describe some of the more popular alternatives.

**** Strip charts @@latex:\index{strip chart}@@ (also known as Dot plots) @@latex:\index{dot plot| see\{strip chart\}}@@
:PROPERTIES:
:CUSTOM_ID: par-Strip-charts
:END:

These can be used for discrete or continuous data, and usually look
best when the data set is not too large. Along the horizontal axis is
a numerical scale above which the data values are plotted. We can do
it in \(\mathsf{R}\) with a call to the =stripchart=
@@latex:\index{stripchart@\texttt{stripchart}}@@ function. There are three
available methods.
- overplot :: plots ties covering each other. This method is good to
              display only the distinct values assumed by the data
              set.
- jitter :: adds some noise to the data in the \(y\) direction in
            which case the data values are not covered up by ties.
- stack :: plots repeated values stacked on top of one another. This
           method is best used for discrete data with a lot of ties;
           if there are no repeats then this method is identical to
           overplot.

See Figure [[fig-stripcharts]], which was produced by the following code.

#+BEGIN_SRC R :exports code :eval never
stripchart(precip, xlab="rainfall")
stripchart(rivers, method="jitter", xlab="length")
stripchart(discoveries, method="stack", xlab="number")
#+END_SRC

The leftmost graph is a strip chart of the =precip= data. The graph
shows tightly clustered values in the middle with some others falling
balanced on either side, with perhaps slightly more falling to the
left. Later we will call this a symmetric distribution, see Section
[[#sub-Shape]]. The middle graph is of the =rivers= data, a vector of
length 141. There are several repeated values in the rivers data, and
if we were to use the overplot method we would lose some of them in
the display. This plot shows a what we will later call a right-skewed
shape with perhaps some extreme values on the far right of the
display. The third graph strip charts =discoveries= data which are
literally a textbook example of a right skewed distribution.

#+NAME: stripcharts
#+BEGIN_SRC R :exports results :results graphics :file fig/datadesc-stripcharts.ps
par(mfrow = c(3,1)) # 3 plots: 3 rows, 1 column
stripchart(precip, xlab="rainfall", cex.lab = cexlab)
stripchart(rivers, method="jitter", xlab="length", cex.lab = cexlab)
stripchart(discoveries, method="stack", xlab="number", ylim = c(0,3), cex.lab = cexlab)
par(mfrow = c(1,1)) # back to normal
#+END_SRC

#+NAME: fig-stripcharts
#+CAPTION[Strip charts of =precip=, =rivers=, and =discoveries=]: \small Three stripcharts of three data sets.  The first graph uses the =overplot= method, the second the =jitter= method, and the third the =stack= method.
#+ATTR_LaTeX: :width 0.9\textwidth :placement [ht!]
#+RESULTS: stripcharts
[[file:fig/datadesc-stripcharts.ps]]

The =DOTplot= @@latex:\index{DOTplot@\texttt{DOTplot}}@@ function in the
=UsingR= @@latex:\index{R packages!UsingR@\texttt{UsingR}}@@ package
\cite{UsingR} is another alternative.

**** Histogram @@latex:\index{Histogram}@@

These are typically used for continuous data. A histogram is
constructed by first deciding on a set of classes, or bins, which
partition the real line into a set of boxes into which the data values
fall. Then vertical bars are drawn over the bins with height
proportional to the number of observations that fell into the bin.

These are one of the most common summary displays, and they are often
misidentified as "Bar Graphs" (see below.) The scale on the \(y\)
axis can be frequency, percentage, or density (relative
frequency). The term histogram was coined by Karl Pearson in 1891, see
\cite{Miller}.

#+ATTR_LATEX: :options [\textbf{Annual Precipitation in US Cities}]
# +BEGIN_exampletoo
<<exa-annual>> We are going to take another look at the =precip=
@@latex:\index{Data sets!precip@\texttt{precip}}@@ data that we
investigated earlier. The strip chart in Figure [[fig-stripcharts]]
suggested a loosely balanced distribution; let us now look to see what
a histogram says.

There are many ways to plot histograms in \(\mathsf{R}\), and one of
the easiest is with the =hist= @@latex:\index{hist@\texttt{hist}}@@
function. The following code produces the plots in Figure
[[fig-histograms]].

#+BEGIN_SRC R :exports code :eval never
hist(precip, main = "")
hist(precip, freq = FALSE, main = "")
#+END_SRC

Notice the argument \(\mathtt{main = ""}\) which suppresses the main
title from being displayed -- it would have said "Histogram of
=precip=" otherwise. The plot on the left is a frequency histogram
(the default), and the plot on the right is a relative frequency
histogram (=freq = FALSE=).

#+NAME: histograms
#+BEGIN_SRC R :exports results :results graphics :file fig/datadesc-histograms.ps
par(mfrow = c(1,2))
hist(precip, main = "", cex.lab = cexlab)
hist(precip, freq = FALSE, main = "", cex.lab = cexlab)
par(mfrow = c(1,1))
#+END_SRC

#+NAME: fig-histograms
#+CAPTION[(Relative) frequency histograms of the =precip= data]: \small (Relative) frequency histograms of the =precip= data.
#+ATTR_LaTeX: :width 0.9\textwidth :placement [ht!]
#+RESULTS: histograms
[[file:fig/datadesc-histograms.ps]]

# +END_exampletoo

Please mind the biggest weakness of histograms: the graph obtained
strongly depends on the bins chosen. Choose another set of bins, and
you will get a different histogram. Moreover, there are not any
definitive criteria by which bins should be defined; the best choice
for a given data set is the one which illuminates the data set's
underlying structure (if any). Luckily for us there are algorithms to
automatically choose bins that are likely to display well, and more
often than not the default bins do a good job. This is not always the
case, however, and a responsible statistician will investigate many
bin choices to test the stability of the display.

Recall that the strip chart in Figure [[fig-stripcharts]]
suggested a relatively balanced shape to the =precip= data
distribution. Watch what happens when we change the bins slightly
(with the =breaks= argument to =hist=). See Figure [[fig-histograms-bins]]
which was produced by the following code.

#+NAME: histograms-bins
#+BEGIN_SRC R :exports results :results graphics silent :file fig/datadesc-histograms-bins.ps
par(mfrow = c(1,3))
hist(precip, breaks = 10, main = "", cex.lab = cexlab)
hist(precip, breaks = 25, main = "", cex.lab = cexlab)
hist(precip, breaks = 50, main = "", cex.lab = cexlab)
par(mfrow = c(1,1))
#+END_SRC

#+NAME: fig-histograms-bins
#+CAPTION[More histograms of the =precip= data]: \small More histograms of the =precip= data.
#+ATTR_LaTeX: :width 0.9\textwidth :placement [ht!]
#+RESULTS: histograms-bins
[[file:fig/datadesc-histograms-bins.ps]]

The leftmost graph (with =breaks = 10=) shows that the distribution is
not balanced at all. There are two humps: a big one in the middle and
a smaller one to the left. Graphs like this often indicate some
underlying group structure to the data; we could now investigate
whether the cities for which rainfall was measured were similar in
some way, with respect to geographic region, for example.

The rightmost graph in Figure [[fig-histograms-bins]] shows what happens when
the number of bins is too large: the histogram is too grainy and hides
the rounded appearance of the earlier histograms. If we were to
continue increasing the number of bins we would eventually get all
observed bins to have exactly one element, which is nothing more than
a glorified strip chart.

**** Stem-and-leaf displays (more to be said in Section [[#sec-Exploratory-Data-Analysis]])
Stem-and-leaf displays (also known as stemplots) have two basic parts:
/stems/ and /leaves/. The final digit of the data values is taken to
be a /leaf/, and the leading digit(s) is (are) taken to be /stems/. We
draw a vertical line, and to the left of the line we list the
stems. To the right of the line, we list the leaves beside their
corresponding stem. There will typically be several leaves for each
stem, in which case the leaves accumulate to the right. It is
sometimes necessary to round the data values, especially for larger
data sets.

#+ATTR_LATEX: :options [\textbf{Driver Deaths in the United Kingdom}]
# +BEGIN_exampletoo
<<exa-ukdriverdeaths-first>> =UKDriverDeaths= @@latex:\index{Data
sets!UKDriverDeaths@\texttt{UKDriverDeaths}}@@ is a time series that
contains the total car drivers killed or seriously injured in Great
Britain monthly from Jan 1969 to Dec 1984. See
=?UKDriverDeaths=. Compulsory seat belt use was introduced on January
31, 1983. We construct a stem and leaf diagram in \(\mathsf{R}\) with
the =stem.leaf= @@latex:\index{stem.leaf@\texttt{stem.leaf}}@@
function from the =aplpack= @@latex:\index{R packages@\textsf{R}
packages!aplpack@\texttt{aplpack}}@@ package\cite{aplpack}.

#+BEGIN_SRC R :exports both :results output pp  
stem.leaf(UKDriverDeaths, depth = FALSE)
#+END_SRC

#+RESULTS:
#+begin_example
1 | 2: represents 120
 leaf unit: 10
            n: 192
   10 | 57
   11 | 136678
   12 | 123889
   13 | 0255666888899
   14 | 00001222344444555556667788889
   15 | 0000111112222223444455555566677779
   16 | 01222333444445555555678888889
   17 | 11233344566667799
   18 | 00011235568
   19 | 01234455667799
   20 | 0000113557788899
   21 | 145599
   22 | 013467
   23 | 9
   24 | 7
HI: 2654
#+END_example

The display shows a more or less balanced mound-shaped distribution,
with one or maybe two humps, a big one and a smaller one just to its
right. Note that the data have been rounded to the tens place so that
each datum gets only one leaf to the right of the dividing line.

Notice that the \texttt{depth}s @@latex:\index{depths}@@ have been
suppressed. To learn more about this option and many others, see
Section [[#sec-Exploratory-Data-Analysis]]. Unlike a histogram, the original
data values may be recovered from the stem-and-leaf display -- modulo
the rounding -- that is, starting from the top and working down we can
read off the data values 1050, 1070, 1110, 1130, and so forth.

# +END_exampletoo

**** Index plots

Done with the =plot= @@latex:\index{plot@\texttt{plot}}@@ function. These are
good for plotting data which are ordered, for example, when the data
are measured over time. That is, the first observation was measured at
time 1, the second at time 2, /etc/. It is a two dimensional plot, in
which the index (or time) is the \(x\) variable and the measured value
is the \(y\) variable. There are several plotting methods for index
plots, and we mention two of them:

- spikes :: draws a vertical line from the \(x\)-axis to the observation height.
- points :: plots a simple point at the observation height.

*Level of Lake Huron 1875-1972.* Brockwell and Davis
\cite{Brockwell1991} give the annual measurements of the level (in
feet) of Lake Huron from 1875--1972. The data are stored in the time
series =LakeHuron=. @@latex:\index{Data
sets!LakeHuron@\texttt{LakeHuron}}@@ See =?LakeHuron=. Figure
[[fig-indpl-lakehuron]] was produced with the following code:

Here is how to do it with base \(\mathsf{R}\).

#+BEGIN_SRC R :exports code :eval never
plot(LakeHuron)
plot(LakeHuron, type = "p")
plot(LakeHuron, type = "h")
#+END_SRC

The plots show an overall decreasing trend to the observations, and
there appears to be some seasonal variation that increases over time.

#+NAME: indpl-lakehuron
#+BEGIN_SRC R :exports results :results graphics silent :file fig/datadesc-indpl-lakehuron.ps
par(mfrow = c(3,1))
plot(LakeHuron, cex.lab = cexlab)
plot(LakeHuron, type = "p", cex.lab = cexlab)
plot(LakeHuron, type = "h", cex.lab = cexlab)
par(mfrow = c(1,1))
#+END_SRC

#+NAME: fig-indpl-lakehuron
#+CAPTION[Index plots of the =LakeHuron= data]: \small Index plots of the =LakeHuron= data.
#+ATTR_LaTeX: :width 0.9\textwidth :placement [ht!]
#+RESULTS: indpl-lakehuron
[[file:fig/datadesc-indpl-lakehuron.ps]]

**** Density estimates						       :TODO:

The default method uses a Gaussian kernel density estimate.

#+BEGIN_SRC R :eval never
# The Old Faithful geyser data
d <- density(faithful$eruptions, bw = "sj")
d
plot(d)
hist(precip, freq = FALSE)
lines(density(precip))
#+END_SRC

*** Qualitative Data, Categorical Data, and Factors
:PROPERTIES:
:CUSTOM_ID: sub-Qualitative-Data
:END:

Qualitative data are simply any type of data that are not numerical,
or do not represent numerical quantities. Examples of qualitative
variables include a subject's name, gender, race/ethnicity, political
party, socioeconomic status, class rank, driver's license number, and
social security number (SSN).

Please bear in mind that some data /look/ to be quantitative but are
/not/, because they do not represent numerical quantities and do not
obey mathematical rules. For example, a person's shoe size is
typically written with numbers: 8, or 9, or 12, or
\(12\,\frac{1}{2}\). Shoe size is not quantitative, however, because
if we take a size 8 and combine with a size 9 we do not get a size 17.

Some qualitative data serve merely to /identify/ the observation (such
a subject's name, driver's license number, or SSN). This type of data
does not usually play much of a role in statistics. But other
qualitative variables serve to /subdivide/ the data set into
categories; we call these /factors/. In the above examples, gender,
race, political party, and socioeconomic status would be considered
factors (shoe size would be another one). The possible values of a
factor are called its /levels/. For instance, the factor /gender/
would have two levels, namely, male and female. Socioeconomic status
typically has three levels: high, middle, and low.

Factors may be of two types: /nominal/ @@latex:\index{nominal data}@@ and
/ordinal/ @@latex:\index{ordinal data}@@. Nominal factors have levels that
correspond to names of the categories, with no implied
ordering. Examples of nominal factors would be hair color, gender,
race, or political party. There is no natural ordering to "Democrat"
and "Republican"; the categories are just names associated with
different groups of people.

In contrast, ordinal factors have some sort of ordered structure to
the underlying factor levels. For instance, socioeconomic status would
be an ordinal categorical variable because the levels correspond to
ranks associated with income, education, and occupation. Another
example of ordinal categorical data would be class rank.

Factors have special status in \(\mathsf{R}\). They are represented
internally by numbers, but even when they are written numerically
their values do not convey any numeric meaning or obey any
mathematical rules (that is, Stage III cancer is not Stage I cancer +
Stage II cancer).

# +BEGIN_exampletoo
The =state.abb= @@latex:\index{Data sets!state.abb@\texttt{state.abb}}@@ vector
gives the two letter postal abbreviations for all 50 states.

#+BEGIN_SRC R :exports both :results output pp  
str(state.abb)
#+END_SRC

#+RESULTS:
:  chr [1:50] "AL" "AK" "AZ" "AR" "CA" "CO" "CT" "DE" "FL" "GA" "HI" ...

These would be ID data. The =state.name= @@latex:\index{Data
sets!state.name@\texttt{state.name}}@@ vector lists all of the complete
names and those data would also be ID.
# +END_exampletoo


#+ATTR_LATEX: :options [\textbf{U.S. State Facts and Features}]
# +BEGIN_exampletoo
The U.S. Department of Commerce of the U.S. Census Bureau releases all
sorts of information in the /Statistical Abstract of the United
States/, and the =state.region= @@latex:\index{Data
sets!state.region@\texttt{state.region}}@@ data lists each of the 50
states and the region to which it belongs, be it Northeast, South,
North Central, or West. See =?state.region=.

#+BEGIN_SRC R :exports both :results output pp  
str(state.region)
state.region[1:5]
#+END_SRC

#+RESULTS:
:  Factor w/ 4 levels "Northeast","South",..: 2 4 4 2 4 4 1 2 2 2 ...
: [1] South West  West  South West 
: Levels: Northeast South North Central West

The =str= @@latex:\index{str@\texttt{str}}@@ output shows that =state.region= is
already stored internally as a factor and it lists a couple of the
factor levels. To see all of the levels we printed the first five
entries of the vector in the second line.
# +END_exampletoo

*** Displaying Qualitative Data
:PROPERTIES:
:CUSTOM_ID: sub-Displaying-Qualitative-Data
:END:

**** Tables
:PROPERTIES:
:CUSTOM_ID: par-Tables
:END:

One of the best ways to summarize qualitative data is with a table of
the data values. We may count frequencies with the =table= function or
list proportions with the =prop.table=
@@latex:\index{prop.table@\texttt{prop.table}}@@ function (whose input is a
frequency table). In the \(\mathsf{R}\) Commander you can do it with
=Statistics= \(\triangleright\) =Frequency Distribution...=
Alternatively, to look at tables for all factors in the =Active data
set= @@latex:\index{Active data set@\texttt{Active data set}}@@ you can do
=Statistics= \(\triangleright\) =Summaries= \(\triangleright\) =Active
Dataset=.

#+BEGIN_SRC R :exports code :results silent 
Tbl <- table(state.division)
#+END_SRC

#+BEGIN_SRC R :exports both :results output pp  
Tbl
#+END_SRC

#+RESULTS:
: state.division
:        New England    Middle Atlantic     South Atlantic East South Central 
:                  6                  3                  8                  4 
: West South Central East North Central West North Central           Mountain 
:                  4                  5                  7                  8 
:            Pacific 
:                  5

#+BEGIN_SRC R :exports both :results output pp  
Tbl/sum(Tbl)      # relative frequencies
#+END_SRC

#+RESULTS:
: state.division
:        New England    Middle Atlantic     South Atlantic East South Central 
:               0.12               0.06               0.16               0.08 
: West South Central East North Central West North Central           Mountain 
:               0.08               0.10               0.14               0.16 
:            Pacific 
:               0.10

#+BEGIN_SRC R :exports both :results output pp  
prop.table(Tbl)   # same thing
#+END_SRC

#+RESULTS:
: state.division
:        New England    Middle Atlantic     South Atlantic East South Central 
:               0.12               0.06               0.16               0.08 
: West South Central East North Central West North Central           Mountain 
:               0.08               0.10               0.14               0.16 
:            Pacific 
:               0.10

**** Bar Graphs
:PROPERTIES:
:CUSTOM_ID: par-Bar-Graphs
:END:

A bar graph is the analogue of a histogram for categorical data. A bar
is displayed for each level of a factor, with the heights of the bars
proportional to the frequencies of observations falling in the
respective categories. A disadvantage of bar graphs is that the levels
are ordered alphabetically (by default), which may sometimes obscure
patterns in the display.

#+ATTR_LATEX: :options [\textbf{U.S. State Facts and Features}]
# +BEGIN_exampletoo
The =state.region= data lists each of the 50 states and the region to
which it belongs, be it Northeast, South, North Central, or West. See
=?state.region=. It is already stored internally as a factor. We make
a bar graph with the =barplot=
@@latex:\index{barplot@\texttt{barplot}}@@ function:

#+BEGIN_SRC R :exports code :eval never
barplot(table(state.region), cex.names = 1.20)
barplot(prop.table(table(state.region)), cex.names = 1.20)
#+END_SRC

See Figure [[fig-bar-gr-stateregion]]. The display on the left is a frequency
bar graph because the \(y\) axis shows counts, while the display on
the left is a relative frequency bar graph. The only difference
between the two is the scale. Looking at the graph we see that the
majority of the fifty states are in the South, followed by West, North
Central, and finally Northeast. Over 30% of the states are in the
South.

Notice the =cex.names= @@latex:\index{cex.names@\texttt{cex.names}}@@
argument that we used, above. It expands the names on the \(x\) axis
by 20% which makes them easier to read. See =?par=
@@latex:\index{par@\texttt{par}}@@ for a detailed list of additional
plot parameters.

#+NAME: bar-gr-stateregion
#+BEGIN_SRC R :exports results :results graphics silent :file fig/datadesc-bar-gr-stateregion.ps
par(mfrow = c(2,1)) # 2 plots: 2 rows, 1 column
barplot(table(state.region), cex.names = 1.2)
barplot(prop.table(table(state.region)), cex.names = 1.2)
par(mfrow = c(1,1)) # back to normal
#+END_SRC

#+NAME: fig-bar-gr-stateregion
#+CAPTION[Bar graphs of the =state.region= data]: \small The top graph is a frequency barplot made with =table= and the bottom is a relative frequency barplot made with =prop.table=.
#+ATTR_LaTeX: :width 1.0\textwidth :placement [ht!]
#+RESULTS: bar-gr-stateregion
[[file:fig/datadesc-bar-gr-stateregion.ps]]

# +END_exampletoo

**** Pareto Diagrams
:PROPERTIES:
:CUSTOM_ID: par-Pareto-Diagrams
:END:

A pareto diagram is a lot like a bar graph except the bars are
rearranged such that they decrease in height going from left to
right. The rearrangement is handy because it can visually reveal
structure (if any) in how fast the bars decrease -- this is much more
difficult when the bars are jumbled.

#+ATTR_LATEX: :options [\textbf{U.S. State Facts and Features}]
# +BEGIN_exampletoo
The =state.division= @@latex:\index{Data
sets!state.division@\texttt{state.division}}@@ data record the
division (New England, Middle Atlantic, South Atlantic, East South
Central, West South Central, East North Central, West North Central,
Mountain, and Pacific) of the fifty states. We can make a pareto
diagram with either the =RcmdrPlugin.IPSUR= @@latex:\index{R
packages@\textsf{R}
packages!RcmdrPlugin.IPSUR@\texttt{RcmdrPlugin.IPSUR}}@@ package
\cite{RcmdrPlugin.IPSUR} or with the =pareto.chart=
@@latex:\index{pareto.chart@\texttt{pareto.chart}}@@ function from the
=qcc= @@latex:\index{R packages@\textsf{R}
packages!qcc@\texttt{qcc}}@@ package \cite{qcc}. See Figure
[[fig-Pareto-chart]]. The code follows.

#+NAME: Pareto-chart
#+BEGIN_SRC R :exports both :results graphics silent :file fig/datadesc-Pareto-chart.ps
pareto.chart(table(state.division), ylab="Frequency", cex.lab = cexlab)
#+END_SRC

#+NAME: fig-Pareto-chart
#+CAPTION[Pareto chart of the =state.division= data]: \small Pareto chart of the =state.division= data.
#+ATTR_LaTeX: :width 0.9\textwidth :placement [ht!]
#+RESULTS: Pareto-chart
[[file:fig/datadesc-Pareto-chart.ps]]

# +END_exampletoo

**** Dot Charts
:PROPERTIES:
:CUSTOM_ID: par-Dotcharts
:END:

These are a lot like a bar graph that has been turned on its side with
the bars replaced by dots on horizontal lines. They do not convey any
more (or less) information than the associated bar graph, but the
strength lies in the economy of the display. Dot charts are so compact
that it is easy to graph very complicated multi-variable interactions
together in one graph. See Section [[#sec-Comparing-Data-Sets]]. We will give
an example here using the same data as above for comparison. The graph
was produced by the following code.

# +BEGIN_exampletoo

#+NAME: dot-charts
#+BEGIN_SRC R :exports both :results graphics :file fig/datadesc-dot-charts.ps
x <- table(state.region)
dotchart(as.vector(x), labels = names(x), cex.lab = cexlab)
#+END_SRC

#+NAME: fig-dot-charts
#+CAPTION[Dot chart of the \texttt{state.region} data]: \small Dot chart of the \texttt{state.region} data.
#+ATTR_LaTeX: :width 0.9\textwidth :placement [ht!]
#+RESULTS: dot-charts
[[file:fig/datadesc-dot-charts.ps]]

See Figure [[fig-dot-charts]]. Compare it to Figure
[[fig-bar-gr-stateregion]].

# +END_exampletoo

**** Pie Graphs
:PROPERTIES:
:CUSTOM_ID: par-Pie-Graphs
:END:

These can be done with \(\mathsf{R}\) and the \(\mathsf{R}\)
Commander, but they fallen out of favor in recent years because
researchers have determined that while the human eye is good at
judging linear measures, it is notoriously bad at judging relative
areas (such as those displayed by a pie graph). Pie charts are
consequently a very bad way of displaying information. A bar chart or
dot chart is a preferable way of displaying qualitative data. See
=?pie= @@latex:\index{pie@\texttt{pie}}@@ for more information.

We are not going to do any examples of a pie graph and discourage
their use elsewhere.

*** Logical Data
:PROPERTIES:
:CUSTOM_ID: sub-Logical-Data
:END:

There is another type of information recognized by \(\mathsf{R}\)
which does not fall into the above categories. The value is either
=TRUE= or =FALSE= (note that equivalently you can use =1 = TRUE=, =0 =
FALSE=). Here is an example of a logical vector:

#+BEGIN_SRC R :exports both :results output pp  
x <- 5:9
y <- (x < 7.3)
y
#+END_SRC

#+RESULTS:
: [1]  TRUE  TRUE  TRUE FALSE FALSE

Many functions in \(\mathsf{R}\) have options that the user may or may
not want to activate in the function call. For example, the
=stem.leaf= function has the =depths= argument which is =TRUE= by
default. We saw in Section [[#sub-Quantitative-Data]] how to turn the option
off, simply enter =stem.leaf(x, depths = FALSE)= and they will not be
shown on the display.

We can swap =TRUE= with =FALSE= with the exclamation point =!=.

#+BEGIN_SRC R :exports both :results output pp  
!y
#+END_SRC

#+RESULTS:
: [1] FALSE FALSE FALSE  TRUE  TRUE

*** Missing Data
:PROPERTIES:
:CUSTOM_ID: sub-Missing-Data
:END:

Missing data are a persistent and prevalent problem in many
statistical analyses, especially those associated with the social
sciences. \(\mathsf{R}\) reserves the special symbol =NA= to
representing missing data.

Ordinary arithmetic with =NA= values give =NA='s (addition,
subtraction, /etc/.) and applying a function to a vector that has an
=NA= in it will usually give an =NA=.

#+BEGIN_SRC R :exports both :results output pp  
x <- c(3, 7, NA, 4, 7)
y <- c(5, NA, 1, 2, 2)
x + y
#+END_SRC

#+RESULTS:
: [1]  8 NA NA  6  9

Some functions have a =na.rm= argument which when =TRUE= will ignore
missing data as if they were not there (such as =mean=, =var=, =sd=,
=IQR=, =mad=, ...).

#+BEGIN_SRC R :exports both :results output pp  
sum(x)
sum(x, na.rm = TRUE)
#+END_SRC

#+RESULTS:
: [1] NA
: [1] 21

Other functions do not have a =na.rm= argument and will return =NA= or
an error if the argument has \texttt{NA}s. In those cases we can find
the locations of any \texttt{NA}s with the =is.na= function and remove
those cases with the =[]= operator.

#+BEGIN_SRC R :exports both :results output pp  
is.na(x)
z <- x[!is.na(x)]
sum(z)
#+END_SRC

#+RESULTS:
: [1] FALSE FALSE  TRUE FALSE FALSE
: [1] 21

The analogue of =is.na= for rectangular data sets (or data frames) is
the =complete.cases= function. See Appendix [[#sec-Editing-Data-Sets]].

*** Other Data Types
:PROPERTIES:
:CUSTOM_ID: sub-other-data-types
:END:

** Features of Data Distributions
:PROPERTIES:
:CUSTOM_ID: sec-features-of-data
:END:

Given that the data have been appropriately displayed, the next step
is to try to identify salient features represented in the graph. The
acronym to remember is /C/-enter, /U/-nusual features, /S/-pread, and
/S/-hape. (CUSS).

*** Center
:PROPERTIES:
:CUSTOM_ID: sub-Center
:END:

One of the most basic features of a data set is its center. Loosely
speaking, the center of a data set is associated with a number that
represents a middle or general tendency of the data. Of course, there
are usually several values that would serve as a center, and our later
tasks will be focused on choosing an appropriate one for the data at
hand. Judging from the histogram that we saw in Figure
[[fig-histograms-bins]], a measure of center would be about
SRC_R[:eval no-export]{round(mean(precip))} 35.

*** Spread
:PROPERTIES:
:CUSTOM_ID: sub-Spread
:END:

The spread of a data set is associated with its variability; data sets
with a large spread tend to cover a large interval of values, while
data sets with small spread tend to cluster tightly around a central
value.

*** Shape
:PROPERTIES:
:CUSTOM_ID: sub-Shape
:END:

When we speak of the /shape/ of a data set, we are usually referring
to the shape exhibited by an associated graphical display, such as a
histogram. The shape can tell us a lot about any underlying structure
to the data, and can help us decide which statistical procedure we
should use to analyze them.

**** Symmetry and Skewness

A distribution is said to be /right-skewed/ (or /positively skewed/)
if the right tail seems to be stretched from the center. A
/left-skewed/ (or /negatively skewed/) distribution is stretched to
the left side. A symmetric distribution has a graph that is balanced
about its center, in the sense that half of the graph may be reflected
about a central line of symmetry to match the other half.

We have already encountered skewed distributions: both the discoveries
data in Figure [[fig-stripcharts]] and the =precip= data in Figure
[[fig-histograms-bins]] appear right-skewed. The =UKDriverDeaths= data in
Example [[exa-ukdriverdeaths-first]] is relatively symmetric (but note the one
extreme value 2654 identified at the bottom of the stem-and-leaf
display).

**** Kurtosis

Another component to the shape of a distribution is how "peaked" it
is. Some distributions tend to have a flat shape with thin
tails. These are called /platykurtic/, and an example of a platykurtic
distribution is the uniform distribution; see Section [[#sec-The-Continuous-Uniform]]. On the other end of the spectrum are distributions with a
steep peak, or spike, accompanied by heavy tails; these are called
/leptokurtic/. Examples of leptokurtic distributions are the Laplace
distribution and the logistic distribution. See Section [[#sec-Other-Continuous-Distributions]]. In between are distributions (called
/mesokurtic/) with a rounded peak and moderately sized tails. The
standard example of a mesokurtic distribution is the famous
bell-shaped curve, also known as the Gaussian, or normal,
distribution, and the binomial distribution can be mesokurtic for
specific choices of \(p\). See Sections [[#sec-binom-dist]] and [[#sec-The-Normal-Distribution]].

*** Clusters and Gaps
:PROPERTIES:
:CUSTOM_ID: sub-clusters-and-gaps
:END:

Clusters or gaps are sometimes observed in quantitative data
distributions. They indicate clumping of the data about distinct
values, and gaps may exist between clusters. Clusters often suggest an
underlying grouping to the data. For example, take a look at the
=faithful= data which contains the duration of =eruptions= and the
=waiting= time between eruptions of the Old Faithful geyser in
Yellowstone National Park. Do not be frightened by the complicated
information at the left of the display for now; we will learn how to
interpret it in Section [[#sec-Exploratory-Data-Analysis]].

<<exa-stemleaf-multiple-lines-stem>>
#+BEGIN_SRC R :exports both :results output pp
with(faithful, stem.leaf(eruptions))
#+END_SRC

#+RESULTS:
#+begin_example
1 | 2: represents 1.2
 leaf unit: 0.1
            n: 272
   12     s | 667777777777
   51    1. | 888888888888888888888888888899999999999
   71    2* | 00000000000011111111
   87     t | 2222222222333333
   92     f | 44444
   94     s | 66
   97    2. | 889
   98    3* | 0
  102     t | 3333
  108     f | 445555
  118     s | 6666677777
  (16)   3. | 8888888889999999
  138    4* | 0000000000000000111111111111111
  107     t | 22222222222233333333333333333
   78     f | 44444444444445555555555555555555555
   43     s | 6666666666677777777777
   21    4. | 88888888888899999
    4    5* | 0001
#+END_example

There are definitely two clusters of data here; an upper cluster and a
lower cluster.


*** Extreme Observations and other Unusual Features
:PROPERTIES:
:CUSTOM_ID: sub-Extreme-Observations-and
:END:

Extreme observations fall far from the rest of the data. Such
observations are troublesome to many statistical procedures; they
cause exaggerated estimates and instability. It is important to
identify extreme observations and examine the source of the data more
closely. There are many possible reasons underlying an extreme
observation:
- *Maybe the value is a typographical error.* Especially with large
  data sets becoming more prevalent, many of which being recorded by
  hand, mistakes are a common problem. After closer scrutiny, these
  can often be fixed.
- *Maybe the observation was not meant for the study*, because it does
  not belong to the population of interest. For example, in medical
  research some subjects may have relevant complications in their
  genealogical history that would rule out their participation in the
  experiment. Or when a manufacturing company investigates the
  properties of one of its devices, perhaps a particular product is
  malfunctioning and is not representative of the majority of the
  items.
- *Maybe it indicates a deeper trend or phenomenon.* Many of the most
  influential scientific discoveries were made when the investigator
  noticed an unexpected result, a value that was not predicted by the
  classical theory. Albert Einstein, Louis Pasteur, and others built
  their careers on exactly this circumstance.

** Descriptive Statistics
:PROPERTIES:
:CUSTOM_ID: sec-Descriptive-Statistics
:END:

One of my favorite professors would repeatedly harp, "You cannot do
statistics without data."

*What do I want them to know?*
- The fundamental data types we encounter most often, how to classify
  given data into a likely type, and that sometimes the distinction is
  blurry.\

*** Frequencies and Relative Frequencies
:PROPERTIES:
:CUSTOM_ID: sub-Frequencies-and-Relative
:END:

These are used for categorical data. The idea is that there are a
number of different categories, and we would like to get some idea
about how the categories are represented in the population.

*** Measures of Center
:PROPERTIES:
:CUSTOM_ID: sub-Measures-of-Center
:END:

The /sample mean/ is denoted \(\overline{x}\) (read "\(x\)-bar") and
is simply the arithmetic average of the observations:
\begin{equation} 
\overline{x}=\frac{x_{1}+x_{2}+\cdots+x_{n}}{n}=\frac{1}{n}\sum_{i=1}^{n}x_{i}.
\end{equation}
- Good: natural, easy to compute, has nice mathematical properties
- Bad: sensitive to extreme values

It is appropriate for use with data sets that are not highly skewed
without extreme observations.

The /sample median/ is another popular measure of center and is
denoted \(\tilde{x}\). To calculate its value, first sort the data
into an increasing sequence of numbers. If the data set has an odd
number of observations then \(\tilde{x}\) is the value of the middle
observation, which lies in position \((n+1)/2\); otherwise, there are
two middle observations and \(\tilde{x}\) is the average of those
middle values.
- Good: resistant to extreme values, easy to describe
- Bad: not as mathematically tractable, need to sort the data to calculate

One desirable property of the sample median is that it is /resistant/
to extreme observations, in the sense that the value of \(\tilde{x}\)
depends only on those data values in the middle, and is quite
unaffected by the actual values of the outer observations in the
ordered list. The same cannot be said for the sample mean. Any
significant changes in the magnitude of an observation \(x_{k}\)
results in a corresponding change in the value of the
mean. Consequently, the sample mean is said to be /sensitive/ to
extreme observations.

The /trimmed mean/ is a measure designed to address the sensitivity of
the sample mean to extreme observations. The idea is to "trim" a
fraction (less than 1/2) of the observations off each end of the
ordered list, and then calculate the sample mean of what remains. We
will denote it by \(\overline{x}_{t=0.05}\).

- Good: resistant to extreme values, shares nice statistical properties
- Bad: need to sort the data

**** How to do it with \(\mathsf{R}\)

- You can calculate frequencies or relative frequencies with the
  =table= function, and relative frequencies with
  =prop.table(table())=.
- You can calculate the sample mean of a data vector =x= with the
  command =mean(x)=.
- You can calculate the sample median of =x= with the command =median(x)=.
- You can calculate the trimmed mean with the =trim= argument;
  =mean(x, trim = 0.05)=.

*** Order Statistics and the Sample Quantiles
:PROPERTIES:
:CUSTOM_ID: sub-Order-Statistics-and
:END:

A common first step in an analysis of a data set is to sort the
values. Given a data set \(x_{1}\), \(x_{2}\), ..., \(x_{n}\), we may
sort the values to obtain an increasing sequence
\begin{equation} 
x_{(1)}\leq x_{(2)}\leq x_{(3)}\leq\cdots\leq x_{(n)}
\end{equation}
and the resulting values are called the /order statistics/. The
\(k^{\mathrm{th}}\) entry in the list, \(x_{(k)}\), is the
\(k^{\mathrm{th}}\) order statistic, and approximately \(100(k/n)\)%
of the observations fall below \(x_{(k)}\). The order statistics give
an indication of the shape of the data distribution, in the sense that
a person can look at the order statistics and have an idea about where
the data are concentrated, and where they are sparse.

The /sample quantiles/ are related to the order
statistics. Unfortunately, there is not a universally accepted
definition of them. Indeed, \(\mathsf{R}\) is equipped to calculate
quantiles using nine distinct definitions! We will describe the
default method (=type = 7=), but the interested reader can see the
details for the other methods with =?quantile=.

Suppose the data set has \(n\) observations. Find the sample quantile
of order \(p\) (\(0<p<1\)), denoted \(\tilde{q}_{p}\) , as follows:

- First step: :: sort the data to obtain the order statistics
                 \(x_{(1)}\), \(x_{(2)}\), ...,\(x_{(n)}\).
- Second step: :: calculate \((n-1)p+1\) and write it in the form
                  \(k.d\), where \(k\) is an integer and \(d\) is a
                  decimal.
- Third step: :: The sample quantile \(\tilde{q}_{p}\) is
   \begin{equation}
      \tilde{q}_{p}=x_{(k)}+d(x_{(k+1)}-x_{(k)}).
   \end{equation}

The interpretation of \(\tilde{q}_{p}\) is that approximately \(100p\)
% of the data fall below the value \(\tilde{q}_{p}\).

Keep in mind that there is not a unique definition of percentiles,
quartiles, /etc/. Open a different book, and you'll find a different
definition. The difference is small and seldom plays a role except in
small data sets with repeated values. In fact, most people do not even
notice in common use.

Clearly, the most popular sample quantile is \(\tilde{q}_{0.50}\),
also known as the sample median, \(\tilde{x}\). The closest runners-up
are the /first quartile/ \(\tilde{q}_{0.25}\) and the /third quartile/
\(\tilde{q}_{0.75}\) (the /second quartile/ is the median).

**** How to do it with \(\mathsf{R}\)

*At the command prompt* We can find the order statistics of a data set
stored in a vector =x= with the command =sort(x)=.

We can calculate the sample quantiles of any order \(p\) where
\(0<p<1\) for a data set stored in a data vector =x= with the
=quantile= function, for instance, the command =quantile(x, probs =
c(0, 0.25, 0.37))= will return the smallest observation, the first
quartile, \(\tilde{q}_{0.25}\), and the 37th sample quantile,
\(\tilde{q}_{0.37}\). For \(\tilde{q}_{p}\) simply change the values
in the =probs= argument to the value \(p\).


*With the R Commander* we can find the order statistics of a variable
in the =Active data set= by doing =Data= \(\triangleright\) =Manage
variables in Active data set...= \(\triangleright\) =Compute new
variable...= In the =Expression to compute= dialog simply type
=sort(varname)=, where =varname= is the variable that it is desired to
sort.

In =Rcmdr=, we can calculate the sample quantiles for a particular
variable with the sequence =Statistics= \(\triangleright\) =Summaries=
\(\triangleright\) =Numerical Summaries...= We can automatically
calculate the quartiles for all variables in the =Active data set=
with the sequence =Statistics= \(\triangleright\) =Summaries=
\(\triangleright\) =Active Dataset=.

*** Measures of Spread
:PROPERTIES:
:CUSTOM_ID: sub-Measures-of-Spread
:END:

**** Sample Variance and Standard Deviation

The /sample variance/ is denoted \(s^{2}\) and is calculated with the
formula
\begin{equation}
s^{2}=\frac{1}{n-1}\sum_{i=1}^{n}(x_{i}-\overline{x})^{2}.
\end{equation}
The /sample standard deviation/ is \(s=\sqrt{s^{2}}\). Intuitively,
the sample variance is approximately the average squared distance of
the observations from the sample mean. The sample standard deviation
is used to scale the estimate back to the measurement units of the
original data.
- Good: tractable, has nice mathematical/statistical properties
- Bad: sensitive to extreme values
We will spend a lot of time with the variance and standard deviation
in the coming chapters. In the meantime, the following two rules give
some meaning to the standard deviation, in that there are bounds on
how much of the data can fall past a certain distance from the mean.

#+BEGIN_fact
Chebychev's Rule: The proportion of observations within \(k\) standard
deviations of the mean is at least \(1-1/k^{2}\), /i.e./, at least
75%, 89%, and 94% of the data are within 2, 3, and 4 standard
deviations of the mean, respectively.
#+END_fact

Note that Chebychev's Rule does not say anything about when \(k=1\),
because \(1-1/1^{2}=0\), which states that at least 0% of the
observations are within one standard deviation of the mean (which is
not saying much).

Chebychev's Rule applies to any data distribution, /any/ list of
numbers, no matter where it came from or what the histogram looks
like. The price for such generality is that the bounds are not very
tight; if we know more about how the data are shaped then we can say
more about how much of the data can fall a given distance from the
mean.

#+BEGIN_fact
<<fac-Empirical-Rule>> *Empirical Rule:* If data follow a bell-shaped
curve, then approximately 68%, 95%, and 99.7% of the data are within
1, 2, and 3 standard deviations of the mean, respectively.
#+END_fact

**** Interquartile Range

Just as the sample mean is sensitive to extreme values, so the
associated measure of spread is similarly sensitive to
extremes. Further, the problem is exacerbated by the fact that the
extreme distances are squared. We know that the sample quartiles are
resistant to extremes, and a measure of spread associated with them is
the /interquartile range/ (\(IQR\)) defined by
\(IQR=q_{0.75}-q_{0.25}\).
- Good: stable, resistant to outliers, robust to nonnormality, easy to
  explain
- Bad: not as tractable, need to sort the data, only involves the
  middle 50% of the data.

**** Median Absolute Deviation

A measure even more robust than the \(IQR\) is the /median absolute
deviation/ (\(MAD\)). To calculate it we first get the median
\(\widetilde{x}\), next the /absolute deviations/
\(|x_{1}-\tilde{x}|\), \(|x_{2}-\tilde{x}|\), ...,
\(|x_{n}-\tilde{x}|\), and the \(MAD\) is proportional to the median
of those deviations:
\begin{equation}
MAD\propto\mbox{median}(|x_{1}-\tilde{x}|,\ |x_{2}-\tilde{x}|,\ldots,|x_{n}-\tilde{x}|).
\end{equation}
That is, the
\(MAD=c\cdot\mbox{median}(|x_{1}-\tilde{x}|,\ |x_{2}-\tilde{x}|,\ldots,|x_{n}-\tilde{x}|)\),
where \(c\) is a constant chosen so that the \(MAD\) has nice
properties. The value of \(c\) in \(\mathsf{R}\) is by default
\(c=1.4286\). This value is chosen to ensure that the estimator of
\(\sigma\) is correct, on the average, under suitable sampling
assumptions (see Section [[#sec-Point-Estimation-1]]).
- Good: stable, very robust, even more so than the \(IQR\).
- Bad: not tractable, not well known and less easy to explain.

**** Comparing Apples to Apples

We have seen three different measures of spread which, for a given
data set, will give three different answers. Which one should we use?
It depends on the data set. If the data are well behaved, with an
approximate bell-shaped distribution, then the sample mean and sample
standard deviation are natural choices with nice mathematical
properties. However, if the data have an unusual or skewed shape with
several extreme values, perhaps the more resistant choices among the
\(IQR\) or \(MAD\) would be more appropriate.

However, once we are looking at the three numbers it is important to
understand that the estimators are not all measuring the same
quantity, on the average. In particular, it can be shown that when the
data follow an approximately bell-shaped distribution, then on the
average, the sample standard deviation \(s\) and the \(MAD\) will be
the approximately the same value, namely, \(\sigma\), but the \(IQR\)
will be on the average 1.349 times larger than \(s\) and the
\(MAD\). See [[#cha-Sampling-Distributions]] for more details.

**** How to do it with \(\mathsf{R}\)

*At the command prompt* we may compute the sample range with
=range(x)= and the sample variance with =var(x)=, where =x= is a
numeric vector. The sample standard deviation is =sqrt(var(x))= or
just =sd(x)=. The \(IQR\) is =IQR(x)= and the median absolute
deviation is =mad(x)=.

*With the R Commander* we can calculate the sample standard deviation
with the =Statistics= \(\triangleright\) =Summaries=
\(\triangleright\) =Numerical Summaries...=
combination. \(\mathsf{R}\) Commander does not calculate the \(IQR\)
or \(MAD\) in any of the menu selections, by default.

*** Measures of Shape
:PROPERTIES:
:CUSTOM_ID: sub-Measures-of-Shape
:END: 

**** Sample Skewness

The /sample skewness/, denoted by \(g_{1}\), is defined by the formula
\begin{equation}
g_{1}=\frac{1}{n}\frac{\sum_{i=1}^{n}(x_{i}-\overline{x})^{3}}{s^{3}}.
\end{equation}
The sample skewness can be any value \(-\infty<g_{1}<\infty\). The
sign of \(g_{1}\) indicates the direction of skewness of the
distribution. Samples that have \(g_{1}>0\) indicate right-skewed
distributions (or positively skewed), and samples with \(g_{1}<0\)
indicate left-skewed distributions (or negatively skewed). Values of
\(g_{1}\) near zero indicate a symmetric distribution. These are not
hard and fast rules, however. The value of \(g_{1}\) is subject to
sampling variability and thus only provides a suggestion to the
skewness of the underlying distribution.

We still need to know how big is "big", that is, how do we judge
whether an observed value of \(g_{1}\) is far enough away from zero
for the data set to be considered skewed to the right or left? A good
rule of thumb is that data sets with skewness larger than
\(2\sqrt{6/n}\) in magnitude are substantially skewed, in the
direction of the sign of \(g_{1}\). See Tabachnick & Fidell
\cite{Tabachnick2006} for details.

**** Sample Excess Kurtosis

The /sample excess kurtosis/, denoted by \(g_{2}\), is given by the
formula
\begin{equation}
g_{2}=\frac{1}{n}\frac{\sum_{i=1}^{n}(x_{i}-\overline{x})^{4}}{s^{4}}-3.
\end{equation}
The sample excess kurtosis takes values \(-2\leq g_{2}<\infty\). The
subtraction of 3 may seem mysterious but it is done so that mound
shaped samples have values of \(g_{2}\) near zero. Samples with
\(g_{2}>0\) are called /leptokurtic/, and samples with \(g_{2}<0\) are
called /platykurtic/. Samples with \(g_{2}\approx0\) are called
/mesokurtic/.

As a rule of thumb, if \(|g_{2}|>4\sqrt{6/n}\) then the sample excess
kurtosis is substantially different from zero in the direction of the
sign of \(g_{2}\). See Tabachnick & Fidell \cite{Tabachnick2006} for
details.

Notice that both the sample skewness and the sample kurtosis are
invariant with respect to location and scale, that is, the values of
\(g_{1}\) and \(g_{2}\) do not depend on the measurement units of the
data.

**** How to do it with \(\mathsf{R}\)

The =e1071= package \cite{e1071} has the =skewness= function for the
sample skewness and the =kurtosis= function for the sample excess
kurtosis. Both functions have a =na.rm= argument which is =FALSE= by
default.

# +BEGIN_exampletoo
We said earlier that the =discoveries= data looked positively skewed;
let's see what the statistics say:
# +END_exampletoo

#+BEGIN_SRC R :exports both :results output pp  
skewness(discoveries)
2*sqrt(6/length(discoveries))
#+END_SRC

#+RESULTS:
: [1] 1.2076
: [1] 0.4898979

The data are definitely skewed to the right. Let us check the sample
excess kurtosis of the =UKDriverDeaths= data:

#+BEGIN_SRC R :exports both :results output pp  
kurtosis(UKDriverDeaths)
4*sqrt(6/length(UKDriverDeaths))
#+END_SRC

#+RESULTS:
: [1] 0.07133848
: [1] 0.7071068

so that the =UKDriverDeaths= data appear to be mesokurtic, or at least
not substantially leptokurtic.

** Exploratory Data Analysis
:PROPERTIES:
:CUSTOM_ID: sec-Exploratory-Data-Analysis
:END:

This field was founded (mostly) by John Tukey (1915-2000). Its tools
are useful when not much is known regarding the underlying causes
associated with the data set, and are often used for checking
assumptions. For example, suppose we perform an experiment and collect
some data... now what? We look at the data using exploratory visual
tools.

*** More About Stem-and-leaf Displays

There are many bells and whistles associated with stemplots, and the
=stem.leaf= function can do many of them.

- Trim Outliers: :: Some data sets have observations that fall far
                    from the bulk of the other data (in a sense made
                    more precise in Section [[#sub-Outliers]]). These extreme
                    observations often obscure the underlying
                    structure to the data and are best left out of the
                    data display. The =trim.outliers= argument (which
                    is =TRUE= by default) will separate the extreme
                    observations from the others and graph the
                    stemplot without them; they are listed at the
                    bottom (respectively, top) of the stemplot with
                    the label =HI= (respectively =LO=).
- Split Stems: :: The standard stemplot has only one line per stem,
                  which means that all observations with first digit
                  =3= are plotted on the same line, regardless of the
                  value of the second digit. But this gives some
                  stemplots a "skyscraper" appearance, with too many
                  observations stacked onto the same stem. We can
                  often fix the display by increasing the number of
                  lines available for a given stem. For example, we
                  could make two lines per stem, say, =3*= and
                  =3.=. Observations with second digit 0 through 4
                  would go on the upper line, while observations with
                  second digit 5 through 9 would go on the lower
                  line. (We could do a similar thing with five lines
                  per stem, or even ten lines per stem.) The end
                  result is a more spread out stemplot which often
                  looks better. A good example of this was shown on
                  page \pageref{exa-stemleaf-multiple-lines-stem}.
- Depths: :: these are used to give insight into the balance of the
             observations as they accumulate toward the median. In a
             column beside the standard stemplot, the frequency of the
             stem containing the sample median is shown in
             parentheses. Next, frequencies are accumulated from the
             outside inward, including the outliers. Distributions
             that are more symmetric will have better balanced depths
             on either side of the sample median.

**** How to do it with \(\mathsf{R}\)

The basic command is =stem(x)= or a more sophisticated version written
by Peter Wolf called =stem.leaf(x)= in the \(\mathsf{R}\)
Commander. We will describe =stem.leaf= since that is the one used by
\(\mathsf{R}\) Commander.

WARNING: Sometimes when making a stem-and-leaf display the result will
not be what you expected. There are several reasons for this:
- Stemplots by default will trim extreme observations (defined in
  Section [[#sub-Outliers]]) from the display. This in some cases will result
  in stemplots that are not as wide as expected.
- The leafs digit is chosen automatically by =stem.leaf= according to
  an algorithm that the computer believes will represent the data
  well. Depending on the choice of the digit, =stem.leaf= may drop
  digits from the data or round the values in unexpected ways.


<<ite-stemplot-rivers>> Let us take a look at the =rivers= data set

#+BEGIN_SRC R :exports both :results output pp  
stem.leaf(rivers)
#+END_SRC

#+RESULTS:
#+BEGIN_example
1 | 2: represents 120
 leaf unit: 10
            n: 141
    1     1 | 3
   29     2 | 0111133334555556666778888899
   64     3 | 00000111122223333455555666677888999
  (18)    4 | 011222233344566679
   59     5 | 000222234467
   47     6 | 0000112235789
   34     7 | 12233368
   26     8 | 04579
   21     9 | 0008
   17    10 | 035
   14    11 | 07
   12    12 | 047
    9    13 | 0
HI: 1450 1459 1770 1885 2315 2348 2533 3710
#+END_example

The stem-and-leaf display shows a right-skewed shape to the =rivers=
data distribution. Notice that the last digit of each of the data
values were dropped from the display. Notice also that there were
eight extreme observations identified by the computer, and their exact
values are listed at the bottom of the stemplot. Look at the scale on
the left of the stemplot and try to imagine how ridiculous the graph
would have looked had we tried to include enough stems to include
these other eight observations; the stemplot would have stretched over
several pages. Notice finally that we can use the depths to
approximate the sample median for these data. The median lies in the
row identified by =(18)=, which means that the median is the average
of the ninth and tenth observation on that row. Those two values
correspond to =43= and =43=, so a good guess for the median would
be 430. (For the record, the sample median is
\(\widetilde{x}=425\). Recall that stemplots round the data to the
nearest stem-leaf pair.)

Next let us see what the =precip= data look like.

#+BEGIN_SRC R :exports both :results output pp  
stem.leaf(precip)
#+END_SRC

#+RESULTS:
#+BEGIN_example
1 | 2: represents 12
 leaf unit: 1
            n: 70
LO: 7 7.2 7.8 7.8
    8    1* | 1344
   13    1. | 55677
   16    2* | 024
   18    2. | 59
   28    3* | 0000111234
  (15)   3. | 555566677788899
   27    4* | 0000122222334
   14    4. | 56688899
    6    5* | 44
    4    5. | 699
HI: 67
#+END_example

Here is an example of split stems, with two lines per stem. The final
digit of each datum has been dropped for the display. The data appear
to be left skewed with four extreme values to the left and one extreme
value to the right. The sample median is approximately 37 (it turns
out to be 36.6).

*** Hinges and the Five Number Summary
:PROPERTIES:
:CUSTOM_ID: sub-hinges-and-5NS
:END:

Given a data set \(x_{1}\), \(x_{2}\), ..., \(x_{n}\), the hinges are
found by the following method:
- Find the order statistics \(x_{(1)}\), \(x_{(2)}\), ...,
  \(x_{(n)}\).
- The /lower hinge/ \(h_{L}\) is in position \(L=\left\lfloor
  (n+3)/2\right\rfloor / 2\), where the symbol \( \left\lfloor
  x\right\rfloor \) denotes the largest integer less than or equal to
  \(x\). If the position \(L\) is not an integer, then the hinge
  \(h_{L}\) is the average of the adjacent order statistics.
- The /upper hinge/ \(h_{U}\) is in position \(n+1-L\).
Given the hinges, the /five number summary/ (\(5NS\)) is
\begin{equation} 
5NS=(x_{(1)},\ h_{L},\ \tilde{x},\ h_{U},\ x_{(n)}).
\end{equation}
An advantage of the \(5NS\) is that it reduces a potentially large
data set to a shorter list of only five numbers, and further, these
numbers give insight regarding the shape of the data distribution
similar to the sample quantiles in Section [[#sub-Order-Statistics-and]].

**** How to do it with \(\mathsf{R}\)

If the data are stored in a vector =x=, then you can compute the
\(5NS\) with the =fivenum= function.

*** Boxplots
:PROPERTIES:
:CUSTOM_ID: sub-boxplots
:END:

A boxplot is essentially a graphical representation of the \(5NS\). It
can be a handy alternative to a stripchart when the sample size is
large.

A boxplot is constructed by drawing a box alongside the data axis with
sides located at the upper and lower hinges. A line is drawn parallel
to the sides to denote the sample median. Lastly, whiskers are
extended from the sides of the box to the maximum and minimum data
values (more precisely, to the most extreme values that are not
potential outliers, defined below).

Boxplots are good for quick visual summaries of data sets, and the
relative positions of the values in the \(5NS\) are good at indicating
the underlying shape of the data distribution, although perhaps not as
effectively as a histogram. Perhaps the greatest advantage of a
boxplot is that it can help to objectively identify extreme
observations in the data set as described in the next section.

Boxplots are also good because one can visually assess multiple
features of the data set simultaneously:
- Center :: can be estimated by the sample median, \(\tilde{x}\).
- Spread :: can be judged by the width of the box, \(h_{U}-h_{L}\). We
            know that this will be close to the \(IQR\), which can be
            compared to \(s\) and the \(MAD\), perhaps after rescaling
            if appropriate.
- Shape :: is indicated by the relative lengths of the whiskers, and
           the position of the median inside the box. Boxes with
           unbalanced whiskers indicate skewness in the direction of
           the long whisker. Skewed distributions often have the
           median tending in the opposite direction of
           skewness. Kurtosis can be assessed using the box and
           whiskers. A wide box with short whiskers will tend to be
           platykurtic, while a skinny box with wide whiskers
           indicates leptokurtic distributions.
- Extreme observations :: are identified with open circles (see
     below).

*** Outliers
:PROPERTIES:
:CUSTOM_ID: sub-Outliers
:END:

A /potential outlier/ is any observation that falls beyond 1.5 times
the width of the box on either side, that is, any observation less
than \(h_{L}-1.5(h_{U}-h_{L})\) or greater than
\(h_{U}+1.5(h_{U}-h_{L})\). A /suspected outlier/ is any observation
that falls beyond 3 times the width of the box on either side. In
\(\mathsf{R}\), both potential and suspected outliers (if present) are
denoted by open circles; there is no distinction between the two.

When potential outliers are present, the whiskers of the boxplot are
then shortened to extend to the most extreme observation that is not a
potential outlier. If an outlier is displayed in a boxplot, the index
of the observation may be identified in a subsequent plot in =Rcmdr=
by clicking the =Identify outliers with mouse= option in the =Boxplot=
dialog.

What do we do about outliers? They merit further investigation. The
primary goal is to determine why the observation is outlying, if
possible. If the observation is a typographical error, then it should
be corrected before continuing. If the observation is from a subject
that does not belong to the population of interest, then perhaps the
datum should be removed. Otherwise, perhaps the value is hinting at
some hidden structure to the data.

**** How to do it with \(\mathsf{R}\)

The quickest way to visually identify outliers is with a boxplot,
described above. Another way is with the =boxplot.stats= function.

#+ATTR_LATEX: :options [\textbf{Lengths of Major North American Rivers}]
# +BEGIN_exampletoo
We will look for potential outliers in the =rivers= data.

#+BEGIN_SRC R :exports both :results output pp  
boxplot.stats(rivers)$out
#+END_SRC

#+RESULTS:
:  [1] 1459 1450 1243 2348 3710 2315 2533 1306 1270 1885 1770

We may change the =coef= argument to 3 (it is 1.5 by default) to
identify suspected outliers.

#+BEGIN_SRC R :exports both :results output pp  
boxplot.stats(rivers, coef = 3)$out
#+END_SRC

#+RESULTS:
: [1] 2348 3710 2315 2533 1885

# +END_exampletoo

*** Standardizing variables

It is sometimes useful to compare data sets with each other on a scale
that is independent of the measurement units. Given a set of observed
data \(x_{1}\), \(x_{2}\), ..., \(x_{n}\) we get \(z\) scores, denoted
\(z_{1}\), \(z_{2}\), ..., \(z_{n}\), by means of the following
formula \[ z_{i}=\frac{x_{i}-\overline{x}}{s},\quad
i=1,\,2,\,\ldots,\, n.  \]

**** How to do it with \(\mathsf{R}\)

The =scale= function will rescale a numeric vector (or data frame) by
subtracting the sample mean from each value (column) and/or by
dividing each observation by the sample standard deviation.

** Multivariate Data and Data Frames
:PROPERTIES:
:CUSTOM_ID: sec-multivariate-data
:END:

We have had experience with vectors of data, which are long lists of
numbers. Typically, each entry in the vector is a single measurement
on a subject or experimental unit in the study. We saw in Section
[[#sub-Vectors]] how to form vectors with the =c= function or the =scan=
function.

However, statistical studies often involve experiments where there are
two (or more) measurements associated with each subject. We display
the measured information in a rectangular array in which each row
corresponds to a subject, and the columns contain the measurements for
each respective variable. For instance, if one were to measure the
height and weight and hair color of each of 11 persons in a research
study, the information could be represented with a rectangular
array. There would be 11 rows. Each row would have the person's height
in the first column and hair color in the second column.

The corresponding objects in \(\mathsf{R}\) are called /data frames/,
and they can be constructed with the =data.frame= function. Each row
is an observation, and each column is a variable.

# +BEGIN_exampletoo

Suppose we have two vectors =x= and =y= and we want to make a data
frame out of them.

#+BEGIN_SRC R :exports code :results silent
x <- 5:8
y <- letters[3:6]
A <- data.frame(v1 = x, v2 = y)
#+END_SRC

# +END_exampletoo


Notice that =x= and =y= are the same length. This is /necessary/. Also
notice that =x= is a numeric vector and =y= is a character vector. We
may choose numeric and character vectors (or even factors) for the
columns of the data frame, but each column must be of exactly one
type. That is, we can have a column for =height= and a column for
=gender=, but we will get an error if we try to mix function =height=
(numeric) and =gender= (character or factor) information in the same
column.

Indexing of data frames is similar to indexing of vectors. To get the
entry in row \(i\) and column \(j\) do =A[i,j]=. We can get entire
rows and columns by omitting the other index.

#+BEGIN_SRC R :exports both :results output pp
A[3, ]
A[ , 1]
A[ , 2]
#+END_SRC

#+RESULTS:
:   v1 v2
: 3  7  e
: [1] 5 6 7 8
: [1] c d e f
: Levels: c d e f

There are several things happening above. Notice that =A[3, ]= gave a
data frame (with the same entries as the third row of =A=) yet =A[ ,
1]= is a numeric vector. =A[ ,2]= is a factor vector because the
default setting for =data.frame= is =stringsAsFactors = TRUE=.

Data frames have a =names= attribute and the names may be extracted
with the =names= function. Once we have the names we may extract given
columns by way of the dollar sign.

#+BEGIN_SRC R :exports both :results output pp
names(A)
A['v1']
#+END_SRC

#+RESULTS:
: [1] "v1" "v2"
:   v1
: 1  5
: 2  6
: 3  7
: 4  8

The above is identical to =A[ ,1]=. 

*** Bivariate Data						       :TODO:
:PROPERTIES:
:CUSTOM_ID: sub-Bivariate-Data
:END:

- Stacked bar charts
- odds ratio and relative risk
- Introduce the sample correlation coefficient.

The *sample Pearson product-moment correlation coefficient*:

\[
r=\frac{\sum_{i=1}^{n}(x_{i}-\overline{x})(y_{i}-\overline{y})}{\sqrt{\sum_{i=1}^{n}(x_{i}-\overline{x})}\sqrt{\sum_{i=1}^{n}(y_{i}-\overline{y})}}
\]
- independent of scale
- \(-1< r <1\)
- measures /strength/ and /direction/ of linear association
- Two-Way Tables. Done with =table=, or in the \(\mathsf{R}\)
  Commander by following =Statistics= \(\triangleright\) =Contingency
  Tables= \(\triangleright\)} =Two-way Tables=. You can also enter and
  analyze a two-way table.
  - table
  - prop.table
  - addmargins
  - rowPercents (Rcmdr)
  - colPercents (Rcmdr)
  - totPercents(Rcmdr)
  - A <- xtabs(~ gender + race, data = RcmdrTestDrive)
  - xtabs( Freq ~ Class + Sex, data = Titanic) from built in table
  - barplot(A, legend.text=TRUE) 
  - barplot(t(A), legend.text=TRUE) 
  - barplot(A, legend.text=TRUE, beside = TRUE)
  - spineplot(gender ~ race, data = RcmdrTestDrive)
  - Spine plot: plots categorical versus categorical
- Scatterplot: look for linear association and correlation. 
  - carb ~ optden, data = Formaldehyde (boring)
  - conc ~ rate, data = Puromycin
  - xyplot(accel ~ dist, data = attenu) nonlinear association
  - xyplot(eruptions ~ waiting, data = faithful) (linear, two groups)
  - xyplot(Petal.Width ~ Petal.Length, data = iris)
  - xyplot(pressure ~ temperature, data = pressure) (exponential growth)
  - xyplot(weight ~ height, data = women) (strong positive linear)

*** Multivariate Data
:PROPERTIES:
:CUSTOM_ID: sub-Multivariate-Data
:END:

Multivariate Data Display

- Multi-Way Tables. You can do this with =table=, or in \(\mathsf{R}\)
  Commander by following =Statistics= \(\triangleright\) =Contingency
  Tables= \(\triangleright\) =Multi-way Tables=.
- Scatterplot matrix. used for displaying pairwise scatterplots
  simultaneously. Again, look for linear association and correlation.
- 3D Scatterplot. See Figure [[fig-3D-scatterplot-trees]]
- =plot(state.region, state.division)= 
- =barplot(table(state.division,state.region), legend.text=TRUE)=

#+BEGIN_SRC R :eval never
require(graphics)
mosaicplot(HairEyeColor)
x <- apply(HairEyeColor, c(1, 2), sum)
x
mosaicplot(x, main = "Relation between hair and eye color")
y <- apply(HairEyeColor, c(1, 3), sum)
y
mosaicplot(y, main = "Relation between hair color and sex")
z <- apply(HairEyeColor, c(2, 3), sum)
z
mosaicplot(z, main = "Relation between eye color and sex")
#+END_SRC

** Comparing Populations
:PROPERTIES:
:CUSTOM_ID: sec-Comparing-Data-Sets
:END:

Sometimes we have data from two or more groups (or populations) and we
would like to compare them and draw conclusions. Some issues that we
would like to address:
- Comparing centers and spreads: variation within versus between groups
- Comparing clusters and gaps
- Comparing outliers and unusual features
- Comparing shapes.

*** Numerically

I am thinking here about the =Statistics= \(\triangleright\)
=Numerical Summaries= \(\triangleright\) =Summarize by groups= option
or the =Statistics= \(\triangleright\) =Summaries= \(\triangleright\)
=Table of Statistics= option.

*** Graphically

- Boxplots
  - Variable width: the width of the drawn boxplots are proportional
    to \(\sqrt{n_{i}}\), where \(n_{i}\) is the size of the
    \(i^{\mathrm{th}}\) group. Why? Because many statistics have
    variability proportional to the reciprocal of the square root of
    the sample size.
  - Notches: extend to \(1.58\cdot(h_{U}-h_{L})/\sqrt{n}\). The idea
    is to give roughly a 95% confidence interval for the difference in
    two medians. See Chapter [[#cha-Hypothesis-Testing]].
- Stripcharts
  - stripchart(weight ~ feed, method= "stack", data=chickwts) 
- Bar Graphs
  - barplot(xtabs(Freq ~ Admit + Gender, data = UCBAdmissions))
    stacked bar chart
  - barplot(xtabs(Freq ~ Admit, data = UCBAdmissions))
  - barplot(xtabs(Freq ~ Gender + Admit, data = UCBAdmissions, legend = TRUE, beside = TRUE)  oops, discrimination.
  - barplot(xtabs(Freq ~ Admit+Dept, data = UCBAdmissions), legend = TRUE, beside = TRUE) different departments have different standards
  - barplot(xtabs(Freq ~ Gender+Dept, data = UCBAdmissions), legend = TRUE, beside = TRUE) men mostly applied to easy departments, women mostly applied to difficult departments
  - barplot(xtabs(Freq ~ Gender+Dept, data = UCBAdmissions), legend = TRUE, beside = TRUE)
  - barchart(Admit ~ Freq, data = C)
  - barchart(Admit ~ Freq|Gender, data = C)
  - barchart(Admit ~ Freq | Dept, groups = Gender, data = C)
  - barchart(Admit ~ Freq | Dept, groups = Gender, data = C, auto.key = TRUE)
- Histograms
  - ~ breaks | wool{*}tension, data = warpbreaks
  - ~ weight | feed, data = chickwts
  - ~ weight | group, data = PlantGrowth 
  - ~ count | spray, data = InsectSprays
  - ~ len | dose, data = ToothGrowth
  - ~ decrease | treatment, data = OrchardSprays (or rowpos or colpos)
- Scatterplots

#+BEGIN_SRC R :exports code :eval never
xyplot(Petal.Width ~ Petal.Length, data = iris, group = Species)
#+END_SRC

#+NAME: xyplot-group
#+BEGIN_SRC R :exports results :results graphics silent :file fig/datadesc-xyplot-group.ps
print(xyplot(Petal.Width ~ Petal.Length, data = iris, group = Species))
#+END_SRC

#+NAME: fig-xyplot-group
#+CAPTION[Scatterplot of Petal width versus length in the =iris= data]: \small Scatterplot of Petal width versus length in the =iris= data.
#+ATTR_LaTeX: :width 0.9\textwidth :placement [ht!]
#+RESULTS: xyplot-group
[[file:fig/datadesc-xyplot-group.ps]]

- Scatterplot matrices
  - splom( ~ cbind(GNP.deflator,GNP,Unemployed,Armed.Forces,Population,Year,Employed),  data = longley) 
  - splom( ~ cbind(pop15,pop75,dpi), data = LifeCycleSavings)
  - splom( ~ cbind(Murder, Assault, Rape), data = USArrests)
  - splom( ~ cbind(CONT, INTG, DMNR), data = USJudgeRatings)
  - splom( ~ cbind(area,peri,shape,perm), data = rock)
  - splom( ~ cbind(Air.Flow, Water.Temp, Acid.Conc., stack.loss), data = stackloss)
  - splom( ~ cbind(Fertility,Agriculture,Examination,Education,Catholic,Infant.Mortality), data = swiss)
  - splom(~ cbind(Fertility,Agriculture,Examination), data = swiss) (positive and negative)

- Dot charts
  - dotchart(USPersonalExpenditure)
  - dotchart(t(USPersonalExpenditure))
  - dotchart(WorldPhones) (transpose is no good)
  - freeny.x is no good, neither is volcano
  - dotchart(UCBAdmissions{[},,1{]})
  - dotplot(Survived ~ Freq | Class, groups = Sex, data = B)
  - dotplot(Admit ~ Freq | Dept, groups = Gender, data = C)

- Mosaic plot
  - mosaic( ~ Survived + Class + Age + Sex, data = Titanic) (or just mosaic(Titanic))
  - mosaic( ~ Admit + Dept + Gender, data = UCBAdmissions)

- Spine plots
  - spineplot(xtabs(Freq ~ Admit + Gender, data = UCBAdmissions))
  - # rescaled barplot
- Quantile-quantile plots: There are two ways to do this. One way is
  to compare two independent samples (of the same
  size). qqplot(x,y). Another way is to compare the sample quantiles
  of one variable to the theoretical quantiles of another
  distribution.

Given two samples \(x_{1}\), \(x_{2}\), ..., \(x_{n}\), and \(y_{1}\),
\(y_{2}\), ..., \(y_{n}\), we may find the order statistics
\(x_{(1)}\leq x_{(2)}\leq\cdots\leq x_{(n)}\) and \(y_{(1)}\leq
y_{(2)}\leq\cdots\leq y_{(n)}\). Next, plot the \(n\) points
\((x_{(1)},y_{(1)})\), \((x_{(2)},y_{(2)})\), ...,
\((x_{(n)},y_{(n)})\).

It is clear that if \(x_{(k)}=y_{(k)}\) for all \(k=1,2,\ldots,n\),
then we will have a straight line. It is also clear that in the real
world, a straight line is NEVER observed, and instead we have a
scatterplot that hopefully had a general linear trend. What do the
rules tell us?
- If the \(y\)-intercept of the line is greater (less) than zero, then
  the center of the \(Y\) data is greater (less) than the center of
  the \(X\) data.
- If the slope of the line is greater (less) than one, then the spread of the \(Y\) data is greater (less) than the spread of the \(X\) data.

*** Lattice Graphics
:PROPERTIES:
:CUSTOM_ID: sub-Lattice-Graphics
:END:

The following types of plots are useful when there is one variable of
interest and there is a factor in the data set by which the variable
is categorized.

It is sometimes nice to set =lattice.options(default.theme = "col.whitebg")=

**** Side by side boxplots

#+BEGIN_SRC R :exports code :eval never
bwplot(~weight | feed, data = chickwts)
#+END_SRC

#+NAME: bwplot
#+BEGIN_SRC R :exports results :results graphics silent :file fig/datadesc-bwplot.ps
print(bwplot(~weight | feed, data = chickwts))
#+END_SRC

#+NAME: fig-bwplot
#+CAPTION[Boxplots of =weight= by =feed= type in the =chickwts= data]: \small Boxplots of =weight= by =feed= type in the =chickwts= data.
#+ATTR_LaTeX: :width 0.9\textwidth :placement [ht!]
#+RESULTS: bwplot
[[file:fig/datadesc-bwplot.ps]]

**** Histograms

#+BEGIN_SRC R :exports code :eval never
histogram(~age | education, data = infert)
#+END_SRC

#+NAME: histograms-lattice
#+BEGIN_SRC R :exports results :results graphics :file fig/datadesc-histograms-lattice.ps
print(histogram(~age | education, data = infert))
#+END_SRC

#+NAME: fig-histograms-lattice
#+CAPTION[Histograms of =age= by =education= level]: \small Histograms of =age= by =education= level from the =infert= data.
#+ATTR_LaTeX: :width 0.9\textwidth :placement [ht!]
#+RESULTS: histograms-lattice
[[file:fig/datadesc-histograms-lattice.ps]]

**** Scatterplots

#+BEGIN_SRC R :exports code :eval never
xyplot(Petal.Length ~ Petal.Width | Species, data = iris)
#+END_SRC

#+NAME: xyplot
#+BEGIN_SRC R :exports results :results graphics :file fig/datadesc-xyplot.ps
print(xyplot(Petal.Length ~ Petal.Width | Species, data = iris))
#+END_SRC

#+NAME: fig-xyplot
#+CAPTION[An =xyplot= of =Petal.Length= versus =Petal.Width= by =Species=]: \small An =xyplot= of =Petal.Length= versus =Petal.Width= by =Species= in the =iris= data.
#+ATTR_LaTeX: :width 0.9\textwidth :placement [ht!]
#+RESULTS: xyplot
[[file:fig/datadesc-xyplot.ps]]

**** Coplots

#+BEGIN_SRC R :exports code :eval never
coplot(conc ~ uptake | Type * Treatment, data = CO2)
#+END_SRC

#+NAME: coplot
#+BEGIN_SRC R :exports results :results graphics :file fig/datadesc-coplot.ps
print(coplot(conc ~ uptake | Type * Treatment, data = CO2))
#+END_SRC

#+NAME: fig-coplot
#+CAPTION[A =coplot= of =conc= versus =uptake= by =Type= and =Treatment=]: \small A =coplot= of =conc= versus =uptake= by =Type= and =Treatment=.
#+ATTR_LaTeX: :width 0.9\textwidth :placement [ht!]
#+RESULTS: coplot
[[file:fig/datadesc-coplot.ps]]

#+LaTeX: \newpage{}

** Some Remarks about Plotting

Getting your labels to look right

#+BEGIN_SRC R :eval never
library(ggplot2)
a <- qplot(state.division, geom = "bar")
a + opts(axis.text.x = theme_text(angle = -90, hjust = 0))
#+END_SRC

#+BEGIN_SRC R :eval never
hist(precip, freq = FALSE)
lines(density(precip))
qplot(precip, geom = "density")
m <- ggplot(as.data.frame(precip), aes(x = precip))
m + geom_histogram()
m + geom_histogram(aes(y = ..density..)) + geom_density()
#+END_SRC

** Exercises
#+LaTeX: \setcounter{thm}{0}

Open \(\mathsf{R}\) and issue the following commands at the command
line to get started. Note that you need to have the
=RcmdrPlugin.IPSUR= package \cite{RcmdrPlugin.IPSUR} installed, and
for some exercises you need the =e1071= package \cite{e1071}.

#+BEGIN_SRC R :exports code :results silent
library("RcmdrPlugin.IPSUR")
data(RcmdrTestDrive)
attach(RcmdrTestDrive)
names(RcmdrTestDrive)
#+END_SRC

To load the data in the \(\mathsf{R}\) Commander (=Rcmdr=), click the
=Data Set= button, and select =RcmdrTestDrive= as the active data
set. To learn more about the data set and where it comes from, type
=?RcmdrTestDrive= at the command line.

#+BEGIN_xca
<<xca-summary-RcmdrTestDrive>> Perform a summary of all variables in
=RcmdrTestDrive=. You can do this with the command
=summary(RcmdrTestDrive)=.

Alternatively, you can do this in the =Rcmdr= with the sequence
=Statistics= \(\triangleright\) =Summaries= \(\triangleright\) =Active
Data Set=. Report the values of the summary statistics for each
variable.
#+END_xca

#+BEGIN_xca
Make a table of the =race= variable. Do this with =Statistics=
\(\triangleright\) =Summaries= \(\triangleright\) =Frequency
Distributions - IPSUR...=
1. Which ethnicity has the highest frequency?
1. Which ethnicity has the lowest frequency?
1. Include a bar graph of =race=. Do this with =Graphs=
   \(\triangleright\) =IPSUR - Bar Graph...=
#+END_xca

#+BEGIN_xca
Calculate the average =salary= by the factor =gender=. Do this with
=Statistics= \(\triangleright\) =Summaries= \(\triangleright\) =Table
of Statistics...=
1. Which =gender= has the highest mean =salary=? 
1. Report the highest mean =salary=.
1. Compare the spreads for the genders by calculating the standard
   deviation of =salary= by =gender=. Which =gender= has the biggest
   standard deviation?
1. Make boxplots of =salary= by =gender= with the following method:
   #+BEGIN_quote
   On the =Rcmdr=, click =Graphs= \(\triangleright\) =IPSUR - Boxplot...=
   In the =Variable= box, select =salary=.
   Click the =Plot by groups...= box and select =gender=. Click =OK=.
   Click =OK= to graph the boxplot.
   #+END_quote
   How does the boxplot compare to your answers to (1) and (3)?
#+END_xca

#+BEGIN_xca
For this problem we will study the variable =reduction=.
1. Find the order statistics and store them in a vector =x=. /Hint:/
   =x <- sort(reduction)=
1. Find \(x_{(137)}\), the 137\(^{\mathrm{th}}\) order statistic.
1. Find the IQR.
1. Find the Five Number Summary (5NS).
1. Use the 5NS to calculate what the width of a boxplot of =reduction=
   would be.
1. Compare your answers (3) and (5). Are they the same? If not, are
   they close?
1. Make a boxplot of =reduction=, and include the boxplot in your
   report. You can do this with the =boxplot= function, or in =Rcmdr=
   with =Graphs= \(\triangleright\) =IPSUR - Boxplot...=
1. Are there any potential/suspected outliers? If so, list their
   values. /Hint:/ use your answer to (a).
1. Using the rules discussed in the text, classify answers to (8), if
   any, as /potential/ or /suspected/ outliers.
#+END_xca

#+BEGIN_xca
In this problem we will compare the variables =before= and
=after=. Don't forget =library("e1071")=.
1. Examine the two measures of center for both variables. Judging from
   these measures, which variable has a higher center?
1. Which measure of center is more appropriate for =before=? (You may
   want to look at a boxplot.) Which measure of center is more
   appropriate for =after=?
1. Based on your answer to (2), choose an appropriate measure of
   spread for each variable, calculate it, and report its value. Which
   variable has the biggest spread? (Note that you need to make sure
   that your measures are on the same scale.)
1. Calculate and report the skewness and kurtosis for =before=. Based
   on these values, how would you describe the shape of =before=?
1. Calculate and report the skewness and kurtosis for =after=. Based
   on these values, how would you describe the shape of =after=?
1. Plot histograms of =before= and =after= and compare them to your
   answers to (4) and (5).
#+END_xca

#+BEGIN_xca
Describe the following data sets just as if you were communicating
with an alien, but one who has had a statistics class. Mention the
salient features (data type, important properties, anything
special). Support your answers with the appropriate visual displays
and descriptive statistics.
1. Conversion rates of Euro currencies stored in =euro=.
2. State abbreviations stored in =state.abb=.
3. Areas of the world's landmasses stored in =islands=.
4. Areas of the 50 United States stored in =state.area=.
5. Region of the 50 United States stored in =state.region=.
#+END_xca

* Simple Linear Regression                                              :slr:
:PROPERTIES:
:tangle: R/11-slr.R
:CUSTOM_ID: cha-simple-linear-regression
:END:

#+BEGIN_SRC R :exports none :eval never
#    IPSUR: Introduction to Probability and Statistics Using R
#    Copyright (C) 2014  G. Jay Kerns
#
#    Chapter: Simple Linear Regression
#
#    This file is part of IPSUR.
#
#    IPSUR is free software: you can redistribute it and/or modify
#    it under the terms of the GNU General Public License as published by
#    the Free Software Foundation, either version 3 of the License, or
#    (at your option) any later version.
#
#    IPSUR is distributed in the hope that it will be useful,
#    but WITHOUT ANY WARRANTY; without even the implied warranty of
#    MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
#    GNU General Public License for more details.
#
#    You should have received a copy of the GNU General Public License
#    along with IPSUR.  If not, see <http://www.gnu.org/licenses/>.
#+END_SRC

#+BEGIN_SRC R :exports none :eval no-export
# This chapter's package dependencies
library(ggplot2)
library(HH)
library(lmtest)
#+END_SRC

*What do I want them to know?*

- basic philosophy of SLR and the regression assumptions
- point and interval estimation of the model parameters, and how to
  use it to make predictions
- point and interval estimation of future observations from the model
- regression diagnostics, including \( R^{2} \) and basic residual
  analysis
- the concept of influential versus outlying observations, and how to
  tell the difference

** Basic Philosophy
:PROPERTIES:
:CUSTOM_ID: sec-Basic-Philosophy
:END:

Here we have two variables \(X\) and \(Y\). For our purposes, \(X\) is
not random (so we will write \(x\)), but \(Y\) is random. We believe
that \(Y\) depends in /some/ way on \(x\). Some typical examples of \(
(x,Y) \) pairs are

- \( x = \) study time and \( Y = \) score on a test.
- \( x = \) height and \( Y = \) weight.
- \( x = \) smoking frequency and \( Y = \) age of first heart attack.

Given information about the relationship between \(x\) and \(Y\), we
would like to /predict/ future values of \(Y\) for particular values
of \(x\). This turns out to be a difficult problem[fn:fn-yogi], so
instead we first tackle an easier problem: we estimate \( \mathbb{E} Y
\). How can we accomplish this? Well, we know that \(Y\) depends
somehow on \(x\), so it stands to reason that
\begin{equation}
\mathbb{E} Y = \mu(x),\ \mbox{a function of }x.
\end{equation}

[fn:fn-yogi] Yogi Berra once said, "It is always difficult to make
predictions, especially about the future."

But we should be able to say more than that. To focus our efforts we
impose some structure on the functional form of \(\mu\). For instance,
- if \(\mu(x)=\beta_{0}+\beta_{1}x\), we try to estimate \( \beta_{0}
  \) and \( \beta_{1} \).
- if \( \mu(x) = \beta_{0} + \beta_{1}x + \beta_{2}x^{2} \), we try to
  estimate \(\beta_{0}\), \(\beta_{1}\), and \(\beta_{2}\).
- if \( \mu(x) = \beta_{0} \mathrm{e}^{\beta_{1}x} \), we try to
  estimate \(\beta_{0}\) and \(\beta_{1}\).

This helps us in the sense that we concentrate on the estimation of
just a few parameters, \(\beta_{0}\) and \(\beta_{1}\), say, rather
than some nebulous function. Our /modus operandi/ is simply to perform
the random experiment \(n\) times and observe the \(n\) ordered pairs
of data \( (x_{1},Y_{1}),\ (x_{2},Y_{2}),\ \ldots,(x_{n},Y_{n}) \). We
use these \(n\) data points to estimate the parameters.

More to the point, there are /three simple linear regression/ (SLR)
assumptions @@latex:\index{regression assumptions}@@ that will form
the basis for the rest of this chapter:

#+BEGIN_assumption
We assume that \(\mu\) is a linear function of \(x\), that is, 
\begin{equation}
\mu(x)=\beta_{0}+\beta_{1}x,
\end{equation}
where \(\beta_{0}\) and \(\beta_{1}\) are unknown constants to be
estimated.
#+END_assumption

#+BEGIN_assumption
We further assume that \( Y_{i} \) is \( \mu(x_{i}) \), a "signal",
plus some "error" (represented by the symbol \( \epsilon_{i} \)):
\begin{equation}
Y_{i} = \beta_{0} + \beta_{1}x_{i} + \epsilon_{i}, \quad i = 1,2,\ldots,n.
\end{equation}
#+END_assumption

#+BEGIN_assumption
We lastly assume that the errors are IID normal with mean 0 and
variance \( \sigma^{2} \):
\begin{equation}
\epsilon_{1},\epsilon_{2},\ldots,\epsilon_{n}\sim\mathsf{norm}(\mathtt{mean}=0,\,\mathtt{sd}=\sigma).
\end{equation}
#+END_assumption

#+BEGIN_rem
We assume both the normality of the errors \(\epsilon\) and the
linearity of the mean function \( \mu \). Recall from Proposition
[[pro-mvnorm-cond-dist]] of Chapter
[[#cha-Multivariable-Distributions]] that if \( (X,Y) \sim
\mathsf{mvnorm} \) then the mean of \(Y|x\) is a linear function of
\(x\). This is not a coincidence. In more advanced classes we study
the case that both \(X\) and \(Y\) are random, and in particular, when
they are jointly normally distributed.
#+END_rem

*** What does it all mean?
See Figure [[fig-philosophy]]. Shown in the figure is a solid line, the
regression line @@latex:\index{regression line}@@ \(\mu\), which in
this display has slope 0.5 and /y/-intercept 2.5, that is, \( \mu(x) =
2.5 + 0.5x \). The intuition is that for each given value of \(x\), we
observe a random value of \(Y\) which is normally distributed with a
mean equal to the height of the regression line at that \(x\)
value. Normal densities are superimposed on the plot to drive this
point home; in principle, the densities stand outside of the page,
perpendicular to the plane of the paper. The figure shows three such
values of \(x\), namely, \( x = 1 \), \( x = 2.5 \), and \( x = 4
\). Not only do we assume that the observations at the three locations
are independent, but we also assume that their distributions have the
same spread. In mathematical terms this means that the normal
densities all along the line have identical standard deviations --
there is no "fanning out" or "scrunching in" of the normal densities
as \(x\) increases[fn:fn-cvass].

[fn:fn-cvass] In practical terms, this constant variance assumption is
often violated, in that we often observe scatterplots that fan out
from the line as \(x\) gets large or small. We say under those
circumstances that the data show /heteroscedasticity/. There are
methods to address it, but they fall outside the realm of SLR.

#+NAME: philosophy
#+BEGIN_SRC R :exports results :results graphics :file fig/slr-philosophy.ps
plot(c(0,5), c(0,6.5), type = "n", xlab="x", ylab="y")
abline(h = 0, v = 0, col = "gray60")
abline(a = 2.5, b = 0.5, lwd = 2)
x <- 600:3000/600
y <- dnorm(x, mean = 3, sd = 0.5)
lines(y + 1.0, x)
lines(y + 2.5, x + 0.75)
lines(y + 4.0, x + 1.5)
abline(v = c(1, 2.5, 4), lty = 2, col = "grey")
segments(1, 3, 1 + dnorm(0,0,0.5),3, lty = 2, col = "gray")
segments(2.5, 3.75, 2.5 + dnorm(0,0,0.5), 3.75, lty = 2, col = "gray")
segments(4,4.5, 4 + dnorm(0,0,0.5),4.5, lty = 2, col = "gray")
#+END_SRC

#+NAME: fig-philosophy
#+CAPTION[Philosophical foundations of SLR]: \small Philosophical foundations of SLR.
#+ATTR_LaTeX: :width 0.9\textwidth :placement [ht!]
#+RESULTS: philosophy
[[file:fig/slr-philosophy.ps]]


# +BEGIN_exampletoo
<<exa-Speed-and-Stopping>> *Speed and stopping distance of cars.* We
will use the data frame \texttt{cars} @@latex:\index{Data
sets!cars@\texttt{cars}}@@ from the =datasets= package
\cite{datasets}. It has two variables: =speed= and =dist=. We can take
a look at some of the values in the data frame:
#+BEGIN_SRC R :exports both :results output pp 
head(cars)
#+END_SRC

#+RESULTS:
:   speed dist
: 1     4    2
: 2     4   10
: 3     7    4
: 4     7   22
: 5     8   16
: 6     9   10

The =speed= represents how fast the car was going (\(x\)) in miles per
hour and =dist= (\(Y\)) measures how far it took the car to stop, in
feet. We can make a simple scatterplot of the data with the =qplot=
command in the =ggplot2= package \cite{ggplot2}.

#+NAME: carscatter
#+BEGIN_SRC R :exports both :results graphics :file fig/slr-carscatter.ps
plot(dist ~ speed, data = cars)
#+END_SRC

#+NAME: fig-carscatter
#+CAPTION[Scatterplot of =dist= versus =speed= for the =cars= data]: \small A scatterplot of =dist= versus =speed= for the =cars= data.  There is clearly an upward trend to the plot which is approximately linear. 
#+ATTR_LaTeX: :width 0.9\textwidth :placement [ht!]
#+RESULTS: carscatter
[[file:fig/slr-carscatter.ps]]

You can see the output in Figure [[fig-Scatter-cars]], which was produced by the
following code.

#+BEGIN_SRC R :exports code :eval never :results silent
qplot(speed, dist, data = cars)
#+END_SRC

There is a pronounced upward trend to the data points, and the pattern
looks approximately linear. There does not appear to be substantial
fanning out of the points or extreme values.
# +END_exampletoo

** Estimation
:PROPERTIES:
:CUSTOM_ID: sec-SLR-Estimation
:END:

*** Point Estimates of the Parameters
:PROPERTIES:
:CUSTOM_ID: sub-point-estimate-mle-slr
:END:

Where is \( \mu(x) \)? In essence, we would like to "fit" a line to
the points. But how do we determine a "good" line? Is there a /best/
line? We will use maximum likelihood @@latex:\index{maximum
likelihood}@@ to find it. We know:
\begin{equation}
Y_{i} = \beta_{0} + \beta_{1}x_{i} + \epsilon_{i},\quad i=1,\ldots,n,
\end{equation}
where the \( \epsilon_{i} \) are IID
\(\mathsf{norm}(\mathtt{mean}=0,\,\mathtt{sd}=\sigma) \). Thus \(
Y_{i}\sim\mathsf{norm}(\mathtt{mean}=\beta_{0}+\beta_{1}x_{i},\,\mathtt{sd}=\sigma),\
i=1,\ldots,n \). Furthermore, \( Y_{1},\ldots,Y_{n} \) are independent
-- but not identically distributed. The likelihood
function @@latex:\index{likelihood function}@@ is:
\begin{alignat}{1}
L(\beta_{0},\beta_{1},\sigma)= & \prod_{i=1}^{n}f_{Y_{i}}(y_{i}),\\
= & \prod_{i=1}^{n}(2\pi\sigma^{2})^{-1/2}\exp\left\{ \frac{-(y_{i}-\beta_{0}-\beta_{1}x_{i})^{2}}{2\sigma^{2}}\right\} ,\\
= & (2\pi\sigma^{2})^{-n/2}\exp\left\{ \frac{-\sum_{i=1}^{n}(y_{i}-\beta_{0}-\beta_{1}x_{i})^{2}}{2\sigma^{2}}\right\} .
\end{alignat}
We take the natural logarithm to get
\begin{equation}
\label{eq-regML-lnL}
\ln L(\beta_{0},\beta_{1},\sigma)=-\frac{n}{2}\ln(2\pi\sigma^{2})-\frac{\sum_{i=1}^{n}(y_{i}-\beta_{0}-\beta_{1}x_{i})^{2}}{2\sigma^{2}}.
\end{equation}
We would like to maximize this function of \( \beta_{0} \) and \(
\beta_{1} \). See Appendix [[#sec-Multivariable-Calculus]] which tells us that
we should find critical points by means of the partial
derivatives. Let us start by differentiating with respect to
\(\beta_{0} \):
\begin{equation}
\frac{\partial}{\partial\beta_{0}}\ln L=0-\frac{1}{2\sigma^{2}}\sum_{i=1}^{n}2(y_{i}-\beta_{0}-\beta_{1}x_{i})(-1),
\end{equation}
and the partial derivative equals zero when \(
\sum_{i=1}^{n}(y_{i}-\beta_{0}-\beta_{1}x_{i}) = 0 \), that is, when
\begin{equation}
\label{eq-regML-a}
n \beta_{0} + \beta_{1} \sum_{i=1}^{n} x_{i} = \sum_{i = 1}^{n}y_{i}.
\end{equation}
Moving on, we next take the partial derivative of \( \ln L \)
(Equation \eqref{eq-regML-lnL}) with respect to \( \beta_{1} \) to get

\begin{alignat}{1}
\frac{\partial}{\partial \beta_{1}} \ln L = \ & 0 - \frac{1}{2\sigma^{2}} \sum_{i=1}^{n} 2 (y_{i} - \beta_{0} - \beta_{1} x_{i})(-x_{i}),\\ = & \frac{1}{\sigma^{2}}\sum_{i = 1}^{n}\left(x_{i} y_{i} - \beta_{0}x_{i} - \beta_{1}x_{i}^{2}\right),
\end{alignat}
and this equals zero when the last sum equals zero, that is, when
\begin{equation}
\label{eq-regML-b}
\beta_{0} \sum_{i = 1}^{n}x_{i} + \beta_{1} \sum_{i = 1}^{n}x_{i}^{2} = \sum_{i = 1}^{n}x_{i}y_{i}.
\end{equation}
Solving the system of equations \eqref{eq-regML-a} and \eqref{eq-regML-b}
\begin{eqnarray}
n\beta_{0} + \beta_{1}\sum_{i = 1}^{n}x_{i} & = & \sum_{i = 1}^{n}y_{i}\\
\beta_{0}\sum_{i = 1}^{n}x_{i}+\beta_{1}\sum_{i = 1}^{n}x_{i}^{2} & = & \sum_{i = 1}^{n}x_{i}y_{i}
\end{eqnarray}
for \( \beta_{0} \) and \( \beta_{1} \) (in Exercise [[xca-find-mles-SLR]]) gives
\begin{equation}
\label{eq-regline-slope-formula}
\hat{\beta}_{1} = \frac{\sum_{i = 1}^{n}x_{i}y_{i} - \left.\left(\sum_{i = 1}^{n}x_{i}\right)\left(\sum_{i = 1}^{n}y_{i}\right)\right] n}{\sum_{i = 1}^{n}x_{i}^{2} - \left.\left(\sum_{i = 1}^{n}x_{i}\right)^{2}\right/ n}
\end{equation}
and
\begin{equation}
\hat{\beta}_{0} = \overline{y} - \hat{\beta}_{1}\overline{x}.
\end{equation}

The conclusion? To estimate the mean line 
\begin{equation}
\mu(x) = \beta_{0} + \beta_{1}x,
\end{equation}
we use the "line of best fit"
\begin{equation}
\hat{\mu}(x) = \hat{\beta}_{0} + \hat{\beta}_{1}x,
\end{equation}
where \(\hat{\beta}_{0}\) and \(\hat{\beta}_{1}\) are given as
above. For notation we will usually write \( b_{0} = \hat{\beta_{0}}
\) and \( b_{1}=\hat{\beta_{1}} \) so that \( \hat{\mu}(x) = b_{0} +
b_{1}x \).

#+BEGIN_rem
The formula for \( b_{1} \) in Equation \eqref{eq-regline-slope-formula} gets
the job done but does not really make any sense. There are many
equivalent formulas for \( b_{1} \) that are more intuitive, or at the
least are easier to remember. One of the author's favorites is
\begin{equation}
\label{eq-sample-correlation-formula}
b_{1} = r\frac{s_{y}}{s_{x}},
\end{equation}
where \(r\), \( s_{y} \), and \( s_{x} \) are the sample correlation
coefficient and the sample standard deviations of the \(Y\) and \(x\)
data, respectively. See Exercise
[[xca-show-alternate-slope-formula]]. Also, notice the similarity between
Equation \eqref{eq-sample-correlation-formula} and Equation
\eqref{eq-population-slope-slr}.
#+END_rem

**** How to do it with \(\mathsf{R}\)

#+BEGIN_SRC R :exports none :results silent
tmpcoef <- round(as.numeric(coef(lm(dist ~ speed, cars))), 2)
#+END_SRC

Here we go. \(\mathsf{R}\) will calculate the linear regression line
with the =lm= function. We will store the result in an object which we
will call =cars.lm=. Here is how it works:

#+BEGIN_SRC R :exports code :results silent
cars.lm <- lm(dist ~ speed, data = cars)
#+END_SRC

The first part of the input to the =lm= function, =dist ~ speed=, is a
/model formula/, read like this: =dist= is described (or modeled) by
=speed=. The =data = cars= argument tells \(\mathsf{R}\) where to look
for the variables quoted in the model formula. The output object
=cars.lm= contains a multitude of information. Let's first take a look
at the coefficients of the fitted regression line, which are extracted
by the =coef= function (alternatively, we could just type =cars.lm= to
see the same thing):

#+BEGIN_SRC R :exports both :results output pp 
coef(cars.lm)
#+END_SRC

#+RESULTS:
: (Intercept)       speed 
:  -17.579095    3.932409

The parameter estimates \( b_{0} \) and \( b_{1} \) for the intercept
and slope, respectively, are shown above. 

It is good practice to visually inspect the data with the regression
line added to the plot. To do this we first scatterplot the original
data and then follow with a call to the =abline= function. The inputs
to =abline= are the coefficients of =cars.lm=; see Figure
[[fig-Scatter-cars-regline]].

#+NAME: carline
#+BEGIN_SRC R :exports both :results graphics :file fig/slr-carline.ps
ggplot(cars, aes(x = speed, y = dist)) + 
  geom_point(shape = 19) + 
  geom_smooth(method = lm, se = FALSE)
#+END_SRC

#+NAME: fig-carline
#+CAPTION[Scatterplot with added regression line for the =cars= data]: \small A scatterplot with an added regression line for the =cars= data.
#+ATTR_LaTeX: :width 0.9\textwidth :placement [ht!]
#+RESULTS: carline
[[file:fig/slr-carline.ps]]

To calculate points on the regression line we may simply plug the
desired \(x\) value(s) into \( \hat{\mu} \), either by hand, or with
the =predict= function. The inputs to =predict= are the fitted linear
model object, =cars.lm=, and the desired \(x\) value(s) represented by
a data frame. See the example below.

# +BEGIN_exampletoo
<<exa-regline-cars-interpret>> Using the regression line for the
=cars= data:
1. What is the meaning of \( \mu(60) = \beta_{0} + \beta_{1}(8) \)?
   This represents the average stopping distance (in feet) for a car
   going 8 mph.
2. Interpret the slope \(\beta_{1}\). The true slope \(\beta_{1}\)
   represents the increase in average stopping distance for each mile
   per hour faster that the car drives. In this case, we estimate the
   car to take approximately SRC_R[:eval no-export]{tmpcoef[2]} 3.93 additional feet
   to stop for each additional mph increase in speed.
3. Interpret the intercept \( \beta_{0} \). This would represent the
   mean stopping distance for a car traveling 0 mph (which our
   regression line estimates to be \( SRC_R[:eval no-export]{tmpcoef[1]} -17.58. Of
   course, this interpretation does not make any sense for this
   example, because a car travelling 0 mph takes 0 ft to stop (it was
   not moving in the first place)! What went wrong? Looking at the
   data, we notice that the smallest speed for which we have measured
   data is 4 mph. Therefore, if we predict what would happen for
   slower speeds then we would be /extrapolating/, a dangerous
   practice which often gives nonsensical results.
# +END_exampletoo

*** Point Estimates of the Regression Line
:PROPERTIES:
:CUSTOM_ID: sub-slr-point-est-regline
:END:

We said at the beginning of the chapter that our goal was to estimate
\( \mu = \mathbb{E} Y \), and the arguments in Section
[[#sub-point-estimate-mle-slr]] showed how to obtain an estimate \(
\hat{\mu} \) of \( \mu \) when the regression assumptions hold. Now we
will reap the benefits of our work in more ways than we previously
disclosed. Given a particular value \(x_{0}\), there are two values we
would like to estimate:
1. the mean value of \(Y\) at \(x_{0}\), and
2. a future value of \(Y\) at \(x_{0}\). The first is a number,
   \(\mu(x_{0})\), and the second is a random variable, \(Y(x_{0})\),
   but our point estimate is the same for both: \(\hat{\mu}(x_{0})\).

# +BEGIN_exampletoo
<<exa-regline-cars-pe-8mph>> We may use the regression line to obtain
a point estimate of the mean stopping distance for a car traveling 8
mph: \( \hat{\mu}(15) = b_{0} + (8) (b_{1})\) which is approximately
13.88. We would also use 13.88 as a point estimate for the stopping
distance of a future car traveling 8 mph.
# +END_exampletoo


Note that we actually have observed data for a car traveling 8 mph;
its stopping distance was 16 ft as listed in the fifth row of the
=cars= data (which we saw in Example [[exa-Speed-and-Stopping]]).

#+BEGIN_SRC R :exports both :results output pp
cars[5, ]
#+END_SRC

#+RESULTS:
:   speed dist
: 5     8   16

There is a special name for estimates \( \hat{\mu}(x_{0}) \) when \(
x_{0} \) matches an observed value \(x_{i}\) from the data set. They
are called /fitted values/, they are denoted by \(\hat{Y}_{1}\),
\(\hat{Y}_{2}\), ..., \(\hat{Y}_{n}\) (ignoring repetition), and they
play an important role in the sections that follow.

In an abuse of notation we will sometimes write \(\hat{Y}\) or
\(\hat{Y}(x_{0})\) to denote a point on the regression line even when
\(x_{0}\) does not belong to the original data if the context of the
statement obviates any danger of confusion.

We saw in Example [[exa-regline-cars-interpret]] that spooky things can
happen when we are cavalier about point estimation. While it is
usually acceptable to predict/estimate at values of \(x_{0}\) that
fall within the range of the original \(x\) data, it is reckless to
use \(\hat{\mu}\) for point estimates at locations outside that
range. Such estimates are usually worthless. /Do not extrapolate/
unless there are compelling external reasons, and even then, temper it
with a good deal of caution.

**** How to do it with \(\mathsf{R}\)

The fitted values are automatically computed as a byproduct of the
model fitting procedure and are already stored as a component of the
=cars.lm= object. We may access them with the =fitted= function (we
only show the first five entries):

#+BEGIN_SRC R :exports both :results output pp 
fitted(cars.lm)[1:5]
#+END_SRC

#+RESULTS:
:         1         2         3         4         5 
: -1.849460 -1.849460  9.947766  9.947766 13.880175

Predictions at \(x\) values that are not necessarily part of the
original data are done with the =predict= function. The first argument
is the original =cars.lm= object and the second argument =newdata=
accepts a dataframe (in the same form that was used to fit =cars.lm=)
that contains the locations at which we are seeking predictions. Let
us predict the average stopping distances of cars traveling 6 mph, 8
mph, and 21 mph:

#+BEGIN_SRC R :exports both :results output pp 
predict(cars.lm, newdata = data.frame(speed = c(6, 8, 21)))
#+END_SRC

#+RESULTS:
:         1         2         3 
:  6.015358 13.880175 65.001489

Note that there were no observed cars that traveled 6 mph or 21
mph. Also note that our estimate for a car traveling 8 mph matches the
value we computed by hand in Example [[exa-regline-cars-pe-8mph]].

*** Mean Square Error and Standard Error

To find the MLE of \(\sigma^{2}\) we consider the partial derivative
\begin{equation}
\frac{\partial}{\partial\sigma^{2}}\ln L=\frac{n}{2\sigma^{2}}-\frac{1}{2(\sigma^{2})^{2}}\sum_{i=1}^{n}(y_{i}-\beta_{0}-\beta_{1}x_{i})^{2},
\end{equation}
and after plugging in \(\hat{\beta}_{0}\) and \(\hat{\beta}_{1}\) and
setting equal to zero we get
\begin{equation}
\hat{\sigma^{2}}=\frac{1}{n}\sum_{i=1}^{n}(y_{i}-\hat{\beta}_{0}-\hat{\beta}_{1}x_{i})^{2}=\frac{1}{n}\sum_{i=1}^{n}[y_{i}-\hat{\mu}(x_{i})]^{2}.
\end{equation}
We write \(\hat{Yi}=\hat{\mu}(x_{i})\), and we let
\(E_{i}=Y_{i}-\hat{Y_{i}}\) be the \(i^{\mathrm{th}}\) /residual/. We
see
\begin{equation}
n\hat{\sigma^{2}}=\sum_{i=1}^{n}E_{i}^{2}=SSE=\mbox{ the sum of squared errors.}
\end{equation}
For a point estimate of \(\sigma^{2}\) we use the /mean square error/
\(S^{2}\) defined by
\begin{equation}
S^{2}=\frac{SSE}{n-2},
\end{equation}
and we estimate \(\sigma\) with the /standard error/
\(S=\sqrt{S^{2}}\)[fn:fn-se].

[fn:fn-se] Be careful not to confuse the mean square error \(S^{2}\)
with the sample variance \(S^{2}\) in Chapter
\ref{cha-Describing-Data-Distributions}. Other notation the reader may
encounter is the lowercase \(s^{2}\) or the bulky \(MSE\).

**** How to do it with \(\mathsf{R}\)

The residuals for the model may be obtained with the =residuals=
function; we only show the first few entries in the interest of space:

#+BEGIN_SRC R :exports both :results output pp 
residuals(cars.lm)[1:5]
#+END_SRC

#+RESULTS:
:         1         2         3         4         5 
:  3.849460 11.849460 -5.947766 12.052234  2.119825

#+BEGIN_SRC R :exports none :results silent
tmpred <- round(as.numeric(predict(cars.lm, newdata = data.frame(speed = 8))), 2)
tmps <- round(summary(cars.lm)$sigma, 2)
#+END_SRC

In the last section, we calculated the fitted value for \(x=8\) and
found it to be approximately \( \hat{\mu}(8) \approx\) SRC_R[:eval no-export]{tmpred}
13.88. Now, it turns out that there was only one recorded observation
at \(x = 8\), and we have seen this value in the output of
=head(cars)= in Example [[exa-Speed-and-Stopping]]; it was \(\mathtt{dist}
= 16\) ft for a car with \( \mathtt{speed} = 8 \) mph. Therefore, the
residual should be \(E = Y - \hat{Y}\) which is \(E \approx 16 - \)
SRC_R[:eval no-export]{tmpred} 13.88. Now take a look at the last entry of
=residuals(cars.lm)=, above. It is not a coincidence.

The estimate \(S\) for \(\sigma\) is called the =Residual standard
error= and for the =cars= data is shown a few lines up on the
=summary(cars.lm)= output (see How to do it with \(\mathsf{R}\) in
Section [[#sub-slr-interval-est-params]]). We may read it from there to be \(
S\approx\) SRC_R[:eval no-export]{tmps} 15.38, or we can access it directly from the
=summary= object.

#+BEGIN_SRC R :exports both :results output pp
carsumry <- summary(cars.lm)
carsumry$sigma
#+END_SRC

#+RESULTS:
: [1] 15.37959

*** Interval Estimates of the Parameters
:PROPERTIES:
:CUSTOM_ID: sub-slr-interval-est-params
:END:

We discussed general interval estimation in Chapter [[#cha-Estimation]]. There
we found that we could use what we know about the sampling
distribution of certain statistics to construct confidence intervals
for the parameter being estimated. We will continue in that vein, and
to get started we will determine the sampling distributions of the
parameter estimates, \(b_{1}\) and \(b_{0}\).

To that end, we can see from Equation \eqref{eq-regline-slope-formula} (and it
is made clear in Chapter [[#cha-multiple-linear-regression]]) that \(b_{1}\) is
just a linear combination of normally distributed random variables, so
\(b_{1}\) is normally distributed too. Further, it can be shown that
\begin{equation}
b_{1}\sim\mathsf{norm}\left(\mathtt{mean}=\beta_{1},\,\mathtt{sd}=\sigma_{b_{1}}\right)
\end{equation}
where
\begin{equation}
\sigma_{b_{1}}=\frac{\sigma}{\sqrt{\sum_{i=1}^{n}(x_{i}-\overline{x})^{2}}}
\end{equation}
is called /the standard error of/ \(b_{1}\) which unfortunately
depends on the unknown value of \(\sigma\). We do not lose heart,
though, because we can estimate \(\sigma\) with the standard error
\(S\) from the last section. This gives us an estimate \(S_{b_{1}}\)
for \(\sigma_{b_{1}}\) defined by
\begin{equation}
S_{b_{1}}=\frac{S}{\sqrt{\sum_{i=1}^{n}(x_{i}-\overline{x})^{2}}}.
\end{equation}

Now, it turns out that \(b_{0}\), \(b_{1}\), and \(S\) are mutually
independent (see the footnote in Section [[#sub-mlr-interval-est-params]]). Therefore, the quantity
\begin{equation}
T=\frac{b_{1}-\beta_{1}}{S_{b_{1}}}
\end{equation}
has a \(\mathsf{t}(\mathtt{df}=n-2)\) distribution and a \(100(1 -
\alpha)\% \) confidence interval for \(\beta_{1}\) is given by
\begin{equation}
b_{1}\pm\mathsf{t}_{\alpha/2}(\mathtt{df}=n-1)\, S_{b_{1}.}
\end{equation}

It is also sometimes of interest to construct a confidence interval
for \(\beta_{0}\) in which case we will need the sampling distribution
of \(b_{0}\). It is shown in Chapter [[#cha-multiple-linear-regression]] that
\begin{equation}
b_{0}\sim\mathsf{norm}\left(\mathtt{mean}=\beta_{0},\,\mathtt{sd}=\sigma_{b_{0}}\right),
\end{equation}
where \(\sigma_{b_{0}}\) is given by
\begin{equation}
\sigma_{b_{0}}=\sigma\sqrt{\frac{1}{n}+\frac{\overline{x}^{2}}{\sum_{i=1}^{n}(x_{i}-\overline{x})^{2}}},
\end{equation}
and which we estimate with the \(S_{b_{0}}\) defined by
\begin{equation}
S_{b_{0}}=S\sqrt{\frac{1}{n}+\frac{\overline{x}^{2}}{\sum_{i=1}^{n}(x_{i}-\overline{x})^{2}}}.
\end{equation}
Thus the quantity
\begin{equation}
T=\frac{b_{0}-\beta_{0}}{S_{b_{0}}}
\end{equation}
has a \(\mathsf{t}(\mathtt{df}=n-2)\) distribution and a
\(100(1-\alpha)\%\) confidence interval for \(\beta_{0}\) is given by
\begin{equation}
b_{0}\pm\mathsf{t}_{\alpha/2}(\mathtt{df}=n-1)\, S_{b_{0}}.
\end{equation}

**** How to do it with \(\mathsf{R}\)

#+BEGIN_SRC R :exports none :results silent
A <- matrix(as.numeric(round(carsumry$coef, 3)), nrow = 2)
B <- round(confint(cars.lm), 3)
#+END_SRC

Let us take a look at the output from =summary(cars.lm)=:

#+BEGIN_SRC R :exports both :results output pp 
summary(cars.lm)
#+END_SRC

#+RESULTS:
#+BEGIN_example

Call:
lm(formula = dist ~ speed, data = cars)

Residuals:
    Min      1Q  Median      3Q     Max 
-29.069  -9.525  -2.272   9.215  43.201 

Coefficients:
            Estimate Std. Error t value Pr(>|t|)    
(Intercept) -17.5791     6.7584  -2.601   0.0123 *  
speed         3.9324     0.4155   9.464 1.49e-12 ***
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1

Residual standard error: 15.38 on 48 degrees of freedom
Multiple R-squared:  0.6511,	Adjusted R-squared:  0.6438 
F-statistic: 89.57 on 1 and 48 DF,  p-value: 1.49e-12
#+END_example

In the =Coefficients= section we find the parameter estimates and
their respective standard errors in the second and third columns; the
other columns are discussed in Section [[#sec-Model-Utility-SLR]]. If we
wanted, say, a 95% confidence interval for \(\beta_{1}\) we could use
\( b_{1} = \) SRC_R[:eval no-export]{A[2,1]} 3.932 and \( S_{b_{1}} = \) SRC_R[:eval no-export]{A[2,2]}
0.416 together with a \( \mathsf{t}_{0.025}(\mathtt{df}=23) \)
critical value to calculate \( b_{1} \pm
\mathsf{t}_{0.025}(\mathtt{df} = 23) \cdot S_{b_{1}} \).  Or, we could
use the =confint= function.

#+BEGIN_SRC R :exports both :results output pp 
confint(cars.lm)
#+END_SRC

#+RESULTS:
:                  2.5 %    97.5 %
: (Intercept) -31.167850 -3.990340
: speed         3.096964  4.767853

With 95% confidence, the random interval SRC_R[:eval no-export]{B[2,1]} 3.097 to
SRC_R[:eval no-export]{B[2,2]} 4.768 covers the parameter \(\beta_{1}\).

*** Interval Estimates of the Regression Line
:PROPERTIES:
:CUSTOM_ID: sub-slr-interval-est-regline
:END:

We have seen how to estimate the coefficients of regression line with
both point estimates and confidence intervals. We even saw how to
estimate a value \(\hat{\mu}(x)\) on the regression line for a given
value of \(x\), such as \(x=15\).

But how good is our estimate \(\hat{\mu}(15)\)? How much confidence do
we have in /this/ estimate? Furthermore, suppose we were going to
observe another value of \(Y\) at \(x=15\). What could we say?

Intuitively, it should be easier to get bounds on the mean (average)
value of \(Y\) at \(x_{0}\) -- called a /confidence interval for the
mean value of/ \(Y\) /at/ \(x_{0}\) -- than it is to get bounds on a
future observation of \(Y\) (called a /prediction interval for/ \(Y\)
/at/ \(x_{0}\)). As we shall see, the intuition serves us well and
confidence intervals are shorter for the mean value, longer for the
individual value.

Our point estimate of \(\mu(x_{0})\) is of course
\(\hat{Y}=\hat{Y}(x_{0})\), so for a confidence interval we will need
to know \(\hat{Y}\)'s sampling distribution. It turns out (see Section
) that \(\hat{Y}=\hat{\mu}(x_{0})\) is distributed
\begin{equation}
\hat{Y}\sim\mathsf{norm}\left(\mathtt{mean}=\mu(x_{0}),\:\mathtt{sd}=\sigma\sqrt{\frac{1}{n}+\frac{(x_{0}-\overline{x})^{2}}{\sum_{i=1}^{n}(x_{i}-\overline{x})^{2}}}\right).
\end{equation}

Since \(\sigma\) is unknown we estimate it with \(S\) (we should
expect the appearance of a \(\mathsf{t}(\mathtt{df}=n-2)\)
distribution in the near future).

A \( 100(1-\alpha)\% \) /confidence interval (CI) for/ \(\mu(x_{0})\)
is given by
\begin{equation}
\label{eq-SLR-conf-int-formula}
\hat{Y}\pm\mathsf{t}_{\alpha/2}(\mathtt{df}=n-2)\, S\sqrt{\frac{1}{n}+\frac{(x_{0}-\overline{x}^{2})}{\sum_{i=1}^{n}(x_{i}-\overline{x})^{2}}}.
\end{equation}
Prediction intervals are a little bit different. In order to find
confidence bounds for a new observation of \(Y\) (we will denote it
\(Y_{\mbox{new}}\)) we use the fact that
\begin{equation}
Y_{\mbox{new}}\sim\mathtt{norm}\left(\mathtt{mean}=\mu(x_{0}),\,\mathtt{sd}=\sigma\sqrt{1+\frac{1}{n}+\frac{(x_{0}-\overline{x})^{2}}{\sum_{i=1}^{n}(x_{i}-\overline{x})^{2}}}\right).
\end{equation}
Of course, \(\sigma\) is unknown so we estimate it with \(S\) and a \(
100(1-\alpha)\% \) prediction interval (PI) for a future value of
\(Y\) at \(x_{0}\) is given by
\begin{equation}
\label{eq-SLR-pred-int-formula}
\hat{Y}(x_{0})\pm\mathsf{t}_{\alpha/2}(\mathtt{df}=n-1)\: S\,\sqrt{1+\frac{1}{n}+\frac{(x_{0}-\overline{x})^{2}}{\sum_{i=1}^{n}(x_{i}-\overline{x})^{2}}}.
\end{equation}
We notice that the prediction interval in Equation
\eqref{eq-SLR-pred-int-formula} is wider than the confidence interval
in Equation \eqref{eq-SLR-conf-int-formula}, as we expected at the
beginning of the section.

**** How to do it with \(\mathsf{R}\)

Confidence and prediction intervals are calculated in \(\mathsf{R}\)
with the =predict= @@latex:\index{predict@\texttt{predict}}@@ function, which we
encountered in Section [[#sub-slr-point-est-regline]]. There we neglected to
take advantage of its additional =interval= argument. The general
syntax follows.

# +BEGIN_exampletoo

We will find confidence and prediction intervals for the stopping
distance of a car travelling 5, 6, and 21 mph (note from the graph
that there are no collected data for these speeds). We have computed
=cars.lm= earlier, and we will use this for input to the =predict=
function. Also, we need to tell \(\mathsf{R}\) the values of \(x_{0}\)
at which we want the predictions made, and store the \(x_{0}\) values
in a data frame whose variable is labeled with the correct name. /This
is important/.

#+BEGIN_SRC R :exports code :results silent
new <- data.frame(speed = c(5, 6, 21))
#+END_SRC

Next we instruct \(\mathsf{R}\) to calculate the intervals. Confidence
intervals are given by

#+BEGIN_SRC R :exports both :results output pp 
predict(cars.lm, newdata = new, interval = "confidence")
#+END_SRC

#+RESULTS:
:         fit       lwr      upr
: 1  2.082949 -7.644150 11.81005
: 2  6.015358 -2.973341 15.00406
: 3 65.001489 58.597384 71.40559

#+BEGIN_SRC R :exports none :results output pp 
carsCI <- round(predict(cars.lm, newdata = new, interval = "confidence"), 2)
#+END_SRC

#+RESULTS:

Prediction intervals are given by

#+BEGIN_SRC R :exports both :results output pp 
predict(cars.lm, newdata = new, interval = "prediction")
#+END_SRC

#+RESULTS:
:         fit       lwr      upr
: 1  2.082949 -30.33359 34.49948
: 2  6.015358 -26.18731 38.21803
: 3 65.001489  33.42257 96.58040

#+BEGIN_SRC R :exports none :results silent
carsPI <- round(predict(cars.lm, newdata = new, interval = "prediction"), 2)
#+END_SRC

# +END_exampletoo


The type of interval is dictated by the =interval= argument (which is
=none= by default), and the default confidence level is 95\% (which
can be changed with the =level= argument).

# +BEGIN_exampletoo

Using the =cars= data,
1. Report a point estimate of and a 95% confidence interval for the
   mean stopping distance for a car travelling 5 mph.  The fitted
   value for \(x = 5\) is SRC_R[:eval no-export]{carsCI[1, 1]} 2.08, so a point
   estimate would be SRC_R[:eval no-export]{carsCI[1, 1]} 2.08 ft. The 95% CI is
   given by SRC_R[:eval no-export]{carsCI[1, 2]} -7.64 to SRC_R[:eval no-export]{carsCI[1, 3]} 11.81, so
   with 95% confidence the mean stopping distance lies somewhere
   between SRC_R[:eval no-export]{carsCI[1, 2]} -7.64 ft and SRC_R[:eval no-export]{carsCI[1, 3]} 11.81 ft.
2. Report a point prediction for and a 95% prediction interval for the
   stopping distance of a hypothetical car travelling 21 mph.  The
   fitted value for \(x = 21\) is SRC_R[:eval no-export]{carsPI[3, 1]} 65, so a point
   prediction for the stopping distance is SRC_R[:eval no-export]{carsPI[3, 1]} 65 ft. The
   95% PI is SRC_R[:eval no-export]{carsPI[3, 2]} 33.42 to SRC_R[:eval no-export]{carsPI[3,3]} 96.58, so with
   95% confidence we may assert that the hypothetical stopping
   distance for a car travelling 21 mph would lie somewhere between
   SRC_R[:eval no-export]{carsPI[3, 2]} 33.42 ft and SRC_R[:eval no-export]{carsPI[3, 3]} 96.58 ft.
# +END_exampletoo

*** Graphing the Confidence and Prediction Bands

We earlier guessed that a bound on the value of a single new
observation would be inherently less certain than a bound for an
average (mean) value; therefore, we expect the CIs for the mean to be
tighter than the PIs for a new observation. A close look at the
standard deviations in Equations \eqref{eq-SLR-conf-int-formula} and
\eqref{eq-SLR-pred-int-formula} confirms our guess, but we would like
to see a picture to drive the point home.

We may plot the confidence and prediction intervals with one fell
swoop using the =ci.plot= function from the =HH= package
\cite{HH}. The graph is displayed in Figure [[fig-carscipi]].

#+BEGIN_SRC R :exports code :eval never
library(HH)
ci.plot(cars.lm)
#+END_SRC

Notice that the bands curve outward from the regression line as the
\(x\) values move away from the center. This is expected once we
notice the \((x_{0}-\overline{x})^{2}\) term in the standard deviation
formulas in Equations \eqref{eq-SLR-conf-int-formula} and
\eqref{eq-SLR-pred-int-formula}.

#+NAME: carscipi
#+BEGIN_SRC R :exports both :results graphics :file fig/slr-carscipi.ps
print(ci.plot(cars.lm))
#+END_SRC

#+NAME: fig-carscipi
#+CAPTION[Scatterplot with confidence/prediction bands for the =cars= data]: \small A scatterplot with confidence/prediction bands for the =cars= data.
#+ATTR_LaTeX: :width 0.9\textwidth :placement [ht!]
#+RESULTS: carscipi
[[file:fig/slr-carscipi.ps]]

** Model Utility and Inference
:PROPERTIES:
:CUSTOM_ID: sec-Model-Utility-SLR
:END:

*** Hypothesis Tests for the Parameters
:PROPERTIES:
:CUSTOM_ID: sub-slr-hypoth-test-params
:END:

Much of the attention of SLR is directed toward \(\beta_{1}\) because
when \( \beta_{1}\neq 0 \) the mean value of \(Y\) increases (or
decreases) as \(x\) increases. It is really boring when
\(\beta_{1}=0\), because in that case the mean value of \(Y\) remains
the same, regardless of the value of \(x\) (when the regression
assumptions hold, of course). It is thus very important to decide
whether or not \( \beta_{1} = 0 \). We address the question with a
statistical test of the null hypothesis \(H_{0}:\beta_{1}=0\) versus
the alternative hypothesis \(H_{1}:\beta_{1}\neq0\), and to do that we
need to know the sampling distribution of \(b_{1}\) when the null
hypothesis is true.

To this end we already know from Section [[#sub-slr-interval-est-params]] that
the quantity

\begin{equation} 
T=\frac{b_{1}-\beta_{1}}{S_{b_{1}}}
\end{equation}
has a \(\mathsf{t}(\mathtt{df}=n-2)\) distribution; therefore, when
\(\beta_{1}=0\) the quantity \(b_{1}/S_{b_{1}}\) has a
\(\mathsf{t}(\mathtt{df}=n-2)\) distribution and we can compute a
\(p\)-value by comparing the observed value of \(b_{1}/S{}_{b_{1}}\)
with values under a \(\mathsf{t}(\mathtt{df}=n-2)\) curve.

Similarly, we may test the hypothesis \(H_{0}:\beta_{0}=0\) versus the
alternative \(H_{1}:\beta_{0}\neq0\) with the statistic
\(T=b_{0}/S_{b_{0}}\), where \(S_{b_{0}}\) is given in Section [[#sub-slr-interval-est-params]]. The test is conducted the same way as for
\(\beta_{1}\).

**** How to do it with \(\mathsf{R}\)

Let us take another look at the output from =summary(cars.lm)=:

#+BEGIN_SRC R :exports both :results output pp 
summary(cars.lm)
#+END_SRC

#+RESULTS:
#+BEGIN_example
null device 
          1

Call:
lm(formula = dist ~ speed, data = cars)

Residuals:
    Min      1Q  Median      3Q     Max 
-29.069  -9.525  -2.272   9.215  43.201 

Coefficients:
            Estimate Std. Error t value Pr(>|t|)    
(Intercept) -17.5791     6.7584  -2.601   0.0123 *  
speed         3.9324     0.4155   9.464 1.49e-12 ***
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1

Residual standard error: 15.38 on 48 degrees of freedom
Multiple R-squared:  0.6511,	Adjusted R-squared:  0.6438 
F-statistic: 89.57 on 1 and 48 DF,  p-value: 1.49e-12
#+END_example

In the =Coefficients= section we find the \(t\) statistics and the
\(p\)-values associated with the tests that the respective parameters
are zero in the fourth and fifth columns. Since the \(p\)-values are
(much) less than 0.05, we conclude that there is strong evidence that
the parameters \(\beta_{1}\neq0\) and \(\beta_{0}\neq0\), and as such,
we say that there is a statistically significant linear relationship
between =dist= and =speed=.

*** Simple Coefficient of Determination

It would be nice to have a single number that indicates how well our
linear regression model is doing, and the /simple coefficient of
determination/ is designed for that purpose. In what follows, we
observe the values \(Y_{1}\), \(Y_{2}\), ...,\(Y_{n}\), and the goal
is to estimate \(\mu(x_{0})\), the mean value of \(Y\) at the location
\(x_{0}\).

If we disregard the dependence of \(Y\) and \(x\) and base our
estimate only on the \(Y\) values then a reasonable choice for an
estimator is just the MLE of \(\mu\), which is \(\overline{Y}\). Then
the errors incurred by the estimate are just \(Y_{i}-\overline{Y}\)
and the variation about the estimate as measured by the sample
variance is proportional to
\begin{equation}
SSTO=\sum_{i=1}^{n}(Y_{i}-\overline{Y})^{2}.
\end{equation}
The acronym \(SSTO\) stands for /total sum of squares/.  And we have
additional information, namely, we have values \(x_{i}\) associated
with each value of \(Y_{i}\). We have seen that this information leads
us to the estimate \(\hat{Y_{i}}\) and the errors incurred are just
the residuals, \(E_{i}=Y_{i}-\hat{Y_{i}}\). The variation associated
with these errors can be measured with
\begin{equation}
SSE=\sum_{i=1}^{n}(Y_{i}-\hat{Y_{i}})^{2}.
\end{equation}
We have seen the \(SSE\) before, which stands for the /sum of squared
errors/ or /error sum of squares/. Of course, we would expect the
error to be less in the latter case, since we have used more
information. The improvement in our estimation as a result of the
linear regression model can be measured with the difference \[
(Y_{i}-\overline{Y})-(Y_{i}-\hat{Y_{i}})=\hat{Y_{i}}-\overline{Y}, \]
and we measure the variation in these errors with
\begin{equation}
SSR=\sum_{i=1}^{n}(\hat{Y_{i}}-\overline{Y})^{2},
\end{equation}
also known as the /regression sum of squares/. It is not obvious, but
some algebra proved a famous result known as the *ANOVA Equality*:
\begin{equation}
\label{eq-anovaeq}
\sum_{i=1}^{n}(Y_{i}-\overline{Y})^{2}=\sum_{i=1}^{n}(\hat{Y_{i}}-\overline{Y})^{2}+\sum_{i=1}^{n}(Y_{i}-\hat{Y_{i}})^{2}
\end{equation}
or in other words,
\begin{equation}
SSTO=SSR+SSE.
\end{equation}
This equality has a nice interpretation. Consider \(SSTO\) to be the
/total variation/ of the errors. Think of a decomposition of the total
variation into pieces: one piece measuring the reduction of error from
using the linear regression model, or /explained variation/ (\(SSR\)),
while the other represents what is left over, that is, the errors that
the linear regression model doesn't explain, or /unexplained
variation/ (\(SSE\)). In this way we see that the ANOVA equality
merely partitions the variation into \[ \mbox{total
variation}=\mbox{explained variation}+\mbox{unexplained variation}.
\] For a single number to summarize how well our model is doing we use
the /simple coefficient of determination/ \(r^{2}\), defined by
\begin{equation}
r^{2}=1-\frac{SSE}{SSTO}.
\end{equation}
We interpret \(r^{2}\) as the proportion of total variation that is
explained by the simple linear regression model. When \(r^{2}\) is
large, the model is doing a good job; when \(r^{2}\) is small, the
model is not doing a good job.

Related to the simple coefficient of determination is the sample
correlation coefficient, \(r\). As you can guess, the way we get \(r\)
is by the formula \(|r|=\sqrt{r^{2}}\). The sign of \(r\) is equal the
sign of the slope estimate \(b_{1}\). That is, if the regression line
\(\hat{\mu}(x)\) has positive slope, then
\(r=\sqrt{r^{2}}\). Likewise, if the slope of \(\hat{\mu}(x)\) is
negative, then \(r=-\sqrt{r^{2}}\).

**** How to do it with \(\mathsf{R}\)

The primary method to display partitioned sums of squared errors is
with an /ANOVA table/. The command in \(\mathsf{R}\) to produce such a
table is =anova=. The input to =anova= is the result of an =lm= call
which for the =cars= data is =cars.lm=.

#+BEGIN_SRC R :exports both :results output pp 
anova(cars.lm)
#+END_SRC

#+RESULTS:
: Analysis of Variance Table
: 
: Response: dist
:           Df Sum Sq Mean Sq F value   Pr(>F)    
: speed      1  21186 21185.5  89.567 1.49e-12 ***
: Residuals 48  11354   236.5                     
: ---
: Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1

The output gives
\[
r^{2}=1-\frac{SSE}{SSR+SSE}=1-\frac{11353.5}{21185.5+11353.5}\approx0.65.
\]

The interpretation should be: "The linear regression line accounts for
approximately 65% of the variation of =dist= as explained by =speed=".

The value of \(r^{2}\) is stored in the =r.squared= component of
=summary(cars.lm)=, which we called =carsumry=.

#+BEGIN_SRC R :exports both :results output pp 
carsumry$r.squared
#+END_SRC

#+RESULTS:
: [1] 0.6510794

We already knew this. We saw it in the next to the last line of the
=summary(cars.lm)= output where it was called =Multiple
R-squared=. Listed right beside it is the =Adjusted R-squared= which
we will discuss in Chapter [[#cha-multiple-linear-regression]].  For the =cars=
data, we find \(r\) to be

#+BEGIN_SRC R :exports both :results output pp 
sqrt(carsumry$r.squared)
#+END_SRC

#+RESULTS:
: [1] 0.8068949

We choose the principal square root because the slope of the
regression line is positive.

*** Overall /F/ statistic
:PROPERTIES:
:CUSTOM_ID: sub-slr-overall-F-statistic
:END:

There is another way to test the significance of the linear regression
model. In SLR, the new way also tests the hypothesis
\(H_{0}:\beta_{1}=0\) versus \(H_{1}:\beta_{1}\neq0\), but it is done
with a new test statistic called the /overall F statistic/. It is
defined by
\begin{equation}
\label{eq-slr-overall-F-statistic}
F=\frac{SSR}{SSE/(n-2)}.
\end{equation}

Under the regression assumptions and when \(H_{0}\) is true, the \(F\)
statistic has an \(\mathtt{f}(\mathtt{df1}=1,\,\mathtt{df2}=n-2)\)
distribution. We reject \(H_{0}\) when \(F\) is large -- that is, when
the explained variation is large relative to the unexplained
variation.

All this being said, we have not yet gained much from the overall
\(F\) statistic because we already knew from Section
[[#sub-slr-hypoth-test-params]] how to test \(H_{0}:\beta_{1} =
0\)... we use the Student's \(t\) statistic. What is worse is that (in
the simple linear regression model) it can be proved that the \(F\) in
Equation \eqref{eq-slr-overall-F-statistic} is exactly the Student's
\(t\) statistic for \(\beta_{1}\) squared,

\begin{equation}
F=\left(\frac{b_{1}}{S_{b_{1}}}\right)^{2}.
\end{equation}

So why bother to define the \(F\) statistic? Why not just square the
\(t\) statistic and be done with it? The answer is that the \(F\)
statistic has a more complicated interpretation and plays a more
important role in the multiple linear regression model which we will
study in Chapter [[#cha-multiple-linear-regression]]. See Section
[[#sub-mlr-Overall-F-Test]] for details.

**** How to do it with \(\mathsf{R}\)

The overall \(F\) statistic and \(p\)-value are displayed in the
bottom line of the =summary(cars.lm)= output. It is also shown in the
final columns of =anova(cars.lm)=:

#+BEGIN_SRC R :exports both :results output pp 
anova(cars.lm)
#+END_SRC

#+RESULTS:
: Analysis of Variance Table
: 
: Response: dist
:           Df Sum Sq Mean Sq F value   Pr(>F)    
: speed      1  21186 21185.5  89.567 1.49e-12 ***
: Residuals 48  11354   236.5                     
: ---
: Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1

#+BEGIN_SRC R :exports none :results silent
tmpf <- round(as.numeric(carsumry$fstatistic[1]), 2)
#+END_SRC

Here we see that the \(F\) statistic is SRC_R[:eval no-export]{tmpf} 89.57 with a
\(p\)-value very close to zero. The conclusion: there is very strong
evidence that \(H_{0}:\beta_{1} = 0 \) is false, that is, there is
strong evidence that \( \beta_{1} \neq 0 \). Moreover, we conclude
that the regression relationship between =dist= and =speed= is
significant.

Note that the value of the \(F\) statistic is the same as the
Student's \(t\) statistic for =speed= squared.

** Residual Analysis
:PROPERTIES:
:CUSTOM_ID: sec-Residual-Analysis-SLR
:END:

We know from our model that \(Y=\mu(x)+\epsilon\), or in other words,
\(\epsilon=Y-\mu(x)\). Further, we know that
\(\epsilon\sim\mathsf{norm}(\mathtt{mean}=0,\,\mathtt{sd}=\sigma)\). We
may estimate \(\epsilon_{i}\) with the /residual/
\(E_{i}=Y_{i}-\hat{Y_{i}}\), where
\(\hat{Y_{i}}=\hat{\mu}(x_{i})\). If the regression assumptions hold,
then the residuals should be normally distributed. We check this in
Section [[#sub-Normality-Assumption]]. Further, the residuals should have mean
zero with constant variance \(\sigma^{2}\), and we check this in
Section [[#sub-Constant-Variance-Assumption]]. Last, the residuals should be
independent, and we check this in Section [[#sub-Independence-Assumption]].

In every case, we will begin by looking at residual plots -- that is,
scatterplots of the residuals \(E_{i}\) versus index or predicted
values \(\hat{Y_{i}}\) -- and follow up with hypothesis tests.

*** Normality Assumption
:PROPERTIES:
:CUSTOM_ID: sub-Normality-Assumption
:END:

We can assess the normality of the residuals with graphical methods
and hypothesis tests. To check graphically whether the residuals are
normally distributed we may look at histograms or /q-q/ plots. We
first examine a histogram in Figure [[fig-Normal-q-q-plot-cars]]. There we see
that the distribution of the residuals appears to be mound shaped, for
the most part. We can plot the order statistics of the sample versus
quantiles from a \(\mathsf{norm}(\mathtt{mean}=0,\,\mathtt{sd}=1)\)
distribution with the command =plot(cars.lm, which = 2)=, and the
results are in Figure [[fig-Normal-q-q-plot-cars]]. If the assumption of
normality were true, then we would expect points randomly scattered
about the dotted straight line displayed in the figure. In this case,
we see a slight departure from normality in that the dots show
systematic clustering on one side or the other of the line. The points
on the upper end of the plot also appear begin to stray from the
line. We would say there is some evidence that the residuals are not
perfectly normal.

#+NAME: Normal-q-q-plot-cars
#+BEGIN_SRC R :exports both :results graphics :file fig/slr-Normal-q-q-plot-cars.ps
plot(cars.lm, which = 2)
#+END_SRC

#+NAME: fig-Normal-q-q-plot-cars
#+CAPTION[Normal q-q plot of the residuals for the =cars= data]: \small Used for checking the normality assumption. Look out for any curvature or substantial departures from the straight line; hopefully the dots hug the line closely.
#+ATTR_LaTeX: :width 0.9\textwidth :placement [ht!]
#+RESULTS: Normal-q-q-plot-cars
[[file:fig/slr-Normal-q-q-plot-cars.ps]]

**** Testing the Normality Assumption

Even though we may be concerned about the plots, we can use tests to
determine if the evidence present is statistically significant, or if
it could have happened merely by chance. There are many statistical
tests of normality. We will use the Shapiro-Wilk test, since it is
known to be a good test and to be quite powerful. However, there are
many other fine tests of normality including the Anderson-Darling test
and the Lillefors test, just to mention two of them.

The Shapiro-Wilk test is based on the statistic
\begin{equation}
W=\frac{\left(\sum_{i=1}^{n}a_{i}E_{(i)}\right)^{2}}{\sum_{j=1}^{n}E_{j}^{2}},
\end{equation}
where the \(E_{(i)}\) are the ordered residuals and the \(a_{i}\) are
constants derived from the order statistics of a sample of size \(n\)
from a normal distribution. See Section [[#sub-Shapiro-Wilk-Normality-Test]].
We perform the Shapiro-Wilk test below, using the =shapiro.test=
function from the =stats= package \cite{stats}. The hypotheses are \[
H_{0}:\mbox{ the residuals are normally distributed } \] versus \[
H_{1}:\mbox{ the residuals are not normally distributed.}  \] The
results from \(\mathsf{R}\) are

#+BEGIN_SRC R :exports both :results output pp 
shapiro.test(residuals(cars.lm))
#+END_SRC

#+RESULTS:
: 
: 	Shapiro-Wilk normality test
: 
: data:  residuals(cars.lm)
: W = 0.9451, p-value = 0.02152

For these data we would reject the assumption of normality of the
residuals at the \(\alpha=0.05\) significance level, but do not lose
heart, because the regression model is reasonably robust to departures
from the normality assumption. As long as the residual distribution is
not highly skewed, then the regression estimators will perform
reasonably well. Moreover, departures from constant variance and
independence will sometimes affect the quantile plots and histograms,
therefore it is wise to delay final decisions regarding normality
until all diagnostic measures have been investigated.

*** Constant Variance Assumption
:PROPERTIES:
:CUSTOM_ID: sub-Constant-Variance-Assumption
:END:

We will again go to residual plots to try and determine if the spread
of the residuals is changing over time (or index). However, it is
unfortunately not that easy because the residuals do not have constant
variance! In fact, it can be shown that the variance of the residual
\(E_{i}\) is
\begin{equation}
\mbox{Var$(E_{i})$}=\sigma^{2}(1-h_{ii}),\quad i=1,2,\ldots,n,
\end{equation}
where \(h_{ii}\) is a quantity called the /leverage/ which is defined
below. Consequently, in order to check the constant variance
assumption we must standardize the residuals before plotting. We
estimate the standard error of \(E_{i}\) with
\(s_{E_{i}}=s\sqrt{(1-h_{ii})}\) and define the /standardized
residuals/ \(R_{i}\), \(i=1,2,\ldots,n\), by
\begin{equation} 
R_{i}=\frac{E_{i}}{s\,\sqrt{1-h_{ii}}},\quad i=1,2,\ldots,n.
\end{equation}
For the constant variance assumption we do not need the sign of the
residual so we will plot \(\sqrt{|R_{i}|}\) versus the fitted
values. As we look at a scatterplot of \(\sqrt{|R_{i}|}\) versus
\(\hat{Y}_{i}\) we would expect under the regression assumptions to
see a constant band of observations, indicating no change in the
magnitude of the observed distance from the line. We want to watch out
for a fanning-out of the residuals, or a less common funneling-in of
the residuals. Both patterns indicate a change in the residual
variance and a consequent departure from the regression assumptions,
the first an increase, the second a decrease.

In this case, we plot the standardized residuals versus the fitted
values. The graph may be seen in Figure [[fig-std-resids-fitted-cars]]. For
these data there does appear to be somewhat of a slight fanning-out of
the residuals.

#+NAME: std-resids-fitted-cars
#+BEGIN_SRC R :exports both :results graphics :file fig/slr-std-resids-fitted-cars.ps
plot(cars.lm, which = 3)
#+END_SRC

#+NAME: fig-std-resids-fitted-cars
#+CAPTION[Plot of standardized residuals against the fitted values for the =cars= data]: \small Used for checking the constant variance assumption. Watch out for any fanning out (or in) of the dots; hopefully they fall in a constant band.
#+ATTR_LaTeX: :width 0.9\textwidth :placement [ht!]
#+RESULTS: std-resids-fitted-cars
[[file:fig/slr-std-resids-fitted-cars.ps]]

**** Testing the Constant Variance Assumption

We will use the Breusch-Pagan test to decide whether the variance of
the residuals is nonconstant. The null hypothesis is that the variance
is the same for all observations, and the alternative hypothesis is
that the variance is not the same for all observations. The test
statistic is found by fitting a linear model to the centered squared
residuals,
\begin{equation}
W_{i} = E_{i}^{2} - \frac{SSE}{n}, \quad i=1,2,\ldots,n.
\end{equation}

By default the same explanatory variables are used in the new model
which produces fitted values \(\hat{W}_{i}\), \(i=1,2,\ldots,n\). The
Breusch-Pagan test statistic in \(\mathsf{R}\) is then calculated with
\begin{equation}
BP=n\sum_{i=1}^{n}\hat{W}_{i}^{2}\div\sum_{i=1}^{n}W_{i}^{2}.
\end{equation}
We reject the null hypothesis if \(BP\) is too large, which happens
when the explained variation i the new model is large relative to the
unexplained variation in the original model.  We do it in
\(\mathsf{R}\) with the =bptest= function from the =lmtest= package
\cite{lmtest}.
#+BEGIN_SRC R :exports both :results output pp
bptest(cars.lm)
#+END_SRC

#+RESULTS:
: 
: 	studentized Breusch-Pagan test
: 
: data:  cars.lm
: BP = 3.2149, df = 1, p-value = 0.07297

For these data we would not reject the null hypothesis at the
\(\alpha=0.05\) level. There is relatively weak evidence against the
assumption of constant variance.

*** Independence Assumption
    SCHEDULED: <2014-06-05 Thu>
:PROPERTIES:
:CUSTOM_ID: sub-Independence-Assumption
:END:

One of the strongest of the regression assumptions is the one
regarding independence. Departures from the independence assumption
are often exhibited by correlation (or autocorrelation, literally,
self-correlation) present in the residuals. There can be positive or
negative correlation.

Positive correlation is displayed by positive residuals followed by
positive residuals, and negative residuals followed by negative
residuals. Looking from left to right, this is exhibited by a cyclical
feature in the residual plots, with long sequences of positive
residuals being followed by long sequences of negative ones.

On the other hand, negative correlation implies positive residuals
followed by negative residuals, which are then followed by positive
residuals, /etc/. Consequently, negatively correlated residuals are
often associated with an alternating pattern in the residual plots. We
examine the residual plot in Figure [[fig-resids-fitted-cars]]. There is no
obvious cyclical wave pattern or structure to the residual plot.

#+NAME: resids-fitted-cars
#+BEGIN_SRC R :exports both :results graphics :file fig/slr-resids-fitted-cars.ps
plot(cars.lm, which = 1)
#+END_SRC

#+NAME: fig-resids-fitted-cars
#+CAPTION[Plot of the residuals versus the fitted values for the =cars= data]: \small Used for checking the independence assumption. Watch out for any patterns or structure; hopefully the points are randomly scattered on the plot.
#+ATTR_LaTeX: :width 0.9\textwidth :placement [ht!]
#+RESULTS: resids-fitted-cars
[[file:fig/slr-resids-fitted-cars.ps]]

**** Testing the Independence Assumption

We may statistically test whether there is evidence of autocorrelation
in the residuals with the Durbin-Watson test. The test is based on the
statistic
\begin{equation}
D=\frac{\sum_{i=2}^{n}(E_{i}-E_{i-1})^{2}}{\sum_{j=1}^{n}E_{j}^{2}}.
\end{equation}
Exact critical values are difficult to obtain, but \(\mathsf{R}\) will
calculate the /p-value/ to great accuracy. It is performed with the
=dwtest= function from the =lmtest= package \cite{lmtest}. We will
conduct a two sided test that the correlation is not zero, which is
not the default (the default is to test that the autocorrelation is
positive).

#+BEGIN_SRC R :exports both :results output pp 
dwtest(cars.lm, alternative = "two.sided")
#+END_SRC

#+RESULTS:
: 
: 	Durbin-Watson test
: 
: data:  cars.lm
: DW = 1.6762, p-value = 0.1904
: alternative hypothesis: true autocorrelation is not 0

In this case we do not reject the null hypothesis at the
\(\alpha=0.05\) significance level; there is very little evidence of
nonzero autocorrelation in the residuals.

*** Remedial Measures

We often find problems with our model that suggest that at least one
of the three regression assumptions is violated. What do we do then?
There are many measures at the statistician's disposal, and we mention
specific steps one can take to improve the model under certain types
of violation.

- Mean response is not linear :: We can directly modify the model to
     better approximate the mean response. In particular, perhaps a
     polynomial regression function of the form \[ \mu(x) =
     \beta_{0} + \beta_{1}x_{1} + \beta_{2}x_{1}^{2} \] would be
     appropriate. Alternatively, we could have a function of the form
     \[ \mu(x)=\beta_{0}\mathrm{e}^{\beta_{1}x}.  \] Models like these
     are studied in nonlinear regression courses.
- Error variance is not constant :: Sometimes a transformation of the
     dependent variable will take care of the problem. There is a
     large class of them called /Box-Cox transformations/. They take
     the form
     \begin{equation}
     Y^{\ast}=Y^{\lambda},
     \end{equation}
     where \(\lambda\) is a constant. (The method proposed by Box and
     Cox will determine a suitable value of \(\lambda\) automatically
     by maximum likelihood). The class contains the transformations
     @@latex:\begin{alignat*}{1} \lambda=2,\quad &
     Y^{\ast}=Y^{2}\\ \lambda=0.5,\quad &
     Y^{\ast}=\sqrt{Y}\\ \lambda=0,\quad & Y^{\ast}=\ln\:
     Y\\ \lambda=-1,\quad & Y^{\ast}= 1/Y \end{alignat*}@@
     Alternatively, we can use the method of /weighted least
     squares/. This is studied in more detail in later classes.
- Error distribution is not normal :: The same transformations for
     stabilizing the variance are equally appropriate for smoothing
     the residuals to a more Gaussian form. In fact, often we will
     kill two birds with one stone.
- Errors are not independent :: There is a large class of
     autoregressive models to be used in this situation which occupy
     the latter part of Chapter [[#cha-Time-Series]].

** Other Diagnostic Tools
:PROPERTIES:
:CUSTOM_ID: sec-Other-Diagnostic-Tools-SLR
:END:

There are two types of observations with which we must be especially
careful:
- Influential observations :: are those that have a substantial effect
     on our estimates, predictions, or inferences. A small change in
     an influential observation is followed by a large change in the
     parameter estimates or inferences.
- Outlying observations :: are those that fall fall far from the rest
     of the data. They may be indicating a lack of fit for our
     regression model, or they may just be a mistake or typographical
     error that should be corrected. Regardless, special attention
     should be given to these observations. An outlying observation
     may or may not be influential.

We will discuss outliers first because the notation builds
sequentially in that order.
*** Outliers
There are three ways that an observation \((x_{i},y_{i})\) might be
identified as an outlier: it can have an \(x_{i}\) value which falls
far from the other \(x\) values, it can have a \(y_{i}\) value which
falls far from the other \(y\) values, or it can have both its
\(x_{i}\) and \(y_{i}\) values falling far from the other \(x\) and
\(y\) values.
*** Leverage
Leverage statistics are designed to identify observations which have
\(x\) values that are far away from the rest of the data. In the
simple linear regression model the leverage of \(x_{i}\) is denoted by
\(h_{ii}\) and defined by
\begin{equation}
h_{ii}=\frac{1}{n}+\frac{(x_{i}-\overline{x})^{2}}{\sum_{k=1}^{n}(x_{k}-\overline{x})^{2}},\quad i=1,2,\ldots,n.
\end{equation}
The formula has a nice interpretation in the SLR model: if the
distance from \(x_{i}\) to \(\overline{x}\) is large relative to the
other \(x\)'s then \(h_{ii}\) will be close to 1.

Leverages have nice mathematical properties; for example, they satisfy
\begin{equation}
\label{eq-slr-leverage-between}
0\leq h_{ii}\leq1,
\end{equation}
and their sum is
\begin{eqnarray}
\label{eq-slr-average-leverage}
\sum_{i=1}^{n}h_{ii} & = & \sum_{i=1}^{n}\left[\frac{1}{n}+\frac{(x_{i}-\overline{x})^{2}}{\sum_{k=1}^{n}(x_{k}-\overline{x})^{2}}\right],\\
 & = & \frac{n}{n}+\frac{\sum_{i}(x_{i}-\overline{x})^{2}}{\sum_{k}(x_{k}-\overline{x})^{2}},\\
 & = & 2.
\end{eqnarray}

A rule of thumb is to consider leverage values to be large if they are
more than double their average size (which is \(2/n\) according to
Equation \eqref{eq-slr-average-leverage}). So leverages larger than \(4/n\)
are suspect. Another rule of thumb is to say that values bigger than
0.5 indicate high leverage, while values between 0.3 and 0.5 indicate
moderate leverage.

*** Standardized and Studentized Deleted Residuals

We have already encountered the /standardized residuals/ \(r_{i}\) in
Section [[#sub-Constant-Variance-Assumption]]; they are merely residuals that
have been divided by their respective standard deviations:
\begin{equation}
R_{i}=\frac{E_{i}}{S\sqrt{1-h_{ii}}},\quad i=1,2,\ldots,n.
\end{equation}
Values of \(|R_{i}| > 2\) are extreme and suggest that the observation has an outlying \(y\)-value. 

Now delete the \(i^{\mathrm{th}}\) case and fit the regression
function to the remaining \(n - 1\) cases, producing a fitted value
\(\hat{Y}_{(i)}\) with /deleted residual/
\(D_{i}=Y_{i}-\hat{Y}_{(i)}\). It is shown in later classes that
\begin{equation}
\mbox{Var $(D_{i})$}=\frac{S_{(i)}^{2}}{1-h_{ii}},\quad i=1,2,\ldots,n,
\end{equation}
so that the /studentized deleted residuals/ \(t_{i}\) defined by
\begin{equation}
\label{eq-slr-studentized-deleted-resids}
t_{i}=\frac{D_{i}}{S_{(i)}/(1-h_{ii})},\quad i=1,2,\ldots,n,
\end{equation}
have a \(\mathsf{t}(\mathtt{df}=n-3)\) distribution and we compare observed values of \(t_{i}\) to this distribution to decide whether or not an observation is extreme. 

The folklore in regression classes is that a test based on the
statistic in Equation \eqref{eq-slr-studentized-deleted-resids} can be
too liberal. A rule of thumb is if we suspect an observation to be an
outlier /before/ seeing the data then we say it is significantly
outlying if its two-tailed \(p\)-value is less than \(\alpha\), but if
we suspect an observation to be an outlier /after/ seeing the data
then we should only say it is significantly outlying if its two-tailed
\(p\)-value is less than \(\alpha/n\). The latter rule of thumb is
called the /Bonferroni approach/ and can be overly conservative for
large data sets. The responsible statistician should look at the data
and use his/her best judgement, in every case.

**** How to do it with \(\mathsf{R}\)

We can calculate the standardized residuals with the =rstandard=
function. The input is the =lm= object, which is =cars.lm=.

#+BEGIN_SRC R :exports both :results output pp 
sres <- rstandard(cars.lm)
sres[1:5]
#+END_SRC

#+RESULTS:
:          1          2          3          4          5 
:  0.2660415  0.8189327 -0.4013462  0.8132663  0.1421624

We can find out which observations have studentized residuals larger
than two with the command

#+BEGIN_SRC R :exports both :results output pp 
sres[which(abs(sres) > 2)]
#+END_SRC

#+RESULTS:
:       23       35       49 
: 2.795166 2.027818 2.919060

In this case, we see that observations 23, 35, and 49 are potential
outliers with respect to their \(y\)-value.  We can compute the
studentized deleted residuals with =rstudent=:

#+BEGIN_SRC R :exports both :results output pp 
sdelres <- rstudent(cars.lm)
sdelres[1:5]
#+END_SRC

#+RESULTS:
:          1          2          3          4          5 
:  0.2634500  0.8160784 -0.3978115  0.8103526  0.1407033

We should compare these values with critical values from a
\(\mathsf{t}(\mathtt{df}=n-3)\) distribution, which in this case is
\(\mathsf{t}(\mathtt{df}=50-3=47)\). We can calculate a 0.005 quantile
and check with

#+BEGIN_SRC R :exports both :results output pp 
t0.005 <- qt(0.005, df = 47, lower.tail = FALSE)
sdelres[which(abs(sdelres) > t0.005)]
#+END_SRC

#+RESULTS:
:       23       49 
: 3.022829 3.184993

This means that observations 23 and 49 have a large studentized
deleted residual. The leverages can be found with the =hatvalues=
function:

#+BEGIN_SRC R :exports both :results output pp 
leverage <- hatvalues(cars.lm)
leverage[which(leverage > 4/50)]
#+END_SRC

#+RESULTS:
:          1          2         50 
: 0.11486131 0.11486131 0.08727007

Here we see that observations 1, 2, and 50 have leverages bigger than
double their mean value. These observations would be considered
outlying with respect to their \(x\) value (although they may or may
not be influential).

*** Influential Observations

**** \(DFBETAS\) and \(DFFITS\)

Any time we do a statistical analysis, we are confronted with the
variability of data. It is always a concern when an observation plays
too large a role in our regression model, and we would not like or
procedures to be overly influenced by the value of a single
observation. Hence, it becomes desirable to check to see how much our
estimates and predictions would change if one of the observations were
not included in the analysis. If an observation changes the
estimates/predictions a large amount, then the observation is
influential and should be subjected to a higher level of scrutiny.

We measure the change in the parameter estimates as a result of
deleting an observation with \(DFBETAS\). The \(DFBETAS\) for the
intercept \(b_{0}\) are given by
\begin{equation}
(DFBETAS)_{0(i)}=\frac{b_{0}-b_{0(i)}}{S_{(i)}\sqrt{\frac{1}{n}+\frac{\overline{x}^{2}}{\sum_{i=1}^{n}(x_{i}-\overline{x})^{2}}}},\quad i=1,2,\ldots,n.
\end{equation}
and the \(DFBETAS\) for the slope \(b_{1}\) are given by
\begin{equation}
(DFBETAS)_{1(i)}=\frac{b_{1}-b_{1(i)}}{S_{(i)}\left[\sum_{i=1}^{n}(x_{i}-\overline{x})^{2}\right]^{-1/2}},\quad i=1,2,\ldots,n.
\end{equation}
See Section [[#sec-Residual-Analysis-MLR]] for a better way to write these. The
signs of the \(DFBETAS\) indicate whether the coefficients would
increase or decrease as a result of including the observation. If the
\(DFBETAS\) are large, then the observation has a large impact on
those regression coefficients. We label observations as suspicious if
their \(DFBETAS\) have magnitude greater 1 for small data or
\(2/\sqrt{n}\) for large data sets.  We can calculate the \(DFBETAS\)
with the =dfbetas= function (some output has been omitted):

#+BEGIN_SRC R :exports both :results output pp 
dfb <- dfbetas(cars.lm)
head(dfb)
#+END_SRC

#+RESULTS:
:   (Intercept)       speed
: 1  0.09440188 -0.08624563
: 2  0.29242487 -0.26715961
: 3 -0.10749794  0.09369281
: 4  0.21897614 -0.19085472
: 5  0.03407516 -0.02901384
: 6 -0.11100703  0.09174024

We see that the inclusion of the first observation slightly increases
the =Intercept= and slightly decreases the coefficient on =speed=.

We can measure the influence that an observation has on its fitted
value with \(DFFITS\). These are calculated by deleting an
observation, refitting the model, recalculating the fit, then
standardizing. The formula is
\begin{equation}
(DFFITS)_{i}=\frac{\hat{Y_{i}}-\hat{Y}_{(i)}}{S_{(i)}\sqrt{h_{ii}}},\quad i=1,2,\ldots,n.
\end{equation}
The value represents the number of standard deviations of
\(\hat{Y_{i}}\) that the fitted value \(\hat{Y_{i}}\) increases or
decreases with the inclusion of the \(i^{\textrm{th}}\)
observation. We can compute them with the =dffits= function.

#+BEGIN_SRC R :exports both :results output pp
dff <- dffits(cars.lm)
dff[1:5]
#+END_SRC

#+RESULTS:
:           1           2           3           4           5 
:  0.09490289  0.29397684 -0.11039550  0.22487854  0.03553887

A rule of thumb is to flag observations whose \(DFFIT\) exceeds one in
absolute value, but there are none of those in this data set.

**** Cook's Distance

The \(DFFITS\) are good for measuring the influence on a single fitted
value, but we may want to measure the influence an observation has on
all of the fitted values simultaneously. The statistics used for
measuring this are Cook's distances which may be
calculated[fn:fn-cook] by the formula
\begin{equation}
\label{eq-slr-cooks-distance}
D_{i}=\frac{E_{i}^{2}}{(p+1)S^{2}}\cdot\frac{h_{ii}}{(1-h_{ii})^{2}},\quad i=1,2,\ldots,n.
\end{equation}
It shows that Cook's distance depends both on the residual \(E_{i}\)
and the leverage \(h_{ii}\) and in this way \(D_{i}\) contains
information about outlying \(x\) and \(y\) values.

To assess the significance of \(D\), we compare to quantiles of an
\(\mathsf{f}(\mathtt{df1}=2,\,\mathtt{df2}=n-2)\) distribution. A rule
of thumb is to classify observations falling higher than the
\(50^{\mathrm{th}}\) percentile as being extreme.

[fn:fn-cook] Cook's distances are actually defined by a different
formula than the one shown. The formula in Equation
\eqref{eq-slr-cooks-distance} is algebraically equivalent to the
defining formula and is, in the author's opinion, more transparent.

**** How to do it with \(\mathsf{R}\)

We can calculate the Cook's Distances with the =cooks.distance=
function.

#+BEGIN_SRC R :exports both :results output pp 
cooksD <- cooks.distance(cars.lm)
cooksD[1:4]
#+END_SRC

#+RESULTS:
:           1           2           3           4 
: 0.004592312 0.043513991 0.006202350 0.025467338

We can look at a plot of the Cook's distances with the command
=plot(cars.lm, which = 4)=.

#+NAME: Cooks-distance-cars
#+BEGIN_SRC R :exports both :results graphics :file fig/slr-Cooks-distance-cars.ps
plot(cars.lm, which = 4)
#+END_SRC

#+NAME: fig-Cooks-distance-cars
#+CAPTION[Cook's distances for the =cars= data]: \small Used for checking for influential and/our outlying observations. Values with large Cook's distance merit further investigation.
#+ATTR_LaTeX: :width 0.9\textwidth :placement [ht!]
#+RESULTS: Cooks-distance-cars
[[file:fig/slr-Cooks-distance-cars.ps]]

Observations with the largest Cook's D values are labeled, hence we
see that observations 23, 39, and 49 are suspicious. However, we need
to compare to the quantiles of an \( \mathsf{f}(\mathtt{df1} = 2, \,
\mathtt{df2} = 48) \) distribution:

#+BEGIN_SRC R :exports both :results output pp 
F0.50 <- qf(0.5, df1 = 2, df2 = 48)
any(cooksD > F0.50)
#+END_SRC

#+RESULTS:
: [1] FALSE

We see that with this data set there are no observations with extreme
Cook's distance, after all.

*** All Influence Measures Simultaneously

We can display the result of diagnostic checking all at once in one
table, with potentially influential points displayed. We do it with
the command =influence.measures(cars.lm)=:

#+BEGIN_SRC R :exports code :eval never
influence.measures(cars.lm)
#+END_SRC

The output is a huge matrix display, which we have omitted in the
interest of brevity. A point is identified if it is classified to be
influential with respect to any of the diagnostic measures. Here we
see that observations 2, 11, 15, and 18 merit further investigation.

We can also look at all diagnostic plots at once with the commands

#+BEGIN_SRC R :exports code :eval never
plot(cars.lm)
#+END_SRC

The =par= command is used so that \(2\times 2 = 4\) plots will be
shown on the same display. The diagnostic plots for the =cars= data
are shown in Figure [[fig-Diagnostic-plots-cars]]:

#+NAME: Diagnostic-plots-cars
#+BEGIN_SRC R :exports results :results graphics :file fig/slr-Diagnostic-plots-cars.ps
par(mfrow = c(2,2))
plot(cars.lm)
par(mfrow = c(1,1))
#+END_SRC

#+NAME: fig-Diagnostic-plots-cars
#+CAPTION[Diagnostic plots for the =cars= data]: \small Diagnostic plots for the =cars= data.
#+ATTR_LaTeX: :width 0.9\textwidth :placement [ht!]
#+RESULTS: Diagnostic-plots-cars
[[file:fig/slr-Diagnostic-plots-cars.ps]]

We have discussed all of the plots except the last, which is possibly
the most interesting. It shows Residuals vs. Leverage, which will
identify outlying \(y\) values versus outlying \(x\) values. Here we
see that observation 23 has a high residual, but low leverage, and it
turns out that observations 1 and 2 have relatively high leverage but
low/moderate leverage (they are on the right side of the plot, just
above the horizontal line). Observation 49 has a large residual with a
comparatively large leverage.

We can identify the observations with the =identify= command; it
allows us to display the observation number of dots on the
plot. First, we plot the graph, then we call =identify=:

#+BEGIN_SRC R :exports code :eval never
plot(cars.lm, which = 5)          # std'd resids vs lev plot
identify(leverage, sres, n = 4)   # identify 4 points
#+END_SRC

The graph with the identified points is omitted (but the plain plot is
shown in the bottom right corner of Figure
[[fig-Diagnostic-plots-cars]]). Observations 1 and 2 fall on the far right
side of the plot, near the horizontal axis.

#+LaTeX: \newpage{}

** Exercises
#+LaTeX: \setcounter{thm}{0}

#+BEGIN_xca
Prove the ANOVA equality, Equation \eqref{eq-anovaeq}. /Hint/:
show that
\[
\sum_{i=1}^{n}(Y_{i}-\hat{Y_{i}})(\hat{Y_{i}}-\overline{Y})=0.
\]
#+END_xca

#+BEGIN_xca
<<xca-find-mles-SLR>> Solve the following system of equations for
\(\beta_{1}\) and \(\beta_{0}\) to find the MLEs for slope and
intercept in the simple linear regression model.
\begin{eqnarray*}
n\beta_{0}+\beta_{1}\sum_{i=1}^{n}x_{i} & = & \sum_{i=1}^{n}y_{i}\\
\beta_{0}\sum_{i=1}^{n}x_{i}+\beta_{1}\sum_{i=1}^{n}x_{i}^{2} & = & \sum_{i=1}^{n}x_{i}y_{i}
\end{eqnarray*}
#+END_xca

#+BEGIN_xca
<<xca-show-alternate-slope-formula>> Show that the formula given in
Equation \eqref{eq-sample-correlation-formula} is equivalent to \[
\hat{\beta}_{1} =
\frac{\sum_{i=1}^{n}x_{i}y_{i}-\left.\left(\sum_{i=1}^{n}x_{i}\right)\left(\sum_{i=1}^{n}y_{i}\right)\right/
n}{\sum_{i=1}^{n}x_{i}^{2}-\left.\left(\sum_{i=1}^{n}x_{i}\right)^{2}\right/
n}.  \]
#+END_xca

* Multiple Linear Regression 						:mlr:
:PROPERTIES:
:tangle: R/12-mlr.R
:CUSTOM_ID: cha-multiple-linear-regression
:END:

#+BEGIN_SRC R :exports none :eval never
#    IPSUR: Introduction to Probability and Statistics Using R
#    Copyright (C) 2014  G. Jay Kerns
#
#    Chapter: Multiple Linear Regression
#
#    This file is part of IPSUR.
#
#    IPSUR is free software: you can redistribute it and/or modify
#    it under the terms of the GNU General Public License as published by
#    the Free Software Foundation, either version 3 of the License, or
#    (at your option) any later version.
#
#    IPSUR is distributed in the hope that it will be useful,
#    but WITHOUT ANY WARRANTY; without even the implied warranty of
#    MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
#    GNU General Public License for more details.
#
#    You should have received a copy of the GNU General Public License
#    along with IPSUR.  If not, see <http://www.gnu.org/licenses/>.
#+END_SRC

#+BEGIN_SRC R :exports none :eval no-export
# This chapter's package dependencies
library(ggplot2)
library(scatterplot3d)
library(lattice)
#+END_SRC

#+LaTeX: \noindent 
We know a lot about simple linear regression models, and a next step
is to study multiple regression models that have more than one
independent (explanatory) variable. In the discussion that follows we
will assume that we have \(p\) explanatory variables, where \(p > 1\).

The language is phrased in matrix terms -- for two reasons. First, it
is quicker to write and (arguably) more pleasant to read. Second, the
matrix approach will be required for later study of the subject; the
reader might as well be introduced to it now.

Most of the results are stated without proof or with only a cursory
justification. Those yearning for more should consult an advanced text
in linear regression for details, such as /Applied Linear Regression
Models/ \cite{Neter1996} or /Linear Models: Least Squares and
Alternatives/ \cite{Rao1999}.

*What do I want them to know?*
- the basic MLR model, and how it relates to the SLR
- how to estimate the parameters and use those estimates to make
  predictions
- basic strategies to determine whether or not the model is doing a
  good job
- a few thoughts about selected applications of the MLR, such as
  polynomial, interaction, and dummy variable models
- some of the uses of residuals to diagnose problems
- hints about what will be coming later

** The Multiple Linear Regression Model
:PROPERTIES:
:CUSTOM_ID: sec-The-MLR-Model
:END:

The first thing to do is get some better notation. We will write
\begin{equation}
\mathbf{Y}_{\mathrm{n}\times1}=
\begin{bmatrix}y_{1}\\
y_{2}\\
\vdots\\
y_{n}
\end{bmatrix},
\quad \mbox{and}\quad \mathbf{X}_{\mathrm{n}\times(\mathrm{p}+1)}=
\begin{bmatrix}1 & x_{11} & x_{21} & \cdots & x_{p1}\\
1 & x_{12} & x_{22} & \cdots & x_{p2}\\
\vdots & \vdots & \vdots & \ddots & \vdots\\
1 & x_{1n} & x_{2n} & \cdots & x_{pn}
\end{bmatrix}.
\end{equation}
The vector \(\mathbf{Y}\) is called the /response vector/
@@latex:\index{response vector}@@ and the matrix \(\mathbf{X}\) is called the
/model matrix/ @@latex:\index{model matrix}@@. As in Chapter [[#cha-simple-linear-regression]], the most general assumption that relates \(\mathbf{Y}\) to
\(\mathbf{X}\) is
\begin{equation}
\mathbf{Y}=\mu(\mathbf{X})+\upepsilon,
\end{equation}
where \(\mu\) is some function (the /signal/) and \(\upepsilon\) is
the /noise/ (everything else). We usually impose some structure on
\(\mu\) and \(\upepsilon\). In particular, the standard multiple
linear regression model @@latex:\index{model!multiple linear regression}@@
assumes
\begin{equation}
\mathbf{Y}=\mathbf{X}\upbeta+\upepsilon,
\end{equation}
where the parameter vector \(\upbeta\) looks like 
\begin{equation}
\upbeta_{(\mathrm{p}+1)\times1}=\begin{bmatrix}\beta_{0} & \beta_{1} & \cdots & \beta_{p}\end{bmatrix}^{\mathrm{T}},
\end{equation}
and the random vector
\(\upepsilon_{\mathrm{n}\times1}=\begin{bmatrix}\epsilon_{1} &
\epsilon_{2} & \cdots & \epsilon_{n}\end{bmatrix}^{\mathrm{T}}\) is
assumed to be distributed
\begin{equation}
\upepsilon\sim\mathsf{mvnorm}\left(\mathtt{mean}=\mathbf{0}_{\mathrm{n}\times1},\,\mathtt{sigma}=\sigma^{2}\mathbf{I}_{\mathrm{n}\times\mathrm{n}}\right).
\end{equation}

The assumption on \(\upepsilon\) is equivalent to the assumption that
\(\epsilon_{1}\), \(\epsilon_{2}\), ..., \(\epsilon_{n}\) are IID
\(\mathsf{norm}(\mathtt{mean}=0,\,\mathtt{sd}=\sigma)\). It is a
linear model because the quantity
\(\mu(\mathbf{X})=\mathbf{X}\upbeta\) is linear in the parameters
\(\beta_{0}\), \(\beta_{1}\), ..., \(\beta_{p}\). It may be helpful to
see the model in expanded form; the above matrix formulation is
equivalent to the more lengthy
\begin{equation} 
Y_{i}=\beta_{0}+\beta_{1}x_{1i}+\beta_{2}x_{2i}+\cdots+\beta_{p}x_{pi}+\epsilon_{i},\quad i=1,2,\ldots,n.
\end{equation}

# +BEGIN_exampletoo

*Girth, Height, and Volume for Black Cherry trees.* @@latex:\index{Data
sets!trees@\texttt{trees}}@@ Measurements were made of the girth,
height, and volume of timber in 31 felled black cherry trees. Note
that girth is the diameter of the tree (in inches) measured at 4 ft 6
in above the ground. The variables are

1. =Girth=: tree diameter in inches (denoted \(x_{1}\))
2. =Height=: tree height in feet (\(x_{2}\)).
3. =Volume=: volume of the tree in cubic feet. (\(y\))

The data are in the =datasets= package \cite{datasets} and are already
on the search path; they can be viewed with

#+BEGIN_SRC R :exports both :results output pp 
head(trees)
#+END_SRC

#+RESULTS:
:   Girth Height Volume
: 1   8.3     70   10.3
: 2   8.6     65   10.3
: 3   8.8     63   10.2
: 4  10.5     72   16.4
: 5  10.7     81   18.8
: 6  10.8     83   19.7

Let us take a look at a visual display of the data. For multiple
variables, instead of a simple scatterplot we use a scatterplot matrix
which is made with the =splom= function in the =lattice= package
\cite{lattice} as shown below. The plot is shown in Figure
[[fig-splom-trees]].

#+NAME: splom-trees
#+BEGIN_SRC R :exports both :results graphics :file fig/mlr-splom-trees.ps
splom(trees)
#+END_SRC

#+NAME: fig-splom-trees
#+CAPTION[Scatterplot matrix of =trees= data]: \small A scatterplot matrix of the =trees= data.
#+ATTR_LaTeX: :width 0.9\textwidth :placement [ht!]
#+RESULTS: splom-trees
[[file:fig/mlr-splom-trees.ps]]

The dependent (response) variable =Volume= is listed in the first row
of the scatterplot matrix. Moving from left to right, we see an
approximately linear relationship between =Volume= and the independent
(explanatory) variables =Height= and =Girth=. A first guess at a model
for these data might be
\begin{equation}
Y=\beta_{0}+\beta_{1}x_{1}+\beta_{2}x_{2}+\epsilon,
\end{equation}
in which case the quantity
\(\mu(x_{1},x_{2})=\beta_{0}+\beta_{1}x_{1}+\beta_{2}x_{2}\) would
represent the mean value of \(Y\) at the point \((x_{1},x_{2})\).
# +END_exampletoo

*** What does it mean?

The interpretation is simple. The intercept \(\beta_{0}\) represents
the mean =Volume= when all other independent variables are zero. The
parameter \(\beta_{i}\) represents the change in mean =Volume= when
there is a unit increase in \(x_{i}\), while the other independent
variable is held constant. For the =trees= data, \(\beta_{1}\)
represents the change in average =Volume= as =Girth= increases by one
unit when the =Height= is held constant, and \(\beta_{2}\) represents
the change in average =Volume= as =Height= increases by one unit when
the =Girth= is held constant.


In simple linear regression, we had one independent variable and our
linear regression surface was 1D, simply a line. In multiple
regression there are many independent variables and so our linear
regression surface will be many-D... in general, a hyperplane. But
when there are only two explanatory variables the hyperplane is just
an ordinary plane and we can look at it with a 3D scatterplot.

One way to do this is with the \(\mathsf{R}\) Commander in the =Rcmdr=
package \cite{Rcmdr}. It has a 3D scatterplot option under the
=Graphs= menu. It is especially great because the resulting graph is
dynamic; it can be moved around with the mouse, zoomed, /etc/. But
that particular display does not translate well to a printed book.

Another way to do it is with the =scatterplot3d= function in the
=scatterplot3d= package \cite{scatterplot3d}. The code follows, and
the result is shown in Figure [[fig-3D-scatterplot-trees]].

#+NAME: 3D-scatterplot-trees
#+BEGIN_SRC R :exports both :results graphics :file fig/mlr-3D-scatterplot-trees.ps
s3d <- with(trees, scatterplot3d(Girth, Height, Volume, pch = 16, 
                                 highlight.3d = TRUE, angle = 60))
fit <- lm(Volume ~ Girth + Height, data = trees)
#+END_SRC

#+NAME: fig-3D-scatterplot-trees
#+CAPTION[3D scatterplot with regression plane for the =trees= data]: \small A 3D scatterplot with regression plane for the =trees= data.
#+ATTR_LaTeX: :width 0.9\textwidth :placement [ht!]
#+RESULTS: 3D-scatterplot-trees
[[file:fig/mlr-3D-scatterplot-trees.ps]]

Looking at the graph we see that the data points fall close to a plane
in three dimensional space. (The plot looks remarkably good. In the
author's experience it is rare to see points fit so well to the plane
without some additional work.)

** Estimation and Prediction
:PROPERTIES:
:CUSTOM_ID: sec-Estimation-and-Prediction-MLR
:END:

*** Parameter estimates
:PROPERTIES:
:CUSTOM_ID: sub-mlr-parameter-estimates
:END:

We will proceed exactly like we did in Section [[#sec-SLR-Estimation]]. We know
\begin{equation}
\upepsilon\sim\mathsf{mvnorm}\left(\mathtt{mean}=\mathbf{0}_{\mathrm{n}\times1},\,\mathtt{sigma}=\sigma^{2}\mathbf{I}_{\mathrm{n}\times\mathrm{n}}\right),
\end{equation}
which means that \(\mathbf{Y}=\mathbf{X}\upbeta+\upepsilon\) has an \(\mathsf{mvnorm}\left(\mathtt{mean}=\mathbf{X}\upbeta,\,\mathtt{sigma}=\sigma^{2}\mathbf{I}_{\mathrm{n}\times\mathrm{n}}\right)\) distribution. Therefore, the likelihood function @@latex:\index{likelihood function}@@ is
\begin{equation}
L(\upbeta,\sigma)=\frac{1}{2\pi^{n/2}\sigma}\exp\left\{ -\frac{1}{2\sigma^{2}}\left(\mathbf{Y}-\mathbf{X}\upbeta\right)^{\mathrm{T}}\left(\mathbf{Y}-\mathbf{X}\upbeta\right)\right\}.
\end{equation}

To /maximize/ the likelihood @@latex:\index{maximum likelihood}@@ in \(\upbeta\),
we need to /minimize/ the quantity
\(g(\upbeta)=\left(\mathbf{Y}-\mathbf{X}\upbeta\right)^{\mathrm{T}}\left(\mathbf{Y}-\mathbf{X}\upbeta\right)\). We
do this by differentiating \(g\) with respect to \(\upbeta\). (It may
be a good idea to brush up on the material in Appendices [[#sec-Linear-Algebra]]] and [[#sec-Multivariable-Calculus]].) First we will rewrite \(g\):
\begin{equation}
g(\upbeta)=\mathbf{Y}^{\mathrm{T}}\mathbf{Y}-\mathbf{Y}^{\mathrm{T}}\mathbf{X}\upbeta-\upbeta^{\mathrm{T}}\mathbf{X}^{\mathrm{T}}\mathbf{Y}+\upbeta^{\mathrm{T}}\mathbf{X}^{\mathrm{T}}\mathbf{X}\upbeta,
\end{equation}
which can be further simplified to
\(g(\upbeta)=\mathbf{Y}^{\mathrm{T}}\mathbf{Y}-2\upbeta^{\mathrm{T}}\mathbf{X}^{\mathrm{T}}\mathbf{Y}+\upbeta^{\mathrm{T}}\mathbf{X}^{\mathrm{T}}\mathbf{X}\upbeta\)
since \(\upbeta^{\mathrm{T}}\mathbf{X}^{\mathrm{T}}\mathbf{Y}\) is
\(1\times1\) and thus equal to its transpose. Now we differentiate to
get
\begin{equation}
\frac{\partial g}{\partial\upbeta}=\mathbf{0}-2\mathbf{X}^{\mathrm{T}}\mathbf{Y}+2\mathbf{X}^{\mathrm{T}}\mathbf{X}\upbeta,
\end{equation}
since \(\mathbf{X}^{\mathrm{T}}\mathbf{X}\) is symmetric. Setting the
derivative equal to the zero vector yields the so called "normal
equations" @@latex:\index{normal equations}@@
\begin{equation}
\mathbf{X}^{\mathrm{T}}\mathbf{X}\upbeta=\mathbf{X}^{\mathrm{T}}\mathbf{Y}.
\end{equation}

In the case that \(\mathbf{X}^{\mathrm{T}}\mathbf{X}\) is
invertible[fn:fn-invert], we may solve the equation for \(\upbeta\) to
get the maximum likelihood estimator of \(\upbeta\) which we denote by
\(\mathbf{b}\):
\begin{equation}
\label{eq-b-formula-matrix}
\mathbf{b}=\left(\mathbf{X}^{\mathrm{T}}\mathbf{X}\right)^{-1}\mathbf{X}^{\mathrm{T}}\mathbf{Y}.
\end{equation}

[fn:fn-invert] We can find solutions of the normal equations even when
\(\mathbf{X}^{\mathrm{T}}\mathbf{X}\) is not of full rank, but the
topic falls outside the scope of this book. The interested reader can
consult an advanced text such as Rao \cite{Rao1999}.

#+BEGIN_rem
The formula in Equation \eqref{eq-b-formula-matrix} is convenient for
mathematical study but is inconvenient for numerical
computation. Researchers have devised much more efficient algorithms
for the actual calculation of the parameter estimates, and we do not
explore them here.
#+END_rem

#+BEGIN_rem
We have only found a critical value, and have not actually shown that
the critical value is a minimum. We omit the details and refer the
interested reader to \cite{Rao1999}.
#+END_rem

**** How to do it with \(\mathsf{R}\)

We do all of the above just as we would in simple linear
regression. The powerhouse is the =lm= @@latex:\index{lm@\texttt{lm}}@@
function. Everything else is based on it. We separate explanatory
variables in the model formula by a plus sign.

#+BEGIN_SRC R :exports both :results output pp 
trees.lm <- lm(Volume ~ Girth + Height, data = trees)
trees.lm
#+END_SRC

#+RESULTS:
: 
: Call:
: lm(formula = Volume ~ Girth + Height, data = trees)
: 
: Coefficients:
: (Intercept)        Girth       Height  
:    -57.9877       4.7082       0.3393

We see from the output that for the =trees= data our parameter
estimates are \[ \mathbf{b}=\begin{bmatrix}-58.0 & 4.7 &
0.3\end{bmatrix}, \] and consequently our estimate of the mean
response is \(\hat{\mu}\) given by
\begin{alignat}{1} 
\hat{\mu}(x_{1},x_{2}) = & \ b_{0} + b_{1} x_{1} + b_{2}x_{2},\\ \approx & -58.0 + 4.7 x_{1} + 0.3 x_{2}.
\end{alignat} 

We could see the entire model matrix \(\mathbf{X}\) with the
=model.matrix= @@latex:\index{model.matrix@\texttt{model.matrix}}@@
function, but in the interest of brevity we only show the first few
rows.

#+BEGIN_SRC R :exports both :results output pp 
head(model.matrix(trees.lm))
#+END_SRC

#+RESULTS:
:   (Intercept) Girth Height
: 1           1   8.3     70
: 2           1   8.6     65
: 3           1   8.8     63
: 4           1  10.5     72
: 5           1  10.7     81
: 6           1  10.8     83

*** Point Estimates of the Regression Surface
:PROPERTIES:
:CUSTOM_ID: sub-mlr-point-est-regsurface
:END:

The parameter estimates \(\mathbf{b}\) make it easy to find the fitted
values @@latex:\index{fitted values}@@, \(\hat{\mathbf{Y}}\). We write them
individually as \(\hat{Y}_{i}\), \(i=1,2,\ldots,n\), and recall that
they are defined by
\begin{eqnarray}
\hat{Y}_{i} & = & \hat{\mu}(x_{1i},x_{2i}),\\
 & = & b_{0}+b_{1}x_{1i}+b_{2}x_{2i},\quad i=1,2,\ldots,n.
\end{eqnarray}
They are expressed more compactly by the matrix equation
\begin{equation}
\hat{\mathbf{Y}}=\mathbf{X}\mathbf{b}.
\end{equation}
From Equation \eqref{eq-b-formula-matrix} we know that
\(\mathbf{b}=\left(\mathbf{X}^{\mathrm{T}}\mathbf{X}\right)^{-1}\mathbf{X}^{\mathrm{T}}\mathbf{Y}\),
so we can rewrite
\begin{eqnarray}
\hat{\mathbf{Y}} & = & \mathbf{X}\left[\left(\mathbf{X}^{\mathrm{T}}\mathbf{X}\right)^{-1}\mathbf{X}^{\mathrm{T}}\mathbf{Y}\right],\\
 & = & \mathbf{H}\mathbf{Y},
\end{eqnarray}
where
\(\mathbf{H}=\mathbf{X}\left(\mathbf{X}^{\mathrm{T}}\mathbf{X}\right)^{-1}\mathbf{X}^{\mathrm{T}}\)
is appropriately named /the hat matrix/ @@latex:\index{hat matrix}@@ because it
"puts the hat on \(\mathbf{Y}\)". The hat matrix is very important in
later courses. Some facts about \(\mathbf{H}\) are
- \(\mathbf{H}\) is a symmetric square matrix, of dimension
  \(\mathrm{n}\times\mathrm{n}\).
- The diagonal entries \(h_{ii}\) satisfy \(0\leq h_{ii}\leq1\)
  (compare to Equation \eqref{eq-slr-leverage-between}).
- The trace is \(\mathrm{tr}(\mathbf{H})=p\).
- \(\mathbf{H}\) is /idempotent/ (also known as a /projection matrix/)
  which means that \(\mathbf{H}^{2}=\mathbf{H}\). The same is true of
  \(\mathbf{I}-\mathbf{H}\).


Now let us write a column vector
\(\mathbf{x}_{0}=(x_{10},x_{20})^{\mathrm{T}}\) to denote given values
of the explanatory variables =Girth == \(x_{10}\) and =Height ==
\(x_{20}\). These values may match those of the collected data, or
they may be completely new values not observed in the original data
set. We may use the parameter estimates to find
\(\hat{Y}(\mathbf{x}_{0})\), which will give us
1. an estimate of \(\mu(\mathbf{x}_{0})\), the mean value of a future
   observation at \(\mathbf{x}_{0}\), and
2. a prediction for \(Y(\mathbf{x}_{0})\), the actual value of a
   future observation at \(\mathbf{x}_{0}\).

We can represent \(\hat{Y}(\mathbf{x}_{0})\) by the matrix equation
\begin{equation}
\label{eq-mlr-single-yhat-matrix}
\hat{Y}(\mathbf{x}_{0})=\mathbf{x}_{0}^{\mathrm{T}}\mathbf{b},
\end{equation}
which is just a fancy way to write
\begin{equation}
\hat{Y}(x_{10},x_{20})=b_{0}+b_{1}x_{10}+b_{2}x_{20}.
\end{equation}
 
# +BEGIN_exampletoo

If we wanted to predict the average volume of black cherry trees that
have =Girth = 15= in and are =Height = 77= ft tall then we would use
the estimate
\begin{alignat*}{1}
\hat{\mu}(15,\,77)= & -58+4.7(15)+0.3(77),\\
\approx & 35.6\mbox{\,\ ft}^{3}.
\end{alignat*}

We would use the same estimate \(\hat{Y}=35.6\) to predict the
measured =Volume= of another black cherry tree -- yet to be observed
-- that has =Girth = 15= in and is =Height = 77= ft tall.
# +END_exampletoo

**** How to do it with \(\mathsf{R}\)

The fitted values are stored inside =trees.lm= and may be accessed
with the =fitted= function. We only show the first five fitted values.

#+BEGIN_SRC R :exports both :results output pp 
fitted(trees.lm)[1:5]
#+END_SRC

#+RESULTS:
:         1         2         3         4         5 
:  4.837660  4.553852  4.816981 15.874115 19.869008

The syntax for general prediction does not change much from simple
linear regression. The computations are done with the =predict=
function as described below.

The only difference from SLR is in the way we tell \(\mathsf{R}\) the
values of the explanatory variables for which we want predictions. In
SLR we had only one independent variable but in MLR we have many (for
the =trees= data we have two). We will store values for the
independent variables in the data frame =new=, which has two columns
(one for each independent variable) and three rows (we shall make
predictions at three different locations).

#+BEGIN_SRC R :exports code :results silent 
new <- data.frame(Girth = c(9.1, 11.6, 12.5), Height = c(69, 74, 87))
#+END_SRC

We can view the locations at which we will predict:

#+BEGIN_SRC R :exports both :results output pp 
new
#+END_SRC

#+RESULTS:
:   Girth Height
: 1   9.1     69
: 2  11.6     74
: 3  12.5     87

We continue just like we would have done in SLR.

#+BEGIN_SRC R :exports both :results output pp 
predict(trees.lm, newdata = new)
#+END_SRC

#+RESULTS:
:         1         2         3 
:  8.264937 21.731594 30.379205

#+BEGIN_SRC R :exports none :results silent
treesFIT <- round(predict(trees.lm, newdata = new), 1)
#+END_SRC

# +BEGIN_exampletoo

Using the =trees= data,
1. Report a point estimate of the mean =Volume= of a tree of =Girth=
   9.1 in and =Height= 69 ft.  The fitted value for \(x_{1}=9.1\) and
   \(x_{2} = 69\) is SRC_R[:eval no-export]{treesFIT[ 1 ]} 8.3, so a point estimate
   would be SRC_R[:eval no-export]{treesFIT[ 1 ]} 8.3 cubic feet.
2. Report a point prediction for and a 95% prediction interval for the
   =Volume= of a hypothetical tree of =Girth= 12.5 in and =Height= 87
   ft.  The fitted value for \(x_{1} = 12.5\) and \(x_{2} = 87\) is
   SRC_R[:eval no-export]{treesFIT[ 3 ]} 30.4, so a point prediction for the =Volume=
   is SRC_R[:eval no-export]{treesFIT[ 3 ]} 30.4 cubic feet.
# +END_exampletoo

*** Mean Square Error and Standard Error
:PROPERTIES:
:CUSTOM_ID: sub-mlr-mse-se
:END:

The residuals are given by
\begin{equation}
\mathbf{E}=\mathbf{Y}-\hat{\mathbf{Y}}=\mathbf{Y}-\mathbf{H}\mathbf{Y}=(\mathbf{I}-\mathbf{H})\mathbf{Y}.
\end{equation}
Now we can use Theorem [[thm-mvnorm-dist-matrix-prod]] to see that the
residuals are distributed
\begin{equation}
\mathbf{E}\sim\mathsf{mvnorm}(\mathtt{mean}=\mathbf{0},\,\mathtt{sigma}=\sigma^{2}(\mathbf{I}-\mathbf{H})),
\end{equation}
since
\((\mathbf{I}-\mathbf{H})\mathbf{X}\upbeta=\mathbf{X}\upbeta-\mathbf{X}\upbeta=\mathbf{0}\)
and
\((\mathbf{I}-\mathbf{H})\,(\sigma^{2}\mathbf{I})\,(\mathbf{I}-\mathbf{H})^{\mathrm{T}}=\sigma^{2}(\mathbf{I}-\mathbf{H})^{2}=\sigma^{2}(\mathbf{I}-\mathbf{H})\). Thesum
of squared errors \(SSE\) is just
\begin{equation}
SSE=\mathbf{E}^{\mathrm{T}}\mathbf{E}=\mathbf{Y}^{\mathrm{T}}(\mathbf{I}-\mathbf{H})(\mathbf{I}-\mathbf{H})\mathbf{Y}=\mathbf{Y}^{\mathrm{T}}(\mathbf{I}-\mathbf{H})\mathbf{Y}.
\end{equation}
Recall that in SLR we had two parameters (\(\beta_{0}\) and
\(\beta_{1}\)) in our regression model and we estimated \(\sigma^{2}\)
with \(s^{2}=SSE/(n-2)\). In MLR, we have \(p+1\) parameters in our
regression model and we might guess that to estimate \(\sigma^{2}\) we
would use the /mean square error/ \(S^{2}\) defined by
\begin{equation}
S^{2}=\frac{SSE}{n-(p+1)}.
\end{equation}
That would be a good guess. The /residual standard error/ is
\(S=\sqrt{S^{2}}\).

**** How to do it with \(\mathsf{R}\)

The residuals are also stored with =trees.lm= and may be accessed with
the =residuals= function. We only show the first five residuals.

#+BEGIN_SRC R :exports both :results output pp 
residuals(trees.lm)[1:5]
#+END_SRC

#+RESULTS:
:          1          2          3          4          5 
:  5.4623403  5.7461484  5.3830187  0.5258848 -1.0690084

The =summary= function output (shown later) lists the =Residual
Standard Error= which is just \(S=\sqrt{S^{2}}\). It is stored in the
=sigma= component of the =summary= object.

#+BEGIN_SRC R :exports both :results output pp 
treesumry <- summary(trees.lm)
treesumry$sigma
#+END_SRC

#+RESULTS:
: [1] 3.881832

For the =trees= data we find \(s \approx\) SRC_R[:eval no-export]{round(treesumry$sigma,3)}.

*** Interval Estimates of the Parameters
:PROPERTIES:
:CUSTOM_ID: sub-mlr-interval-est-params
:END:

We showed in Section [[#sub-mlr-parameter-estimates]] that
\(\mathbf{b}=\left(\mathbf{X}^{\mathrm{T}}\mathbf{X}\right)^{-1}\mathbf{X}^{\mathrm{T}}\mathbf{Y}\),
which is really just a big matrix -- namely
\(\left(\mathbf{X}^{\mathrm{T}}\mathbf{X}\right)^{-1}\mathbf{X}^{\mathrm{T}}\)
-- multiplied by \(\mathbf{Y}\). It stands to reason that the sampling
distribution of \(\mathbf{b}\) would be intimately related to the
distribution of \(\mathbf{Y}\), which we assumed to be
\begin{equation}
\mathbf{Y}\sim\mathsf{mvnorm}\left(\mathtt{mean}=\mathbf{X}\upbeta,\,\mathtt{sigma}=\sigma^{2}\mathbf{I}\right).
\end{equation}
Now recall Theorem [[thm-mvnorm-dist-matrix-prod]] that we said we were
going to need eventually (the time is now). That proposition
guarantees that
\begin{equation}
\label{eq-distn-b-mlr}
\mathbf{b}\sim\mathsf{mvnorm}\left(\mathtt{mean}=\upbeta,\,\mathtt{sigma}=\sigma^{2}\left(\mathbf{X}^{\mathrm{T}}\mathbf{X}\right)^{-1}\right),
\end{equation}
since
\begin{equation}
\mathbb{E}\mathbf{b}=\left(\mathbf{X}^{\mathrm{T}}\mathbf{X}\right)^{-1}\mathbf{X}^{\mathrm{T}}(\mathbf{X}\upbeta)=\upbeta,
\end{equation}
and
\begin{equation}
\mbox{Var}(\mathbf{b})=\left(\mathbf{X}^{\mathrm{T}}\mathbf{X}\right)^{-1}\mathbf{X}^{\mathrm{T}}(\sigma^{2}\mathbf{I})\mathbf{X}\left(\mathbf{X}^{\mathrm{T}}\mathbf{X}\right)^{-1}=\sigma^{2}\left(\mathbf{X}^{\mathrm{T}}\mathbf{X}\right)^{-1},
\end{equation}
the first equality following because the matrix
\(\left(\mathbf{X}^{\mathrm{T}}\mathbf{X}\right)^{-1}\) is symmetric.

There is a lot that we can glean from Equation \eqref{eq-distn-b-mlr}. First,
it follows that the estimator \(\mathbf{b}\) is unbiased (see Section
[[#sec-Point-Estimation-1]]). Second, the variances of \(b_{0}\), \(b_{1}\),
..., \(b_{n}\) are exactly the diagonal elements of
\(\sigma^{2}\left(\mathbf{X}^{\mathrm{T}}\mathbf{X}\right)^{-1}\),
which is completely known except for that pesky parameter
\(\sigma^{2}\). Third, we can estimate the standard error of \(b_{i}\)
(denoted \(S_{b_{i}}\)) with the mean square error \(S\) (defined in
the previous section) multiplied by the corresponding diagonal element
of \(\left(\mathbf{X}^{\mathrm{T}}\mathbf{X}\right)^{-1}\). Finally,
given estimates of the standard errors we may construct confidence
intervals for \(\beta_{i}\) with an interval that looks like
\begin{equation}
b_{i}\pm\mathsf{t}_{\alpha/2}(\mathtt{df}=n-p-1)S_{b_{i}}.
\end{equation}
The degrees of freedom for the Student's \(t\) distribution[fn:fn-tdf]
are the same as the denominator of \(S^{2}\).

[fn:fn-tdf] We are taking great leaps over the mathematical
details. In particular, we have yet to show that \(s^{2}\) has a
chi-square distribution and we have not even come close to showing
that \(b_{i}\) and \(s_{b_{i}}\) are independent. But these are
entirely outside the scope of the present book and the reader may rest
assured that the proofs await in later classes. See C.R. Rao for more.

**** How to do it with \(\mathsf{R}\)

To get confidence intervals for the parameters we need only use
=confint= @@latex:\index{confint@\texttt{confint}}@@:

#+BEGIN_SRC R :exports both :results output pp 
confint(trees.lm)
#+END_SRC

#+RESULTS:
:                    2.5 %      97.5 %
: (Intercept) -75.68226247 -40.2930554
: Girth         4.16683899   5.2494820
: Height        0.07264863   0.6058538

#+BEGIN_SRC R :exports none :results silent
treesPAR <- round(confint(trees.lm), 1)
#+END_SRC

For example, using the calculations above we say that for the
regression model =Volume ~ Girth + Height= we are 95% confident that
the parameter \(\beta_{1}\) lies somewhere in the interval
SRC_R[:eval no-export]{treesPAR[2, 1]} 4.2 to SRC_R[:eval no-export]{treesPAR[2, 2]} 5.2.

*** Confidence and Prediction Intervals

We saw in Section [[#sub-mlr-point-est-regsurface]] how to make point estimates
of the mean value of additional observations and predict values of
future observations, but how good are our estimates? We need
confidence and prediction intervals to gauge their accuracy, and lucky
for us the formulas look similar to the ones we saw in SLR.

In Equation \eqref{eq-mlr-single-yhat-matrix} we wrote \(
\hat{Y}(\mathbf{x}_{0})=\mathbf{x}_{0}^{\mathrm{T}}\mathbf{b} \), and
in Equation \eqref{eq-distn-b-mlr} we saw that
\begin{equation}
\mathbf{b}\sim\mathsf{mvnorm}\left(\mathtt{mean} = \upbeta,\,\mathtt{sigma}=\sigma^{2}\left(\mathbf{X}^{\mathrm{T}}\mathbf{X}\right)^{-1}\right).
\end{equation}
The following is therefore immediate from Theorem
[[thm-mvnorm-dist-matrix-prod]]:
\begin{equation}
\hat{Y}(\mathbf{x}_{0})\sim\mathsf{mvnorm}\left(\mathtt{mean}=\mathbf{x}_{0}^{\mathrm{T}}\upbeta,\,\mathtt{sigma}=\sigma^{2}\mathbf{x}_{0}^{\mathrm{T}}\left(\mathbf{X}^{\mathrm{T}}\mathbf{X}\right)^{-1}\mathbf{x}_{0}\right).
\end{equation}
It should be no surprise that confidence intervals for the mean value
of a future observation at the location
\(\mathbf{x}_{0}=\begin{bmatrix}x_{10} & x_{20} & \ldots &
x_{p0}\end{bmatrix}^{\mathrm{T}}\) are given by
\begin{equation}
\hat{Y}(\mathbf{x}_{0})\pm\mathsf{t}_{\alpha/2}(\mathtt{df}=n-p-1)\, S\sqrt{\mathbf{x}_{0}^{\mathrm{T}}\left(\mathbf{X}^{\mathrm{T}}\mathbf{X}\right)^{-1}\mathbf{x}_{0}}.
\end{equation}
Intuitively,
\(\mathbf{x}_{0}^{\mathrm{T}}\left(\mathbf{X}^{\mathrm{T}}\mathbf{X}\right)^{-1}\mathbf{x}_{0}\)
measures the distance of \(\mathbf{x}_{0}\) from the center of the
data. The degrees of freedom in the Student's \(t\) critical value are
\(n-(p+1)\) because we need to estimate \(p+1\) parameters.

Prediction intervals for a new observation at \(\mathbf{x}_{0}\) are
given by
\begin{equation}
\hat{Y}(\mathbf{x}_{0})\pm\mathsf{t}_{\alpha/2}(\mathtt{df}=n-p-1)\, S\sqrt{1+\mathbf{x}_{0}^{\mathrm{T}}\left(\mathbf{X}^{\mathrm{T}}\mathbf{X}\right)^{-1}\mathbf{x}_{0}}.
\end{equation}
The prediction intervals are wider than the confidence intervals, just as in Section [[#sub-slr-interval-est-regline]].

**** How to do it with \(\mathsf{R}\)

The syntax is identical to that used in SLR, with the proviso that we
need to specify values of the independent variables in the data frame
=new= as we did in Section [[#sub-slr-interval-est-regline]] (which we
repeat here for illustration).

#+BEGIN_SRC R :exports code :results silent 
new <- data.frame(Girth = c(9.1, 11.6, 12.5), Height = c(69, 74, 87))
#+END_SRC

Confidence intervals are given by

#+BEGIN_SRC R :exports both :results output pp 
predict(trees.lm, newdata = new, interval = "confidence")
#+END_SRC

#+RESULTS:
:         fit      lwr      upr
: 1  8.264937  5.77240 10.75747
: 2 21.731594 20.11110 23.35208
: 3 30.379205 26.90964 33.84877

#+BEGIN_SRC R :exports none :results silent
treesCI <- round(predict(trees.lm, newdata = new, interval = "confidence"), 1)
#+END_SRC

Prediction intervals are given by

#+BEGIN_SRC R :exports both :results output pp 
predict(trees.lm, newdata = new, interval = "prediction")
#+END_SRC

#+RESULTS:
:         fit         lwr      upr
: 1  8.264937 -0.06814444 16.59802
: 2 21.731594 13.61657775 29.84661
: 3 30.379205 21.70364103 39.05477

#+BEGIN_SRC R :exports none :results silent
treesPI <- round(predict(trees.lm, newdata = new, interval = "prediction"), 1)
#+END_SRC

As before, the interval type is decided by the =interval= argument and
the default confidence level is 95% (which can be changed with the
=level= argument).

# +BEGIN_exampletoo

Using the =trees= data, 

1. Report a 95% confidence interval for the mean =Volume= of a tree of
   =Girth= 9.1 in and =Height= 69 ft. The 95% CI is given by
   SRC_R[:eval no-export]{treesCI[1, 2]} 5.8 to SRC_R[:eval no-export]{treesCI[1, 3]} 10.8, so with 95%
   confidence the mean =Volume= lies somewhere between
   SRC_R[:eval no-export]{treesCI[1, 2]} 5.8 cubic feet and SRC_R[:eval no-export]{treesCI[1, 3]} 10.8 cubic feet.
2. Report a 95% prediction interval for the =Volume= of a hypothetical
   tree of =Girth= 12.5 in and =Height= 87 ft. The 95% prediction
   interval is given by SRC_R[:eval no-export]{treesCI[3, 2]} 26.9 to
   SRC_R[:eval no-export]{treesCI[3,3]} 33.8, so with 95% confidence we may assert that
   the hypothetical =Volume= of a tree of =Girth= 12.5 in and =Height=
   87 ft would lie somewhere between SRC_R[:eval no-export]{treesCI[3, 2]} 26.9
   cubic feet and SRC_R[:eval no-export]{treesCI[3, 3]} 33.8 feet.

# +END_exampletoo

** Model Utility and Inference
:PROPERTIES:
:CUSTOM_ID: sec-Model-Utility-and-MLR
:END:

*** Multiple Coefficient of Determination

We saw in Section [[#sub-mlr-mse-se]] that the error sum of squares \(SSE\) can be conveniently written in MLR as 
\begin{equation}
\label{eq-mlr-sse-matrix}
SSE=\mathbf{Y}^{\mathrm{T}}(\mathbf{I}-\mathbf{H})\mathbf{Y}.
\end{equation}
It turns out that there are equally convenient formulas for the total sum of squares \(SSTO\) and the regression sum of squares \(SSR\). They are:
\begin{alignat}{1}
\label{eq-mlr-ssto-matrix}
SSTO= & \mathbf{Y}^{\mathrm{T}}\left(\mathbf{I}-\frac{1}{n}\mathbf{J}\right)\mathbf{Y}
\end{alignat}
and
\begin{alignat}{1}
\label{eq-mlr-ssr-matrix}
SSR= & \mathbf{Y}^{\mathrm{T}}\left(\mathbf{H}-\frac{1}{n}\mathbf{J}\right)\mathbf{Y}.
\end{alignat}
(The matrix \(\mathbf{J}\) is defined in Appendix
[[#sec-Linear-Algebra]].) Immediately from Equations
\eqref{eq-mlr-sse-matrix}, \eqref{eq-mlr-ssto-matrix}, and
\eqref{eq-mlr-ssr-matrix} we get the /Anova Equality/
\begin{equation} 
SSTO=SSE+SSR.
\end{equation}
(See Exercise [[xca-anova-equality]].) We define the /multiple coefficient of determination/ by the formula
\begin{equation} 
R^{2}=1-\frac{SSE}{SSTO}.
\end{equation}

We interpret \(R^{2}\) as the proportion of total variation that is
explained by the multiple regression model. In MLR we must be careful,
however, because the value of \(R^{2}\) can be artificially inflated
by the addition of explanatory variables to the model, regardless of
whether or not the added variables are useful with respect to
prediction of the response variable. In fact, it can be proved that
the addition of a single explanatory variable to a regression model
will increase the value of \(R^{2}\), /no matter how worthless/ the
explanatory variable is. We could model the height of the ocean tides,
then add a variable for the length of cheetah tongues on the Serengeti
plain, and our \(R^{2}\) would inevitably increase.

This is a problem, because as the philosopher, Occam, once said:
"causes should not be multiplied beyond necessity". We address the
problem by penalizing \(R^{2}\) when parameters are added to the
model. The result is an /adjusted/ \(R^{2}\) which we denote by
\(\overline{R}^{2}\).
\begin{equation}
\overline{R}^{2}=\left(R^{2}-\frac{p}{n-1}\right)\left(\frac{n-1}{n-p-1}\right).
\end{equation}
It is good practice for the statistician to weigh both \(R^{2}\) and
\(\overline{R}^{2}\) during assessment of model utility. In many cases
their values will be very close to each other. If their values differ
substantially, or if one changes dramatically when an explanatory
variable is added, then (s)he should take a closer look at the
explanatory variables in the model.

**** How to do it with \(\mathsf{R}\)
For the =trees= data, we can get \(R^{2}\) and \(\overline{R}^{2}\)
from the =summary= output or access the values directly by name as
shown (recall that we stored the =summary= object in =treesumry=).

#+BEGIN_SRC R :exports both :results output pp 
treesumry$r.squared
#+END_SRC

#+RESULTS:
: [1] 0.94795

#+BEGIN_SRC R :exports both :results output pp 
treesumry$adj.r.squared
#+END_SRC

#+RESULTS:
: [1] 0.9442322

High values of \(R^{2}\) and \( \overline{R}^2 \) such as these
indicate that the model fits very well, which agrees with what we saw
in Figure [[fig-3D-scatterplot-trees]].

*** Overall /F/-Test
:PROPERTIES:
:CUSTOM_ID: sub-mlr-Overall-F-Test
:END:

Another way to assess the model's utility is to to test the hypothesis
\[ H_{0}:\beta_{1}=\beta_{2}=\cdots=\beta_{p}=0\mbox{ versus
}H_{1}:\mbox{ at least one $\beta_{i}\neq0$}.  \] The idea is that if
all \(\beta_{i}\)'s were zero, then the explanatory variables
\(X_{1},\ldots,X_{p}\) would be worthless predictors for the response
variable \(Y\). We can test the above hypothesis with the overall
\(F\) statistic, which in MLR is defined by
\begin{equation}
F=\frac{SSR/p}{SSE/(n-p-1)}.
\end{equation}
When the regression assumptions hold and under \(H_{0}\), it can be
shown that
\(F\sim\mathsf{f}(\mathtt{df1}=p,\,\mathtt{df2}=n-p-1)\). We reject
\(H_{0}\) when \(F\) is large, that is, when the explained variation
is large relative to the unexplained variation.

**** How to do it with \(\mathsf{R}\)

The overall \(F\) statistic and its associated /p/-value is listed at
the bottom of the =summary= output, or we can access it directly by
name; it is stored in the =fstatistic= component of the =summary=
object.

#+BEGIN_SRC R :exports both :results output pp 
treesumry$fstatistic
#+END_SRC

#+RESULTS:
:    value    numdf    dendf 
: 254.9723   2.0000  28.0000

For the =trees= data, we see that \( F = \)
SRC_R[:eval no-export]{treesumry$fstatistic[ 1 ]} 254.972337410669 with a /p/-value =<
2.2e-16=. Consequently we reject \(H_{0}\), that is, the data provide
strong evidence that not all \(\beta_{i}\)'s are zero.

*** Student's /t/ Tests
:PROPERTIES:
:CUSTOM_ID: sub-mlr-Students-t-Tests
:END:

We know that
\begin{equation}
\mathbf{b}\sim\mathsf{mvnorm}\left(\mathtt{mean}=\upbeta,\,\mathtt{sigma}=\sigma^{2}\left(\mathbf{X}^{\mathrm{T}}\mathbf{X}\right)^{-1}\right)
\end{equation}
and we have seen how to test the hypothesis \(H_{0}:\beta_{1}=\beta_{2}=\cdots=\beta_{p}=0\), but let us now consider the test
\begin{equation}
H_{0}:\beta_{i}=0\mbox{ versus }H_{1}:\beta_{i}\neq0,
\end{equation}
where \(\beta_{i}\) is the coefficient for the \(i^{\textrm{th}}\)
independent variable. We test the hypothesis by calculating a
statistic, examining it's null distribution, and rejecting \(H_{0}\)
if the /p-value/ is small. If \(H_{0}\) is rejected, then we conclude
that there is a significant relationship between \(Y\) and \(x_{i}\)
/in the regression model/ \(Y\sim(x_{1},\ldots,x_{p})\). This last
part of the sentence is very important because the significance of the
variable \(x_{i}\) sometimes depends on the presence of other
independent variables in the model[fn:fn-multic].

[fn:fn-multic] In other words, a variable might be highly significant
one moment but then fail to be significant when another variable is
added to the model. When this happens it often indicates a problem
with the explanatory variables, such as /multicollinearity/. See
Section \ref{sub-Multicollinearity}.

To test the hypothesis we go to find the sampling distribution of \(
b_{i} \), the estimator of the corresponding parameter \( \beta_{i}
\), when the null hypothesis is true. We saw in Section [[#sub-mlr-interval-est-params]] that
\begin{equation}
T_{i}=\frac{b_{i}-\beta_{i}}{S_{b_{i}}}
\end{equation}
has a Student's \(t\) distribution with \(n-(p+1)\) degrees of
freedom. (Remember, we are estimating \(p+1\) parameters.)
Consequently, under the null hypothesis \(H_{0}:\beta_{i}=0\) the
statistic \(t_{i}=b_{i}/S_{b_{i}}\) has a
\(\mathsf{t}(\mathtt{df}=n-p-1)\) distribution.

**** How to do it with \(\mathsf{R}\)

The Student's \(t\) tests for significance of the individual explanatory variables are shown in the =summary= output.

#+BEGIN_SRC R :exports both :results output pp 
treesumry
#+END_SRC

#+RESULTS:
#+BEGIN_example

Call:
lm(formula = Volume ~ Girth + Height, data = trees)

Residuals:
    Min      1Q  Median      3Q     Max 
-6.4065 -2.6493 -0.2876  2.2003  8.4847 

Coefficients:
            Estimate Std. Error t value Pr(>|t|)    
(Intercept) -57.9877     8.6382  -6.713 2.75e-07 ***
Girth         4.7082     0.2643  17.816  < 2e-16 ***
Height        0.3393     0.1302   2.607   0.0145 *  
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1

Residual standard error: 3.882 on 28 degrees of freedom
Multiple R-squared:  0.948,	Adjusted R-squared:  0.9442 
F-statistic:   255 on 2 and 28 DF,  p-value: < 2.2e-16
#+END_example

We see from the /p-values/ that there is a significant linear
relationship between =Volume= and =Girth= and between =Volume= and
=Height= in the regression model =Volume ~ Girth + Height=. Further,
it appears that the =Intercept= is significant in the aforementioned
model.

** Polynomial Regression
:PROPERTIES:
:CUSTOM_ID: sec-Polynomial-Regression
:END:

*** Quadratic Regression Model

In each of the previous sections we assumed that \(\mu\) was a linear
function of the explanatory variables. For example, in SLR we assumed
that \(\mu(x)=\beta_{0}+\beta_{1}x\), and in our previous MLR examples
we assumed \(\mu(x_{1},x_{2}) = \beta_{0}+\beta_{1}x_{1} +
\beta_{2}x_{2}\). In every case the scatterplots indicated that our
assumption was reasonable. Sometimes, however, plots of the data
suggest that the linear model is incomplete and should be modified.

#+NAME: Scatterplot-Volume-Girth-trees
#+BEGIN_SRC R :exports both :results graphics :file fig/mlr-Scatterplot-Volume-Girth-trees.ps
qplot(Girth, Volume, data = trees)
#+END_SRC

#+NAME: fig-Scatterplot-Volume-Girth-trees
#+CAPTION[Scatterplot of =Volume= versus =Girth= for the =trees= data]: \small Scatterplot of =Volume= versus =Girth= for the =trees= data.
#+ATTR_LaTeX: :width 0.9\textwidth :placement [ht!]
#+RESULTS: Scatterplot-Volume-Girth-trees
[[file:fig/mlr-Scatterplot-Volume-Girth-trees.ps]]

For example, let us examine a scatterplot of =Volume= versus =Girth= a
little more closely. See Figure [[fig-Scatterplot-Volume-Girth-trees]]. There
might be a slight curvature to the data; the volume curves ever so
slightly upward as the girth increases. After looking at the plot we
might try to capture the curvature with a mean response such as
\begin{equation}
\mu(x_{1})=\beta_{0}+\beta_{1}x_{1}+\beta_{2}x_{1}^{2}.
\end{equation}
The model associated with this choice of \(\mu\) is
\begin{equation}
Y=\beta_{0}+\beta_{1}x_{1}+\beta_{2}x_{1}^{2}+\epsilon.
\end{equation}
The regression assumptions are the same. Almost everything indeed is
the same. In fact, it is still called a "linear regression model",
since the mean response \(\mu\) is linear /in the parameters/
\(\beta_{0}\), \(\beta_{1}\), and \(\beta_{2}\).

*However, there is one important difference.* When we introduce the
squared variable in the model we inadvertently also introduce strong
dependence between the terms which can cause significant numerical
problems when it comes time to calculate the parameter
estimates. Therefore, we should usually rescale the independent
variable to have mean zero (and even variance one if we wish) *before*
fitting the model. That is, we replace the \(x_{i}\)'s with
\(x_{i}-\overline{x}\) (or \((x_{i}-\overline{x})/s\)) before fitting
the model[fn:fn-ortho].

[fn:fn-ortho] Rescaling the data gets the job done but a better way to
avoid the multicollinearity introduced by the higher order terms is
with /orthogonal polynomials/, whose coefficients are chosen just
right so that the polynomials are not correlated with each other. This
is beginning to linger outside the scope of this book, however, so we
will content ourselves with a brief mention and then stick with the
rescaling approach in the discussion that follows. A nice example of
orthogonal polynomials in action can be run with =example(cars)=.

**** How to do it with \(\mathsf{R}\)

There are multiple ways to fit a quadratic model to the variables
=Volume= and =Girth= using \(\mathsf{R}\).
1. One way would be to square the values for =Girth= and save them in
   a vector =Girthsq=. Next, fit the linear model =Volume ~ Girth +
   Girthsq=.
2. A second way would be to use the /insulate/ function in
   \(\mathsf{R}\), denoted by =I=:
   : Volume ~ Girth + I(Girth^2)
   The second method is shorter than the first but the end result is
   the same. And once we calculate and store the fitted model (in,
   say, =treesquad.lm=) all of the previous comments regarding
   \(\mathsf{R}\) apply.
3. A third and "right" way to do it is with orthogonal polynomials:
   :  Volume ~ poly(Girth, degree = 2)
   See =?poly= and =?cars= for more information. Note that we can
   recover the approach in 2 with =poly(Girth, degree = 2, raw =
   TRUE)=.

# +BEGIN_exampletoo

We will fit the quadratic model to the =trees= data and display the
results with =summary=, being careful to rescale the data before
fitting the model. We may rescale the =Girth= variable to have zero
mean and unit variance on-the-fly with the =scale= function.

#+BEGIN_SRC R :exports both :results output pp 
treesquad.lm <- lm(Volume ~ scale(Girth) + I(scale(Girth)^2), data = trees)
summary(treesquad.lm)
#+END_SRC

#+RESULTS:
#+BEGIN_example

Call:
lm(formula = Volume ~ scale(Girth) + I(scale(Girth)^2), data = trees)

Residuals:
    Min      1Q  Median      3Q     Max 
-5.4889 -2.4293 -0.3718  2.0764  7.6447 

Coefficients:
                  Estimate Std. Error t value Pr(>|t|)    
(Intercept)        27.7452     0.8161  33.996  < 2e-16 ***
scale(Girth)       14.5995     0.6773  21.557  < 2e-16 ***
I(scale(Girth)^2)   2.5067     0.5729   4.376 0.000152 ***
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1

Residual standard error: 3.335 on 28 degrees of freedom
Multiple R-squared:  0.9616,	Adjusted R-squared:  0.9588 
F-statistic: 350.5 on 2 and 28 DF,  p-value: < 2.2e-16
#+END_example

We see that the \(F\) statistic indicates the overall model including
=Girth= and =Girth^2= is significant. Further, there is strong
evidence that both =Girth= and =Girth^2= are significantly related to
=Volume=. We may examine a scatterplot together with the fitted
quadratic function using the =lines= function, which adds a line to
the plot tracing the estimated mean response.

#+NAME: Fitting-the-Quadratic
#+BEGIN_SRC R :exports both :results graphics :file fig/mlr-Fitting-the-Quadratic.ps
a <- ggplot(trees, aes(scale(Girth), Volume))
a + stat_smooth(method = lm, formula = y ~ poly(x, 2)) + geom_point()
#+END_SRC

#+NAME: fig-Fitting-the-Quadratic
#+CAPTION[Quadratic model for the =trees= data]: \small A quadratic model for the =trees= data.
#+ATTR_LaTeX: :width 0.9\textwidth :placement [ht!]
#+RESULTS: Fitting-the-Quadratic
[[file:fig/mlr-Fitting-the-Quadratic.ps]]

The plot is shown in Figure [[fig-Fitting-the-Quadratic]]. Pay attention to
the scale on the \(x\)-axis: it is on the scale of the transformed
=Girth= data and not on the original scale.

# +END_exampletoo



#+BEGIN_rem
When a model includes a quadratic term for an independent variable, it
is customary to also include the linear term in the model. The
principle is called /parsimony/. More generally, if the researcher
decides to include \(x^{m}\) as a term in the model, then (s)he should
also include all lower order terms \(x\), \(x^{2}\), ...,\(x^{m-1}\)
in the model.
#+END_rem

We do estimation/prediction the same way that we did in Section
[[#sub-mlr-point-est-regsurface]], except we do not need a =Height= column
in the dataframe =new= since the variable is not included in the
quadratic model.

#+BEGIN_SRC R :exports both :results output pp 
new <- data.frame(Girth = c(9.1, 11.6, 12.5))
predict(treesquad.lm, newdata = new, interval = "prediction")
#+END_SRC

#+RESULTS:
:        fit       lwr      upr
: 1 11.56982  4.347426 18.79221
: 2 20.30615 13.299050 27.31325
: 3 25.92290 18.972934 32.87286

The predictions and intervals are slightly different from what they
were previously. Notice that it was not necessary to rescale the
=Girth= prediction data before input to the =predict= function; the
model did the rescaling for us automatically.

#+BEGIN_rem
We have mentioned on several occasions that it is important to rescale
the explanatory variables for polynomial regression. Watch what
happens if we ignore this advice:

#+BEGIN_SRC R :exports both :results output pp 
summary(lm(Volume ~ Girth + I(Girth^2), data = trees))
#+END_SRC

#+RESULTS:
#+BEGIN_example

Call:
lm(formula = Volume ~ Girth + I(Girth^2), data = trees)

Residuals:
    Min      1Q  Median      3Q     Max 
-5.4889 -2.4293 -0.3718  2.0764  7.6447 

Coefficients:
            Estimate Std. Error t value Pr(>|t|)    
(Intercept) 10.78627   11.22282   0.961 0.344728    
Girth       -2.09214    1.64734  -1.270 0.214534    
I(Girth^2)   0.25454    0.05817   4.376 0.000152 ***
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1

Residual standard error: 3.335 on 28 degrees of freedom
Multiple R-squared:  0.9616,	Adjusted R-squared:  0.9588 
F-statistic: 350.5 on 2 and 28 DF,  p-value: < 2.2e-16
#+END_example

Now nothing is significant in the model except =Girth^2=. We could
delete the =Intercept= and =Girth= from the model, but the model would
no longer be /parsimonious/. A novice may see the output and be
confused about how to proceed, while the seasoned statistician
recognizes immediately that =Girth= and =Girth^2= are highly
correlated (see Section [[#sub-Multicollinearity]]). The only remedy to this
ailment is to rescale =Girth=, which we should have done in the first
place.

In Example [[exa-mlr-trees-poly-no-rescale]] of Section [[#sec-Partial-F-Statistic]].

*Note:* The =trees= data do not have any qualitative explanatory
variables, so we will construct one for illustrative
purposes[fn:fn-binning].  We will leave the =Girth= variable alone,
but we will replace the variable =Height= by a new variable =Tall=
which indicates whether or not the cherry tree is taller than a
certain threshold (which for the sake of argument will be the sample
median height of 76 ft). That is, =Tall= will be defined by
@@latex:\begin{equation} \mathtt{Tall} = \begin{cases} \mathtt{yes}, & \mbox{if }\mathtt{Height} > 76,\\ \mathtt{no}, & \mbox{if }\mathtt{Height}\leq 76. \end{cases} \end{equation}@@

We can construct =Tall= very quickly in \(\mathsf{R}\) with the =cut=
function:

#+BEGIN_SRC R :exports both :results output pp 
trees$Tall <- cut(trees$Height, breaks = c(-Inf, 76, Inf), 
                  labels = c("no","yes"))
trees$Tall[1:5]
#+END_SRC

#+RESULTS:
: [1] no  no  no  no  yes
: Levels: no yes

Note that =Tall= is automatically generated to be a factor with the
labels in the correct order. See =?cut= for more.

[fn:fn-binning] This procedure of replacing a continuous variable by a
discrete/qualitative one is called /binning/, and is almost /never/
the right thing to do. We are in a bind at this point, however,
because we have invested this chapter in the =trees= data and I do not
want to switch mid-discussion. I am currently searching for a data set
with pre-existing qualitative variables that also conveys the same
points present in the trees data, and when I find it I will update
this chapter accordingly.

Once we have =Tall=, we include it in the regression model just like
we would any other variable. It is handled internally in a special
way. Define a "dummy variable" =Tallyes= that takes values
@@latex:\begin{equation} \mathtt{Tallyes} = \begin{cases} 1, & \mbox{if }\mathtt{Tall}=\mathtt{yes},\\ 0, & \mbox{otherwise.} \end{cases} \end{equation}@@
That is, =Tallyes= is an /indicator variable/ which indicates when a
respective tree is tall. The model may now be written as
\begin{equation}
\mathtt{Volume}=\beta_{0}+\beta_{1}\mathtt{Girth}+\beta_{2}\mathtt{Tallyes}+\epsilon.
\end{equation}
Let us take a look at what this definition does to the mean
response. Trees with =Tall = yes= will have the mean response
\begin{equation}
\mu(\mathtt{Girth})=(\beta_{0}+\beta_{2})+\beta_{1}\mathtt{Girth},
\end{equation}
while trees with =Tall = no= will have the mean response
\begin{equation} 
\mu(\mathtt{Girth})=\beta_{0}+\beta_{1}\mathtt{Girth}.
\end{equation}
In essence, we are fitting two regression lines: one for tall trees,
and one for short trees. The regression lines have the same slope but
they have different \(y\) intercepts (which are exactly
\(|\beta_{2}|\) far apart).

*** How to do it with \(\mathsf{R}\)

The important thing is to double check that the qualitative variable
in question is stored as a factor. The way to check is with the
=class= command. For example,

#+BEGIN_SRC R :exports both :results output pp 
class(trees$Tall)
#+END_SRC

#+RESULTS:
: [1] "factor"

If the qualitative variable is not yet stored as a factor then we may
convert it to one with the =factor= command. See Section
[[#sub-Qualitative-Data]]. Other than this we perform MLR as we
normally would.

#+BEGIN_SRC R :exports both :results output pp 
treesdummy.lm <- lm(Volume ~ Girth + Tall, data = trees)
summary(treesdummy.lm)
#+END_SRC

#+RESULTS:
#+begin_example

Call:
lm(formula = Volume ~ Girth + Tall, data = trees)

Residuals:
    Min      1Q  Median      3Q     Max 
-5.7788 -3.1710  0.4888  2.6737 10.0619 

Coefficients:
            Estimate Std. Error t value Pr(>|t|)    
(Intercept) -34.1652     3.2438  -10.53 3.02e-11 ***
Girth         4.6988     0.2652   17.72  < 2e-16 ***
Tall[T.yes]   4.3072     1.6380    2.63   0.0137 *  
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1

Residual standard error: 3.875 on 28 degrees of freedom
Multiple R-squared:  0.9481,	Adjusted R-squared:  0.9444 
F-statistic: 255.9 on 2 and 28 DF,  p-value: < 2.2e-16
#+end_example

From the output we see that all parameter estimates are statistically
significant and we conclude that the mean response differs for trees
with =Tall = yes= and trees with =Tall = no=.

#+BEGIN_rem
We were somewhat disingenuous when we defined the dummy variable
=Tallyes= because, in truth, \(\mathsf{R}\) defines =Tallyes=
automatically without input from the user[fn:fn-contrast]. Indeed, the
author fit the model beforehand and wrote the discussion afterward
with the knowledge of what \(\mathsf{R}\) would do so that the output
the reader saw would match what (s)he had previously read. The way
that \(\mathsf{R}\) handles factors internally is part of a much
larger topic concerning /contrasts/, which falls outside the scope of
this book. The interested reader should see Neter et al
\cite{Neter1996} or Fox \cite{Fox1997} for more.
#+END_rem

[fn:fn-contrast] That is, \(\mathsf{R}\) by default handles contrasts
according to its internal settings which may be customized by the user
for fine control. Given that we will not investigate contrasts further
in this book it does not serve the discussion to delve into those
settings, either. The interested reader should check =?contrasts= for
details.

#+BEGIN_rem
In general, if an explanatory variable =foo= is qualitative with \(n\)
levels =bar1=, =bar2=, ..., =barn= then \(\mathsf{R}\) will by default
automatically define \(n-1\) indicator variables in the following way:
@@latex:\begin{eqnarray*} \mathtt{foobar2} & = & \begin{cases} 1, & \mbox{if }\mathtt{foo}=\mathtt{"bar2"},\\ 0, & \mbox{otherwise.}\end{cases},\,\ldots,\,\mathtt{foobarn}=\begin{cases} 1, & \mbox{if }\mathtt{foo}=\mathtt{"barn"},\\ 0, & \mbox{otherwise.}\end{cases} \end{eqnarray*}@@
The level =bar1= is represented by
\(\mathtt{foobar2}=\cdots=\mathtt{foobarn}=0\). We just need to make
sure that =foo= is stored as a factor and \(\mathsf{R}\) will take
care of the rest.
#+END_rem

*** Graphing the Regression Lines

We can see a plot of the two regression lines with the following
mouthful of code.

#+NAME: dummy-variable-trees
#+BEGIN_SRC R :exports both :results graphics :file fig/mlr-dummy-variable-trees.ps
treesTall <- split(trees, trees$Tall)
treesTall[["yes"]]$Fit <- predict(treesdummy.lm, treesTall[["yes"]])
treesTall[["no"]]$Fit <- predict(treesdummy.lm, treesTall[["no"]])
plot(Volume ~ Girth, data = trees)
points(Volume ~ Girth, data = treesTall[["yes"]], pch = 1)
points(Volume ~ Girth, data = treesTall[["no"]], pch = 2)
lines(Fit ~ Girth, data = treesTall[["yes"]])
lines(Fit ~ Girth, data = treesTall[["no"]])
#+END_SRC

#+NAME: fig-dummy-variable-trees
#+CAPTION[A dummy variable model for the =trees= data]: \small A dummy variable model for the =trees= data.
#+ATTR_LaTeX: :width 0.9\textwidth :placement [ht!]
#+RESULTS: dummy-variable-trees
[[file:fig/mlr-dummy-variable-trees.ps]]

It may look intimidating but there is reason to the madness. First we
=split= the =trees= data into two pieces, with groups determined by
the =Tall= variable. Next we add the fitted values to each piece via
=predict=. Then we set up a =plot= for the variables =Volume= versus
=Girth=, but we do not plot anything yet (=type = n=) because we want
to use different symbols for the two groups. Next we add =points= to
the plot for the =Tall = yes= trees and use an open circle for a plot
character (=pch = 1=), followed by =points= for the =Tall = no= trees
with a triangle character (=pch = 2=). Finally, we add regression
=lines= to the plot, one for each group.

There are other -- shorter -- ways to plot regression lines by groups,
namely the =scatterplot= function in the =car= package \cite{car} and
the =xyplot= function in the =lattice= package \cite{lattice}. We
elected to introduce the reader to the above approach since many
advanced plots in \(\mathsf{R}\) are done in a similar, consecutive
fashion.

** Partial /F/ Statistic
:PROPERTIES:
:CUSTOM_ID: sec-Partial-F-Statistic
:END:

We saw in Section [[#sub-mlr-Overall-F-Test]] how to test
\(H_{0}:\beta_{0}=\beta_{1}=\cdots=\beta_{p}=0\) with the overall
\(F\) statistic and we saw in Section [[#sub-mlr-Students-t-Tests]] how to
test \(H_{0}:\beta_{i}=0\) that a particular coefficient \(\beta_{i}\)
is zero. Sometimes, however, we would like to test whether a certain
part of the model is significant. Consider the regression model
\begin{equation}
Y=\beta_{0}+\beta_{1}x_{1}+\cdots+\beta_{j}x_{j}+\beta_{j+1}x_{j+1}+\cdots+\beta_{p}x_{p}+\epsilon,
\end{equation}
where \(j\geq1\) and \(p\geq2\). Now we wish to test the hypothesis
\begin{equation}
H_{0}:\beta_{j+1}=\beta_{j+2}=\cdots=\beta_{p}=0
\end{equation}
versus the alternative 
\begin{equation}
H_{1}:\mbox{at least one of $\beta_{j+1},\ \beta_{j+2},\ ,\ldots,\beta_{p}\neq0$}.
\end{equation}

The interpretation of \(H_{0}\) is that none of the variables
\(x_{j+1}\), ...,\(x_{p}\) is significantly related to \(Y\) and the
interpretation of \(H_{1}\) is that at least one of \(x_{j+1}\),
...,\(x_{p}\) is significantly related to \(Y\). In essence, for this
hypothesis test there are two competing models under consideration:
\begin{align}
\mbox{the full model:} & \quad y=\beta_{0}+\beta_{1}x_{1}+\cdots+\beta_{p}x_{p}+\epsilon,\\
\mbox{the reduced model:} & \quad y=\beta_{0}+\beta_{1}x_{1}+\cdots+\beta_{j}x_{j}+\epsilon,
\end{align}

Of course, the full model will always explain the data /better/ than
the reduced model, but does the full model explain the data
/significantly better/ than the reduced model? This question is
exactly what the partial \(F\) statistic is designed to answer.

We first calculate \(SSE_{f}\), the unexplained variation in the full
model, and \(SSE_{r}\), the unexplained variation in the reduced
model. We base our test on the difference \(SSE_{r}-SSE_{f}\) which
measures the reduction in unexplained variation attributable to the
variables \(x_{j+1}\), ..., \(x_{p}\). In the full model there are
\(p+1\) parameters and in the reduced model there are \(j+1\)
parameters, which gives a difference of \(p-j\) parameters (hence
degrees of freedom). The partial /F/ statistic is
\begin{equation}
F=\frac{(SSE_{r}-SSE_{f})/(p-j)}{SSE_{f}/(n-p-1)}.
\end{equation}
It can be shown when the regression assumptions hold under \(H_{0}\)
that the partial \(F\) statistic has an
\(\mathsf{f}(\mathtt{df1}=p-j,\,\mathtt{df2}=n-p-1)\) distribution. We
calculate the \(p\)-value of the observed partial \(F\) statistic and
reject \(H_{0}\) if the \(p\)-value is small.

*** How to do it with \(\mathsf{R}\)

The key ingredient above is that the two competing models are /nested/
in the sense that the reduced model is entirely contained within the
complete model. The way to test whether the improvement is significant
is to compute =lm= objects both for the complete model and the reduced
model then compare the answers with the =anova= function.

# +BEGIN_exampletoo
<<exa-mlr-trees-poly-no-rescale>> For the =trees= data, let us fit a
polynomial regression model and for the sake of argument we will
ignore our own good advice and fail to rescale the explanatory
variables.

#+BEGIN_SRC R :exports both :results output pp 
treesfull.lm <- lm(Volume ~ Girth + I(Girth^2) + Height + 
                   I(Height^2), data = trees)
summary(treesfull.lm)
#+END_SRC

#+RESULTS:
#+BEGIN_example

Call:
lm(formula = Volume ~ Girth + I(Girth^2) + Height + I(Height^2), 
    data = trees)

Residuals:
   Min     1Q Median     3Q    Max 
-4.368 -1.670 -0.158  1.792  4.358 

Coefficients:
             Estimate Std. Error t value Pr(>|t|)    
(Intercept) -0.955101  63.013630  -0.015    0.988    
Girth       -2.796569   1.468677  -1.904    0.068 .  
I(Girth^2)   0.265446   0.051689   5.135 2.35e-05 ***
Height       0.119372   1.784588   0.067    0.947    
I(Height^2)  0.001717   0.011905   0.144    0.886    
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1

Residual standard error: 2.674 on 26 degrees of freedom
Multiple R-squared:  0.9771,	Adjusted R-squared:  0.9735 
F-statistic:   277 on 4 and 26 DF,  p-value: < 2.2e-16
#+END_example

In this ill-formed model nothing is significant except =Girth= and
=Girth^2=. Let us continue down this path and suppose that we would
like to try a reduced model which contains nothing but =Girth= and
=Girth^2= (not even an =Intercept=). Our two models are now
\begin{align*} 
\mbox{the full model:} & \quad Y=\beta_{0}+\beta_{1}x_{1}+\beta_{2}x_{1}^{2}+\beta_{3}x_{2}+\beta_{4}x_{2}^{2}+\epsilon,\\
\mbox{the reduced model:} & \quad Y=\beta_{1}x_{1}+\beta_{2}x_{1}^{2}+\epsilon,
\end{align*}
We fit the reduced model with =lm= and store the results:

#+BEGIN_SRC R :exports code :results silent
treesreduced.lm <- lm(Volume ~ -1 + Girth + I(Girth^2), data = trees)
#+END_SRC

To delete the intercept from the model we used =-1= in the model
formula. Next we compare the two models with the =anova= function. The
convention is to list the models from smallest to largest.

#+BEGIN_SRC R :exports both :results output pp 
anova(treesreduced.lm, treesfull.lm)
#+END_SRC

#+RESULTS:
: Analysis of Variance Table
: 
: Model 1: Volume ~ -1 + Girth + I(Girth^2)
: Model 2: Volume ~ Girth + I(Girth^2) + Height + I(Height^2)
:   Res.Df    RSS Df Sum of Sq      F   Pr(>F)   
: 1     29 321.65                                
: 2     26 185.86  3    135.79 6.3319 0.002279 **
: ---
: Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1

We see from the output that the complete model is highly significant
compared to the model that does not incorporate =Height= or the
=Intercept=. We wonder (with our tongue in our cheek) if the
=Height^2= term in the full model is causing all of the trouble. We
will fit an alternative reduced model that only deletes =Height^2=.

#+BEGIN_SRC R :exports both :results output pp 
treesreduced2.lm <- lm(Volume ~ Girth + I(Girth^2) + Height, 
                       data = trees)
anova(treesreduced2.lm, treesfull.lm)
#+END_SRC

#+RESULTS:
: Analysis of Variance Table
: 
: Model 1: Volume ~ Girth + I(Girth^2) + Height
: Model 2: Volume ~ Girth + I(Girth^2) + Height + I(Height^2)
:   Res.Df    RSS Df Sum of Sq      F Pr(>F)
: 1     27 186.01                           
: 2     26 185.86  1   0.14865 0.0208 0.8865

In this case, the improvement to the reduced model that is
attributable to =Height^2= is not significant, so we can delete
=Height^2= from the model with a clear conscience. We notice that the
/p-value/ for this latest partial \(F\) test is 0.8865, which seems to
be remarkably close to the /p-value/ we saw for the univariate /t/
test of =Height^2= at the beginning of this example. In fact, the
/p-values/ are /exactly/ the same. Perhaps now we gain some insight
into the true meaning of the univariate tests.

# +END_exampletoo

** Residual Analysis and Diagnostic Tools
:PROPERTIES:
:CUSTOM_ID: sec-Residual-Analysis-MLR
:END:

We encountered many, many diagnostic measures for simple linear
regression in Sections [[#sec-Residual-Analysis-SLR]] and [[#sec-Other-Diagnostic-Tools-SLR]]. All of these are valid in multiple linear regression, too, but
there are some slight changes that we need to make for the
multivariate case. We list these below, and apply them to the trees
example.

- Shapiro-Wilk, Breusch-Pagan, Durbin-Watson: :: unchanged from SLR,
     but we are now equipped to talk about the Shapiro-Wilk test
     statistic for the residuals. It is defined by the formula
     \begin{equation}
     W=\frac{\mathbf{a}^{\mathrm{T}}\mathbf{E}^{\ast}}{\mathbf{E}^{\mathrm{T}}\mathbf{E}},
     \end{equation}
     where \(\mathbf{E}^{\ast}\) is the sorted residuals and
     \(\mathbf{a}_{1\times\mathrm{n}}\) is defined by
     \begin{equation}
     \mathbf{a}=\frac{\mathbf{m}^{\mathrm{T}}\mathbf{V}^{-1}}{\sqrt{\mathbf{m}^{\mathrm{T}}\mathbf{V}^{-1}\mathbf{V}^{-1}\mathbf{m}}},
     \end{equation}
     where \(\mathbf{m}_{\mathrm{n}\times1}\) and
     \(\mathbf{V}_{\mathrm{n}\times\mathrm{n}}\) are the mean and
     covariance matrix, respectively, of the order statistics from an
     \(\mathsf{mvnorm}\left(\mathtt{mean}=\mathbf{0},\,\mathtt{sigma}=\mathbf{I}\right)\)
     distribution.
- Leverages: :: are defined to be the diagonal entries of the hat
                matrix \(\mathbf{H}\) (which is why we called them
                \(h_{ii}\) in Section
                [[#sub-mlr-point-est-regsurface]]). The sum of the
                leverages is \(\mbox{tr}(\mathbf{H})=p+1\). One rule
                of thumb considers a leverage extreme if it is larger
                than double the mean leverage value, which is
                \(2(p+1)/n\), and another rule of thumb considers
                leverages bigger than 0.5 to indicate high leverage,
                while values between 0.3 and 0.5 indicate moderate
                leverage.
- Standardized residuals: :: unchanged. Considered extreme if
     \(|R_{i}|>2\).
- Studentized residuals: :: compared to a
     \(\mathsf{t}(\mathtt{df}=n-p-2)\) distribution.
- DFBETAS: :: The formula is generalized to
   \begin{equation}
   (DFBETAS)_{j(i)}=\frac{b_{j}-b_{j(i)}}{S_{(i)}\sqrt{c_{jj}}},\quad j=0,\ldots p,\ i=1,\ldots,n,
   \end{equation}
   where \(c_{jj}\) is the \(j^{\mathrm{th}}\) diagonal entry of
   \((\mathbf{X}^{\mathrm{T}}\mathbf{X})^{-1}\). Values
   larger than one for small data sets or \(2/\sqrt{n}\)
   for large data sets should be investigated.
- DFFITS: :: unchanged. Larger than one in absolute value is
             considered extreme.
- Cook's D: :: compared to an \(\mathsf{f}(\mathtt{df1} = p +
               1,\,\mathtt{df2} = n - p - 1)\)
               distribution. Observations falling higher than the
               50\(^{\textrm{th}}\) percentile are extreme.  Note that
               plugging the value \(p=1\) into the formulas will
               recover all of the ones we saw in Chapter
               [[#cha-simple-linear-regression]].

** Additional Topics
:PROPERTIES:
:CUSTOM_ID: sec-Additional-Topics-MLR
:END:

*** Nonlinear Regression

We spent the entire chapter talking about the =trees= data, and all of
our models looked like =Volume ~ Girth + Height= or a variant of this
model. But let us think again: we know from elementary school that the
volume of a rectangle is \(V=lwh\) and the volume of a cylinder (which
is closer to what a black cherry tree looks like) is
\begin{equation}
V=\pi r^{2}h\quad \mbox{or}\quad V=4\pi dh,
\end{equation}
where \(r\) and \(d\) represent the radius and diameter of the tree,
respectively. With this in mind, it would seem that a more appropriate
model for \(\mu\) might be
\begin{equation}
\label{eq-trees-nonlin-reg}
\mu(x_{1},x_{2})=\beta_{0}x_{1}^{\beta_{1}}x_{2}^{\beta_{2}},
\end{equation}
where \(\beta_{1}\) and \(\beta_{2}\) are parameters to adjust for the
fact that a black cherry tree is not a perfect cylinder.

How can we fit this model? The model is not linear in the parameters
any more, so our linear regression methods will not work... or will
they? In the =trees= example we may take the logarithm of both sides
of Equation \eqref{eq-trees-nonlin-reg} to get
\begin{equation}
\mu^{\ast}(x_{1},x_{2})=\ln\left[\mu(x_{1},x_{2})\right]=\ln\beta_{0}+\beta_{1}\ln x_{1}+\beta_{2}\ln x_{2},
\end{equation}
and this new model \(\mu^{\ast}\) is linear in the parameters
\(\beta_{0}^{\ast}=\ln\beta_{0}\), \(\beta_{1}^{\ast}=\beta_{1}\) and
\(\beta_{2}^{\ast}=\beta_{2}\). We can use what we have learned to fit
a linear model =log(Volume) ~ log(Girth) + log(Height)=, and
everything will proceed as before, with one exception: we will need to
be mindful when it comes time to make predictions because the model
will have been fit on the log scale, and we will need to transform our
predictions back to the original scale (by exponentiating with =exp=)
to make sense.

#+BEGIN_SRC R :exports both :results output pp 
treesNonlin.lm <- lm(log(Volume) ~ log(Girth) + log(Height), data = trees)
summary(treesNonlin.lm)
#+END_SRC

#+RESULTS:
#+BEGIN_example

Call:
lm(formula = log(Volume) ~ log(Girth) + log(Height), data = trees)

Residuals:
      Min        1Q    Median        3Q       Max 
-0.168561 -0.048488  0.002431  0.063637  0.129223 

Coefficients:
            Estimate Std. Error t value Pr(>|t|)    
(Intercept) -6.63162    0.79979  -8.292 5.06e-09 ***
log(Girth)   1.98265    0.07501  26.432  < 2e-16 ***
log(Height)  1.11712    0.20444   5.464 7.81e-06 ***
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1

Residual standard error: 0.08139 on 28 degrees of freedom
Multiple R-squared:  0.9777,	Adjusted R-squared:  0.9761 
F-statistic: 613.2 on 2 and 28 DF,  p-value: < 2.2e-16
#+END_example

This is our best model yet (judging by \(R^{2}\) and
\(\overline{R}^{2}\)), all of the parameters are significant, it is
simpler than the quadratic or interaction models, and it even makes
theoretical sense. It rarely gets any better than that.

We may get confidence intervals for the parameters, but remember that
it is usually better to transform back to the original scale for
interpretation purposes:

#+BEGIN_SRC R :exports both :results output pp 
exp(confint(treesNonlin.lm))
#+END_SRC

#+RESULTS:
:                    2.5 %      97.5 %
: (Intercept) 0.0002561078 0.006783093
: log(Girth)  6.2276411645 8.468066317
: log(Height) 2.0104387829 4.645475188

(Note that we did not update the row labels of the matrix to show that
we exponentiated and so they are misleading as written.) We do
predictions just as before. Remember to transform the response
variable back to the original scale after prediction.

#+BEGIN_SRC R :exports both :results output pp 
new <- data.frame(Girth = c(9.1, 11.6, 12.5), Height = c(69, 74, 87))
exp(predict(treesNonlin.lm, newdata = new, interval = "confidence"))
#+END_SRC

#+RESULTS:
:        fit      lwr      upr
: 1 11.90117 11.25908 12.57989
: 2 20.82261 20.14652 21.52139
: 3 28.93317 27.03755 30.96169

The predictions and intervals are slightly different from those
calculated earlier, but they are close. Note that we did not need to
transform the =Girth= and =Height= arguments in the dataframe
=new=. All transformations are done for us automatically.

*** Real Nonlinear Regression

We saw with the =trees= data that a nonlinear model might be more
appropriate for the data based on theoretical considerations, and we
were lucky because the functional form of \(\mu\) allowed us to take
logarithms to transform the nonlinear model to a linear one. The same
trick will not work in other circumstances, however. We need
techniques to fit general models of the form
\begin{equation}
\mathbf{Y}=\mu(\mathbf{X})+\epsilon,
\end{equation}
where \(\mu\) is some crazy function that does not lend itself to
linear transformations.

There are a host of methods to address problems like these which are
studied in advanced regression classes. The interested reader should
see Neter /et al/ \cite{Neter1996} or Tabachnick and Fidell
\cite{Tabachnick2006}.

It turns out that John Fox has posted an Appendix to his book
\cite{Fox2002} which discusses some of the methods and issues
associated with nonlinear regression; see [[http://cran.r-project.org/doc/contrib/Fox-Companion/appendix.html][here]] for more.  Here is an
example of how it works, based on a question from R-help.

#+BEGIN_SRC R :exports both :results output pp
# fake data 
set.seed(1) 
x <- seq(from = 0, to = 1000, length.out = 200) 
y <- 1 + 2*(sin((2*pi*x/360) - 3))^2 + rnorm(200, sd = 2)
# plot(x, y)
acc.nls <- nls(y ~ a + b*(sin((2*pi*x/360) - c))^2, 
               start = list(a = 0.9, b = 2.3, c = 2.9))
summary(acc.nls)
#plot(x, fitted(acc.nls))
#+END_SRC

#+RESULTS:
#+BEGIN_example

Formula: y ~ a + b * (sin((2 * pi * x/360) - c))^2

Parameters:
  Estimate Std. Error t value Pr(>|t|)    
a  0.95884    0.23097   4.151 4.92e-05 ***
b  2.22868    0.37114   6.005 9.07e-09 ***
c  3.04343    0.08434  36.084  < 2e-16 ***
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1

Residual standard error: 1.865 on 197 degrees of freedom

Number of iterations to convergence: 3 
Achieved convergence tolerance: 6.508e-08
#+END_example

*** Multicollinearity
:PROPERTIES:
:CUSTOM_ID: sub-Multicollinearity
:END:

A multiple regression model exhibits /multicollinearity/ when two or
more of the explanatory variables are substantially correlated with
each other. We can measure multicollinearity by having one of the
explanatory play the role of "dependent variable" and regress it on
the remaining explanatory variables. The the \(R^{2}\) of the
resulting model is near one, then we say that the model is
multicollinear or shows multicollinearity.

Multicollinearity is a problem because it causes instability in the
regression model. The instability is a consequence of redundancy in
the explanatory variables: a high \(R^{2}\) indicates a strong
dependence between the selected independent variable and the
others. The redundant information inflates the variance of the
parameter estimates which can cause them to be statistically
insignificant when they would have been significant otherwise. To wit,
multicollinearity is usually measured by what are called /variance
inflation factors/.

Once multicollinearity has been diagnosed there are several approaches
to remediate it. Here are a couple of important ones.
- Principal Components Analysis. :: This approach casts out two or
     more of the original explanatory variables and replaces them with
     new variables, derived from the original ones, that are by design
     uncorrelated with one another. The redundancy is thus eliminated
     and we may proceed as usual with the new variables in
     hand. Principal Components Analysis is important for other
     reasons, too, not just for fixing multicollinearity problems.
- Ridge Regression. :: The idea of this approach is to replace the
     original parameter estimates with a different type of parameter
     estimate which is more stable under multicollinearity. The
     estimators are not found by ordinary least squares but rather a
     different optimization procedure which incorporates the variance
     inflation factor information.

We decided to omit a thorough discussion of multicollinearity because
we are not equipped to handle the mathematical details. Perhaps the
topic will receive more attention in a later edition.

- What to do when data are not normal
   - Bootstrap (see Chapter [[#cha-resampling-methods]]).

*** Akaike's Information Criterion

\[
AIC = -2\ln L + 2(p + 1)
\]

#+LaTeX: \newpage{}

** Exercises

#+LaTeX: \setcounter{thm}{0}

#+BEGIN_xca
<<xca-anova-equality>> Use Equations \eqref{eq-mlr-sse-matrix},
\eqref{eq-mlr-ssto-matrix}, and \eqref{eq-mlr-ssr-matrix} to prove the
Anova Equality: \[ SSTO = SSE + SSR.\]
#+END_xca

* Resampling Methods                                                 :resamp:
:PROPERTIES:
:tangle: R/13-resamp.R
:CUSTOM_ID: cha-resampling-methods
:END:

#+BEGIN_SRC R :exports none :eval never
#    IPSUR: Introduction to Probability and Statistics Using R
#    Copyright (C) 2014  G. Jay Kerns
#
#    Chapter: Resampling Methods
#
#    This file is part of IPSUR.
#
#    IPSUR is free software: you can redistribute it and/or modify
#    it under the terms of the GNU General Public License as published by
#    the Free Software Foundation, either version 3 of the License, or
#    (at your option) any later version.
#
#    IPSUR is distributed in the hope that it will be useful,
#    but WITHOUT ANY WARRANTY; without even the implied warranty of
#    MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
#    GNU General Public License for more details.
#
#    You should have received a copy of the GNU General Public License
#    along with IPSUR.  If not, see <http://www.gnu.org/licenses/>.
#+END_SRC

#+LaTeX: \noindent 
Computers have changed the face of statistics. Their quick
computational speed and flawless accuracy, coupled with large data
sets acquired by the researcher, make them indispensable for many
modern analyses. In particular, resampling methods (due in large part
to Bradley Efron) have gained prominence in the modern statistician's
repertoire. We first look at a classical problem to get some insight
why.

I have seen /Statistical Computing with \(\mathsf{R}\)/ by Rizzo
\cite{Rizzo2008} and I recommend it to those looking for a more
advanced treatment with additional topics. I believe that /Monte Carlo
Statistical Methods/ by Robert and Casella \cite{Robert2004} has a new
edition that integrates \(\mathsf{R}\) into the narrative.

*What do I want them to know?*
- basic philosophy of resampling and why it is important
- resampling for standard errors and confidence intervals
- resampling for hypothesis tests (permutation tests)

** Introduction
:PROPERTIES:
:CUSTOM_ID: sec-Introduction-Resampling
:END:

-  Classical question :: Given a population of interest, how may we
     effectively learn some of its salient features, /e.g./, the
     population's mean? One way is through representative random
     sampling. Given a random sample, we summarize the information
     contained therein by calculating a reasonable statistic, /e.g./,
     the sample mean. Given a value of a statistic, how do we know
     whether that value is significantly different from that which was
     expected? We don't; we look at the /sampling distribution/ of the
     statistic, and we try to make probabilistic assertions based on a
     confidence level or other consideration. For example, we may find
     ourselves saying things like, "With 95% confidence, the true
     population mean is greater than zero".
- Problem :: Unfortunately, in most cases the sampling distribution is
             /unknown/. Thus, in the past, in efforts to say something
             useful, statisticians have been obligated to place some
             restrictive assumptions on the underlying population. For
             example, if we suppose that the population has a normal
             distribution, then we can say that the distribution of
             \(\overline{X}\) is normal, too, with the same mean (and
             a smaller standard deviation). It is then easy to draw
             conclusions, make inferences, and go on about our
             business.
- Alternative :: We don't know what the underlying population
                 distributions is, so let us /estimate/ it, just like
                 we would with any other parameter. The statistic we
                 use is the /empirical CDF/, that is, the function
                 that places mass \(1/n\) at each of the observed data
                 points \(x_{1},\ldots,x_{n}\) (see Section
                 [[#sec-empirical-distribution]]). As the sample size
                 increases, we would expect the approximation to get
                 better and better (with IID observations, it does,
                 and there is a wonderful theorem by Glivenko and
                 Cantelli that proves it). And now that we have an
                 (estimated) population distribution, it is easy to
                 find the sampling distribution of any statistic we
                 like: just *sample* from the empirical CDF many, many
                 times, calculate the statistic each time, and make a
                 histogram. Done! Of course, the number of samples
                 needed to get a representative histogram is
                 prohibitively large... human beings are simply too
                 slow (and clumsy) to do this tedious procedure.

Fortunately, computers are very skilled at doing simple, repetitive
tasks very quickly and accurately. So we employ them to give us a
reasonable idea about the sampling distribution of our statistic, and
we use the generated sampling distribution to guide our inferences and
draw our conclusions. If we would like to have a better approximation
for the sampling distribution (within the confines of the information
contained in the original sample), we merely tell the computer to
sample more. In this (restricted) sense, we are limited only by our
current computational speed and pocket book.

In short, here are some of the benefits that the advent of resampling
methods has given us:
- Fewer assumptions. :: We are no longer required to assume the
     population is normal or the sample size is large (though, as
     before, the larger the sample the better).
- Greater accuracy. :: Many classical methods are based on rough upper
     bounds or Taylor expansions. The bootstrap procedures can be
     iterated long enough to give results accurate to several decimal
     places, often beating classical approximations.
- Generality. :: Resampling methods are easy to understand and apply
                 to a large class of seemingly unrelated
                 procedures. One no longer needs to memorize long
                 complicated formulas and algorithms.

#+BEGIN_rem
Due to the special structure of the empirical CDF, to get an IID
sample we just need to take a random sample of size \(n\), with
replacement, from the observed data \(x_{1},\ldots,x_{n}\). Repeats
are expected and acceptable. Since we already sampled to get the
original data, the term /resampling/ is used to describe the
procedure.
#+END_rem

*** General bootstrap procedure.

The above discussion leads us to the following general procedure to
approximate the sampling distribution of a statistic
\(S=S(x_{1},x_{2},\ldots,x_{n})\) based on an observed simple random
sample \(\mathbf{x}=(x_{1},x_{2},\ldots,x_{n})\) of size \(n\):
1. Create many many samples \(\mathbf{x}_{1}^{\ast}, \ldots,
   \mathbf{x}_{M}^{\ast}\), called /resamples/, by sampling with
   replacement from the data.
2. Calculate the statistic of interest
   \(S(\mathbf{x}_{1}^{\ast}),\ldots,S(\mathbf{x}_{M}^{\ast})\) for
   each resample. The distribution of the resample statistics is
   called a /bootstrap distribution/.
3. The bootstrap distribution gives information about the sampling
   distribution of the original statistic \(S\). In particular, the
   bootstrap distribution gives us some idea about the center, spread,
   and shape of the sampling distribution of \(S\).

** Bootstrap Standard Errors
:PROPERTIES:
:CUSTOM_ID: sec-Bootstrap-Standard-Errors
:END:

Since the bootstrap distribution gives us information about a
statistic's sampling distribution, we can use the bootstrap
distribution to estimate properties of the statistic. We will
illustrate the bootstrap procedure in the special case that the
statistic \(S\) is a standard error.

# +BEGIN_exampletoo
<<exa-Bootstrap-se-mean>> *Standard error of the mean.* In this
example we illustrate the bootstrap by estimating the standard error
of the sample meanand we will do it in the special case that the
underlying population is
\(\mathsf{norm}(\mathtt{mean}=3,\,\mathtt{sd}=1)\).

Of course, we do not really need a bootstrap distribution here because
from Section [[#sec-sampling-from-normal-dist]] we know that
\(\overline{X}\sim\mathsf{norm}(\mathtt{mean}=3,\,\mathtt{sd}=1/\sqrt{n})\),
but we proceed anyway to investigate how the bootstrap performs when
we know what the answer should be ahead of time.

We will take a random sample of size \(n=25\) from the
population. Then we will /resample/ the data 1000 times to get 1000
resamples of size 25. We will calculate the sample mean of each of the
resamples, and will study the data distribution of the 1000 values of
\(\overline{x}\).

#+BEGIN_SRC R :exports code :results silent 
srs <- rnorm(25, mean = 3)
resamps <- replicate(1000, sample(srs, 25, TRUE), simplify = FALSE)
xbarstar <- sapply(resamps, mean, simplify = TRUE)
#+END_SRC

A histogram of the 1000 values of \(\overline{x}\) is shown in Figure
[[fig-Bootstrap-se-mean]], and was produced by the following code.

#+NAME: Bootstrap-se-mean
#+BEGIN_SRC R :exports both :results graphics :file fig/resamp-Bootstrap-se-mean.ps
hist(xbarstar, breaks = 40, prob = TRUE)
curve(dnorm(x, 3, 0.2), add = TRUE) # overlay true normal density
#+END_SRC

#+NAME: fig-Bootstrap-se-mean
#+CAPTION[Bootstrapping the standard error of the mean, simulated data]: \small The original data were 25 observations generated from a \(\mathsf{norm}(\mathtt{mean}=3,\,\mathtt{sd}=1)\) distribution. We next resampled to get 1000 resamples, each of size 25, and calculated the sample mean for each resample. A histogram of the 1000 values of \(\overline{x}\) is shown above. Also shown (with a solid line) is the true sampling distribution of \(\overline{X}\), which is a \(\mathsf{norm}(\mathtt{mean}=3,\,\mathtt{sd}=0.2)\) distribution. Note that the histogram is centered at the sample mean of the original data, while the true sampling distribution is centered at the true value of \(\mu=3\). The shape and spread of the histogram is similar to the shape and spread of the true sampling distribution.
#+ATTR_LaTeX: :width 0.9\textwidth :placement [ht!]
#+RESULTS: Bootstrap-se-mean
[[file:fig/resamp-Bootstrap-se-mean.ps]]

We have overlain what we know to be the true sampling distribution of
\(\overline{X}\), namely, a
\(\mathsf{norm}(\mathtt{mean}=3,\,\mathtt{sd}=1/\sqrt{25})\)
distribution. The histogram matches the true sampling distribution
pretty well with respect to shape and spread... but notice how the
histogram is off-center a little bit. This is not a coincidence -- in
fact, it can be shown that the mean of the bootstrap distribution is
exactly the mean of the original sample, that is, the value of the
statistic that we originally observed. Let us calculate the mean of
the bootstrap distribution and compare it to the mean of the original
sample:

#+BEGIN_SRC R :exports both :results output pp 
mean(xbarstar)
mean(srs)
mean(xbarstar) - mean(srs)
#+END_SRC

#+RESULTS:
: [1] 3.05643
: [1] 3.05766
: [1] -0.001229869

# +END_exampletoo


Notice how close the two values are. The difference between them is an
estimate of how biased the original statistic is, the so-called
/bootstrap estimate of bias/. Since the estimate is so small we would
expect our original statistic (\(\overline{X}\)) to have small bias,
but this is no surprise to us because we already knew from Section
[[#sec-simple-random-samples]] that \(\overline{X}\) is an unbiased estimator
of the population mean.

Now back to our original problem, we would like to estimate the
standard error of \(\overline{X}\). Looking at the histogram, we see
that the spread of the bootstrap distribution is similar to the spread
of the sampling distribution. Therefore, it stands to reason that we
could estimate the standard error of \(\overline{X}\) with the sample
standard deviation of the resample statistics. Let us try and see.

#+BEGIN_SRC R :exports both :results output pp 
sd(xbarstar)
#+END_SRC

#+RESULTS:
: [1] 0.2134432

We know from theory that the true standard error is
\(1/\sqrt{25}=0.20\). Our bootstrap estimate is not very far from the
theoretical value.

#+BEGIN_rem
What would happen if we take more resamples? Instead of 1000
resamples, we could increase to, say, 2000, 3000, or even
4000... would it help? The answer is both yes and no. Keep in mind
that with resampling methods there are two sources of randomness: that
from the original sample, and that from the subsequent resampling
procedure. An increased number of resamples would reduce the variation
due to the second part, but would do nothing to reduce the variation
due to the first part.

We only took an original sample of size \(n=25\), and resampling more
and more would never generate more information about the population
than was already there. In this sense, the statistician is limited by
the information contained in the original sample.
#+END_rem

#+ATTR_LATEX: :options [\textbf{Standard error of the median}]
# +BEGIN_exampletoo
<<exa-Bootstrap-se-median>> We look at one where we do not know the
answer ahead of time. This example uses the =rivers=
@@latex:\index{Data sets!rivers@\texttt{rivers}}@@ data set. Recall
the stemplot on page \vpageref{ite-stemplot-rivers} that we made for
these data which shows them to be markedly right-skewed, so a natural
estimate of center would be the sample median. Unfortunately, its
sampling distribution falls out of our reach. We use the bootstrap to
help us with this problem, and the modifications to the last example
are trivial.

#+BEGIN_SRC R :exports both :results output pp 
resamps <- replicate(1000, sample(rivers, 141, TRUE), simplify = FALSE)
medstar <- sapply(resamps, median, simplify = TRUE)
sd(medstar)
#+END_SRC

#+RESULTS:
: [1] 27.60941

#+NAME: Bootstrapping-se-median
#+BEGIN_SRC R :exports both :results graphics :file fig/resamp-Bootstrapping-se-median.ps
hist(medstar, breaks = 40, prob = TRUE)
#+END_SRC

#+NAME: fig-Bootstrapping-se-median
#+CAPTION[Bootstrapping the standard error of the median for the =rivers= data]: \small Bootstrapping the standard error of the median for the =rivers= data.
#+ATTR_LaTeX: :width 0.9\textwidth :placement [ht!]
#+RESULTS: Bootstrapping-se-median
[[file:fig/resamp-Bootstrapping-se-median.ps]]

The graph is shown in Figure [[fig-Bootstrapping-se-median]], and was produced
by the following code.

#+BEGIN_SRC R :exports code :eval never
hist(medstar, breaks = 40, prob = TRUE)
#+END_SRC

#+BEGIN_SRC R :exports both :results output pp 
median(rivers)
mean(medstar)
mean(medstar) - median(rivers)
#+END_SRC

#+RESULTS:
: [1] 425
: [1] 426.47
: [1] 1.47

# +END_exampletoo


#+ATTR_LATEX: :options [\textbf{The boot package in R}]
# +BEGIN_exampletoo
It turns out that there are many bootstrap procedures and commands
already built into base \(\mathsf{R}\), in the =boot=
package. Further, inside the =boot= package \cite{boot:1} there is
even a function called =boot=
@@latex:\index{boot@\texttt{boot}}@@. The basic syntax is of the form:

:  boot(data, statistic, R)

# +END_exampletoo

Here, =data= is a vector (or matrix) containing the data to be
resampled, =statistic= is a defined function, /of two arguments/, that
tells which statistic should be computed, and the parameter
\(\mathsf{R}\) specifies how many resamples should be taken.

For the standard error of the mean (Example [[exa-Bootstrap-se-mean]]):

#+BEGIN_SRC R :exports both :results output pp 
mean_fun <- function(x, indices) mean(x[indices])
boot(data = srs, statistic = mean_fun, R = 1000)
#+END_SRC

#+RESULTS:
#+BEGIN_example

ORDINARY NONPARAMETRIC BOOTSTRAP


Call:
boot(data = srs, statistic = mean_fun, R = 1000)


Bootstrap Statistics :
    original      bias    std. error
t1*  3.05766 -0.01117741     0.21834
#+END_example

For the standard error of the median (Example
[[exa-Bootstrap-se-median]]):

#+BEGIN_SRC R :exports both :results output pp 
median_fun <- function(x, indices) median(x[indices])
boot(data = rivers, statistic = median_fun, R = 1000)
#+END_SRC

#+RESULTS:
#+BEGIN_example

ORDINARY NONPARAMETRIC BOOTSTRAP


Call:
boot(data = rivers, statistic = median_fun, R = 1000)


Bootstrap Statistics :
    original  bias    std. error
t1*      425   2.003     26.8763
#+END_example

We notice that the output from both methods of estimating the standard
errors produced similar results. In fact, the =boot= procedure is to
be preferred since it invisibly returns much more information (which
we will use later) than our naive script and it is much quicker in its
computations.

#+BEGIN_rem
Some things to keep in mind about the bootstrap:
- For many statistics, the bootstrap distribution closely resembles
  the sampling distribution with respect to spread and shape. However,
  the bootstrap will not have the same center as the true sampling
  distribution. While the sampling distribution is centered at the
  population mean (plus any bias), the bootstrap distribution is
  centered at the original value of the statistic (plus any bias). The
  =boot= function gives an empirical estimate of the bias of the
  statistic as part of its output.
- We tried to estimate the standard error, but we could have (in
  principle) tried to estimate something else. Note from the previous
  remark, however, that it would be useless to estimate the population
  mean \(\mu\) using the bootstrap since the mean of the bootstrap
  distribution is the observed \(\overline{x}\).
- You don't get something from nothing. We have seen that we can take
  a random sample from a population and use bootstrap methods to get a
  very good idea about standard errors, bias, and the like. However,
  one must not get lured into believing that by doing some random
  resampling somehow one gets more information about the parameters
  than that which was contained in the original sample. Indeed, there
  is some uncertainty about the parameter due to the randomness of the
  original sample, and there is even more uncertainty introduced by
  resampling. One should think of the bootstrap as just another
  estimation method, nothing more, nothing less.

#+END_rem

** Bootstrap Confidence Intervals
:PROPERTIES:
:CUSTOM_ID: sec-Bootstrap-Confidence-Intervals
:END:

*** Percentile Confidence Intervals

As a first try, we want to obtain a 95% confidence interval for a
parameter. Typically the statistic we use to estimate the parameter is
centered at (or at least close by) the parameter; in such cases a 95%
confidence interval for the parameter is nothing more than a 95%
confidence interval for the statistic. And to find a 95% confidence
interval for the statistic we need only go to its sampling
distribution to find an interval that contains 95% of the area. (The
most popular choice is the equal-tailed interval with 2.5% in each
tail.)

This is incredibly easy to accomplish with the bootstrap. We need only
to take a bunch of bootstrap resamples, order them, and choose the
\(\alpha/2\)th and \((1-\alpha)\)th percentiles. There is a function
=boot.ci= @@latex:\index{boot.ci@\texttt{boot.ci}}@@ in \(\mathsf{R}\) already
created to do just this. Note that in order to use the function
=boot.ci= we must first run the =boot= function and save the output in
a variable, for example, =data.boot=. We then plug =data.boot= into
the function =boot.ci=.

#+ATTR_LATEX: :options [\textbf{Percentile interval for the expected value of the median}]
# +BEGIN_exampletoo
<<exa-percentile-interval-median-first>> We will try the naive
approach where we generate the resamples and calculate the percentile
interval by hand.

#+BEGIN_SRC R :exports both :results output pp 
btsamps <- replicate(2000, sample(stack.loss, 21, TRUE), simplify = FALSE)
thetast <- sapply(btsamps, median, simplify = TRUE)
mean(thetast)
median(stack.loss)
quantile(thetast, c(0.025, 0.975))
#+END_SRC

#+RESULTS:
: [1] 14.78
: [1] 15
:  2.5% 97.5% 
:    12    18

# +END_exampletoo

#+ATTR_LATEX: :options [\textbf{Confidence interval for expected value of the median, second try}]
# +BEGIN_exampletoo
Now we will do it the right way with the =boot= function.

#+BEGIN_SRC R :exports both :results output pp 
med_fun <- function(x, ind) median(x[ind])
med_boot <- boot(stack.loss, med_fun, R = 2000)
boot.ci(med_boot, type = c("perc", "norm", "bca"))
#+END_SRC

#+RESULTS:
#+BEGIN_example
BOOTSTRAP CONFIDENCE INTERVAL CALCULATIONS
Based on 2000 bootstrap replicates

CALL : 
boot.ci(boot.out = med_boot, type = c("perc", "norm", "bca"))

Intervals : 
Level      Normal             Percentile            BCa          
95%   (11.93, 18.53 )   (12.00, 18.00 )   (11.00, 18.00 )  
Calculations and Intervals on Original Scale
#+END_example

# +END_exampletoo

*** Student's \(t\) intervals ("normal intervals")

The idea is to use confidence intervals that we already know and let
the bootstrap help us when we get into trouble. We know that a
\(100(1-\alpha)\%\) confidence interval for the mean of a \(SRS(n)\)
from a normal distribution is
\begin{equation} 
\overline{X}\pm\mathsf{t}_{\alpha/2}(\mathtt{df}=n-1)\frac{S}{\sqrt{n}},
\end{equation} 
where \(\mathsf{t}_{\alpha/2}(\mathtt{df}=n-1)\) is the appropriate
critical value from Student's \(t\) distribution, and we remember that
an estimate for the standard error of \(\overline{X}\) is
\(S/\sqrt{n}\). Of course, the estimate for the standard error will
change when the underlying population distribution is not normal, or
when we use a statistic more complicated than \(\overline{X}\). In
those situations the bootstrap will give us quite reasonable estimates
for the standard error. And as long as the sampling distribution of
our statistic is approximately bell-shaped with small bias, the
interval
\begin{equation}
\mbox{statistic}\pm\mathsf{t}_{\alpha/2}(\mathtt{df}=n-1)*\mathrm{SE}(\mbox{statistic})
\end{equation}
 will have approximately \(100(1-\alpha)\%\) confidence of containing
 \(\mathbb{E}(\mathrm{statistic})\).

# +BEGIN_exampletoo

We will use the t-interval method to find the bootstrap CI for the
median. We have looked at the bootstrap distribution; it appears to be
symmetric and approximately mound shaped. Further, we may check that
the bias is approximately 40, which on the scale of these data is
practically negligible. Thus, we may consider looking at the
\(t\)-intervals. Note that, since our sample is so large, instead of
\(t\)-intervals we will essentially be using \(z\)-intervals.
# +END_exampletoo

We see that, considering the scale of the data, the confidence
intervals compare with each other quite well.

#+BEGIN_rem
We have seen two methods for bootstrapping confidence intervals for a
statistic. Which method should we use? If the bias of the bootstrap
distribution is small and if the distribution is close to normal, then
the percentile and \(t\)-intervals will closely agree. If the
intervals are noticeably different, then it should be considered
evidence that the normality and bias conditions are not met. In this
case, /neither/ interval should be used.
#+END_rem
- \(BC_{a}\): bias-corrected and accelerated
   - transformation invariant
   - more correct and accurate
   - not monotone in coverage level?
- \(t\)-intervals
   - more natural
   - numerically unstable
- Can do things like transform scales, compute confidence intervals,
  and then transform back.
- Studentized bootstrap confidence intervals where is the Studentized
  version of is the order statistic of the simulation

** Resampling in Hypothesis Tests
:PROPERTIES:
:CUSTOM_ID: sec-Resampling-in-Hypothesis
:END:

The classical two-sample problem can be stated as follows: given two
groups of interest, we would like to know whether these two groups are
significantly different from one another or whether the groups are
reasonably similar. The standard way to decide is to
1. Go collect some information from the two groups and calculate an
   associated statistic, for example,
   \(\overline{X}_{1}-\overline{X}_{2}\).
2. Suppose that there is no difference in the groups, and find the
   distribution of the statistic in 1.
3. Locate the observed value of the statistic with respect to the
   distribution found in 2. A value in the main body of the
   distribution is not spectacular, it could reasonably have occurred
   by chance. A value in the tail of the distribution is unlikely, and
   hence provides evidence /against/ the null hypothesis that the
   population distributions are the same.

Of course, we usually compute a /p-value/, defined to be the
probability of the observed value of the statistic or more extreme
when the null hypothesis is true. Small \(p\)-values are evidence
against the null hypothesis. It is not immediately obvious how to use
resampling methods here, so we discuss an example.

# +BEGIN_exampletoo
 
A study concerned differing dosages of the antiretroviral drug
AZT. The common dosage is 300mg daily. Higher doses cause more side
affects, but are they significantly higher? We examine for a 600mg
dose. The data are as follows: We compare the scores from the two
groups by computing the difference in their sample means. The 300mg
data were entered in x1 and the 600mg data were entered into x2. The
observed difference was

| 300mg  | 284 | 279 | 289 | 292 | 287 | 295 | 285 | 279 | 306 | 298 |
| 600mg  | 298 | 307 | 297 | 279 | 291 | 335 | 299 | 300 | 306 | 291 |
The average amounts can be found:
: > mean(x1)
: [1] 289.4
: > mean(x2)
: [1] 300.3

with an observed difference of =mean(x2) - mean(x1) = 10.9=. As
expected, the 600 mg measurements seem to have a higher average, and
we might be interested in trying to decide if the average amounts are
=significantly= different. The null hypothesis should be that there is
no difference in the amounts, that is, the groups are more or less the
same. If the null hypothesis were true, then the two groups would
indeed be the same, or just one big group. In that case, the observed
difference in the sample means just reflects the random assignment
into the arbitrary =x1= and =x2= categories. It is now clear how we
may resample, consistent with the null hypothesis.
# +END_exampletoo

*** Procedure:

1. Randomly resample 10 scores from the combined scores of =x1= and
   =x2=, and assign then to the =x1= group. The rest will then be in
   the =x2= group. Calculate the difference in (re)sampled means, and
   store that value.
2. Repeat this procedure many, many times and draw a histogram of the
   resampled statistics, called the /permutation distribution/. Locate
   the observed difference 10.9 on the histogram to get the
   \(p\)-value. If the \(p\)-value is small, then we consider that
   evidence against the hypothesis that the groups are the same.

#+BEGIN_rem
In calculating the permutation test /p-value/, the formula is
essentially the proportion of resample statistics that are greater
than or equal to the observed value. Of course, this is merely an
/estimate/ of the true \(p\)-value. As it turns out, an adjustment of
\(+1\) to both the numerator and denominator of the proportion
improves the performance of the estimated \(p\)-value, and this
adjustment is implemented in the =ts.perm= function.
#+END_rem

#+BEGIN_SRC R :exports both :results output pp 
oneway_test(len ~ supp, data = ToothGrowth)
#+END_SRC

#+RESULTS:
: 
: 	Asymptotic 2-Sample Permutation Test
: 
: data:  len by supp (OJ, VC)
: Z = 1.8734, p-value = 0.06102
: alternative hypothesis: true mu is not equal to 0

*** Comparison with the Two Sample /t/ test

We know from Chapter [[#cha-Hypothesis-Testing]] to use the two-sample
\(t\)-test to tell whether there is an improvement as a result of
taking the intervention class. Note that the \(t\)-test assumes normal
underlying populations, with unknown variance, and small sample
\(n=10\). What does the \(t\)-test say? Below is the output.

#+BEGIN_SRC R :exports both :results output pp 
t.test(len ~ supp, data = ToothGrowth, 
       alt = "greater", var.equal = TRUE)
#+END_SRC

#+RESULTS:
#+BEGIN_example
 
	Two Sample t-test

data:  len by supp
t = 1.9153, df = 58, p-value = 0.0302
alternative hypothesis: true difference in means is greater than 0
95 percent confidence interval:
 0.4708204       Inf
sample estimates:
mean in group OJ mean in group VC 
        20.66333         16.96333
#+END_example

#+BEGIN_SRC R :exports none :results silent
A <- show(oneway_test(len ~ supp, data = ToothGrowth))
B <- t.test(len ~ supp, data = ToothGrowth, alt = "greater", var.equal = TRUE)
#+END_SRC

The \(p\)-value for the \(t\)-test was SRC_R[:eval no-export]{round(B$p.value, 3)}
0.03, while the permutation test \(p\)-value was
SRC_R[:eval no-export]{round(A$p.value, 3)} 0.061. Note that there is an underlying
normality assumption for the \(t\)-test, which isn't present in the
permutation test. If the normality assumption may be questionable,
then the permutation test would be more reasonable. We see what can
happen when using a test in a situation where the assumptions are not
met: smaller \(p\)-values. In situations where the normality
assumptions are not met, for example, small sample scenarios, the
permutation test is to be preferred. In particular, if accuracy is
very important then we should use the permutation test.

#+BEGIN_rem
Here are some things about permutation tests to keep in mind.
- While the permutation test does not require normality of the
  populations (as contrasted with the \(t\)-test), nevertheless it
  still requires that the two groups are exchangeable; see Section
  [[#sec-Exchangeable-Random-Variables]]. In particular, this means that they
  must be identically distributed under the null hypothesis. They must
  have not only the same means, but they must also have the same
  spread, shape, and everything else. This assumption may or may not
  be true in a given example, but it will rarely cause the \(t\)-test
  to outperform the permutation test, because even if the sample
  standard deviations are markedly different it does not mean that the
  population standard deviations are different. In many situations the
  permutation test will also carry over to the \(t\)-test.
- If the distribution of the groups is close to normal, then the
  \(t\)-test \(p\)-value and the bootstrap \(p\)-value will be
  approximately equal. If they differ markedly, then this should be
  considered evidence that the normality assumptions do not hold.
- The generality of the permutation test is such that one can use all
  kinds of statistics to compare the two groups. One could compare the
  difference in variances or the difference in (just about
  anything). Alternatively, one could compare the ratio of sample
  means, \(\overline{X}_{1}/\overline{X}_{2}\). Of course, under the
  null hypothesis this last quantity should be near 1.
- Just as with the bootstrap, the answer we get is subject to
  variability due to the inherent randomness of resampling from the
  data. We can make the variability as small as we like by taking
  sufficiently many resamples. How many? If the conclusion is very
  important (that is, if lots of money is at stake), then take
  thousands. For point estimation problems typically, \(R=1000\)
  resamples, or so, is enough. In general, if the true \(p\)-value is
  \(p\) then the standard error of the estimated \(p\)-value is
  \(\sqrt{p(1-p)/R}\). You can choose \(R\) to get whatever accuracy
  desired.
#+END_rem

- Other possible testing designs:
   - Matched Pairs Designs. 
   - Relationship between two variables. 

#+LaTeX: \newpage{}

** Exercises
#+LaTeX: \setcounter{thm}{0}

* Nonparametric Statistics                                         :nonparam:
:PROPERTIES:
:tangle: R/14-nonparam.R
:CUSTOM_ID: cha-Nonparametric-Statistics
:END:

#+BEGIN_SRC R :exports none :eval never
#    IPSUR: Introduction to Probability and Statistics Using R
#    Copyright (C) 2014  G. Jay Kerns
#
#    Chapter: Nonparametric Statistics
#
#    This file is part of IPSUR.
#
#    IPSUR is free software: you can redistribute it and/or modify
#    it under the terms of the GNU General Public License as published by
#    the Free Software Foundation, either version 3 of the License, or
#    (at your option) any later version.
#
#    IPSUR is distributed in the hope that it will be useful,
#    but WITHOUT ANY WARRANTY; without even the implied warranty of
#    MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
#    GNU General Public License for more details.
#
#    You should have received a copy of the GNU General Public License
#    along with IPSUR.  If not, see <http://www.gnu.org/licenses/>.
#+END_SRC

This chapter is still under substantial revision. At any time you can
preview any released drafts with the development version of the
=IPSUR= package which is available from \(\mathsf{R}\)-Forge:

#+BEGIN_SRC R :exports code :eval never
install.packages("IPSUR", repos="http://R-Forge.R-project.org")
library("IPSUR")
read(IPSUR)
#+END_SRC

* Categorical Data Analysis                                           :categ:
:PROPERTIES:
:tangle: R/15-categ.R
:CUSTOM_ID: cha-Categorical-Data-Analysis
:END:

#+BEGIN_SRC R :exports none :eval never
#    IPSUR: Introduction to Probability and Statistics Using R
#    Copyright (C) 2014  G. Jay Kerns
#
#    Chapter: Categorical Data Analysis
#
#    This file is part of IPSUR.
#
#    IPSUR is free software: you can redistribute it and/or modify
#    it under the terms of the GNU General Public License as published by
#    the Free Software Foundation, either version 3 of the License, or
#    (at your option) any later version.
#
#    IPSUR is distributed in the hope that it will be useful,
#    but WITHOUT ANY WARRANTY; without even the implied warranty of
#    MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
#    GNU General Public License for more details.
#
#    You should have received a copy of the GNU General Public License
#    along with IPSUR.  If not, see <http://www.gnu.org/licenses/>.
#+END_SRC

This chapter is still under substantial revision. At any time you can
preview any released drafts with the development version of the
=IPSUR= package which is available from \(\mathsf{R}\)-Forge:

#+BEGIN_SRC R :exports code :eval never
install.packages("IPSUR", repos="http://R-Forge.R-project.org")
library("IPSUR")
read(IPSUR)
#+END_SRC

* Time Series                                                    :timeseries:
:PROPERTIES:
:tangle: R/16-timeseries.R
:CUSTOM_ID: cha-Time-Series
:END:

#+BEGIN_SRC R :exports none :eval never
#    IPSUR: Introduction to Probability and Statistics Using R
#    Copyright (C) 2014  G. Jay Kerns
#
#    Chapter: Time Series
#
#    This file is part of IPSUR.
#
#    IPSUR is free software: you can redistribute it and/or modify
#    it under the terms of the GNU General Public License as published by
#    the Free Software Foundation, either version 3 of the License, or
#    (at your option) any later version.
#
#    IPSUR is distributed in the hope that it will be useful,
#    but WITHOUT ANY WARRANTY; without even the implied warranty of
#    MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
#    GNU General Public License for more details.
#
#    You should have received a copy of the GNU General Public License
#    along with IPSUR.  If not, see <http://www.gnu.org/licenses/>.
#+END_SRC

This chapter is still under substantial revision. At any time you can
preview any released drafts with the development version of the
=IPSUR= package which is available from \(\mathsf{R}\)-Forge:

#+BEGIN_SRC R :exports code :eval never
install.packages("IPSUR", repos="http://R-Forge.R-project.org")
library("IPSUR")
read(IPSUR)
#+END_SRC

#+LaTeX: \appendix


* R Session Information                                            :appendix:
:PROPERTIES:
:tangle: R/17-appendix.R
:CUSTOM_ID: cha-R-Session-Information
:END:

If you ever write the \(\mathsf{R}\) help mailing list with a
question, then you should include your session information in the
email; it makes the reader's job easier and is requested by the
Posting Guide. Here is how to do that, and below is what the output
looks like.

#+BEGIN_SRC R :exports both :results output pp 
sessionInfo()
#+END_SRC

#+RESULTS:
#+BEGIN_example
R version 3.0.1 (2013-05-16)
Platform: i686-pc-linux-gnu (32-bit)

locale:
[1] C

attached base packages:
 [1] grid      stats4    splines   tcltk     stats     graphics  grDevices utils    
 [9] datasets  methods   base     

other attached packages:
 [1] TeachingDemos_2.9        scatterplot3d_0.3-33     RcmdrPlugin.IPSUR_0.1-8 
 [4] Rcmdr_1.9-6              car_2.0-18               nnet_7.3-7              
 [7] qcc_2.2                  MASS_7.3-28              prob_0.9-2              
[10] lmtest_0.9-31            zoo_1.7-10               Hmisc_3.12-2            
[13] Formula_1.1-1            HH_2.3-37                colorspace_1.2-2        
[16] reshape_0.8.4            plyr_1.8                 latticeExtra_0.6-24     
[19] RColorBrewer_1.0-5       leaps_2.9                multcomp_1.2-19         
[22] lattice_0.20-15          ggplot2_0.9.3.1          e1071_1.6-1             
[25] class_7.3-8              distrEx_2.4              distr_2.4               
[28] SweaveListingUtils_0.6.1 sfsmisc_1.0-24           startupmsg_0.8          
[31] diagram_1.6.1            shape_1.4.0              combinat_0.0-8          
[34] coin_1.0-22              modeltools_0.2-19        mvtnorm_0.9-9995        
[37] survival_2.37-4          boot_1.3-9               aplpack_1.2.7           
[40] actuar_1.1-6            

loaded via a namespace (and not attached):
 [1] cluster_1.14.4  dichromat_2.0-0 digest_0.6.3    gtable_0.1.2    labeling_0.2   
 [6] munsell_0.4.2   proto_0.3-10    reshape2_1.2.2  rpart_4.1-1     scales_0.2.3   
[11] stringr_0.6.2   tools_3.0.1
#+END_example

#+LaTeX: \vfill{}

* GNU Free Documentation License                                   :appendix:
:PROPERTIES:
:tangle: R/17-appendix.R
:END:

#+BEGIN_latex
\label{cha-GNU-Free-Documentation}
\begin{center}
\textbf{\large Version 1.3, 3 November 2008}\bigskip{}

\par\end{center}

\noindent Copyright (C) 2000, 2001, 2002, 2007, 2008 Free Software
Foundation, Inc.

\begin{center}
\url{http://fsf.org/}
\par\end{center}
#+END_latex

#+LaTeX: \noindent 
Everyone is permitted to copy and distribute verbatim copies of this
license document, but changing it is not allowed.

** 0. PREAMBLE

The purpose of this License is to make a manual, textbook, or other
functional and useful document free in the sense of freedom: to assure
everyone the effective freedom to copy and redistribute it, with or
without modifying it, either commercially or
noncommercially. Secondarily, this License preserves for the author
and publisher a way to get credit for their work, while not being
considered responsible for modifications made by others.

This License is a kind of copyleft, which means that derivative works
of the document must themselves be free in the same sense. It
complements the GNU General Public License, which is a copyleft
license designed for free software.

We have designed this License in order to use it for manuals for free
software, because free software needs free documentation: a free
program should come with manuals providing the same freedoms that the
software does. But this License is not limited to software manuals; it
can be used for any textual work, regardless of subject matter or
whether it is published as a printed book. We recommend this License
principally for works whose purpose is instruction or reference.

** 1. APPLICABILITY AND DEFINITIONS

This License applies to any manual or other work, in any medium, that
contains a notice placed by the copyright holder saying it can be
distributed under the terms of this License. Such a notice grants a
world-wide, royalty-free license, unlimited in duration, to use that
work under the conditions stated herein. The Document, below, refers
to any such manual or work. Any member of the public is a licensee,
and is addressed as you. You accept the license if you copy, modify or
distribute the work in a way requiring permission under copyright law.

A Modified Version of the Document means any work containing the
Document or a portion of it, either copied verbatim, or with
modifications and/or translated into another language.

A Secondary Section is a named appendix or a front-matter section of
the Document that deals exclusively with the relationship of the
publishers or authors of the Document to the Document's overall
subject (or to related matters) and contains nothing that could fall
directly within that overall subject. (Thus, if the Document is in
part a textbook of mathematics, a Secondary Section may not explain
any mathematics.) The relationship could be a matter of historical
connection with the subject or with related matters, or of legal,
commercial, philosophical, ethical or political position regarding
them.

The Invariant Sections are certain Secondary Sections whose titles are
designated, as being those of Invariant Sections, in the notice that
says that the Document is released under this License. If a section
does not fit the above definition of Secondary then it is not allowed
to be designated as Invariant. The Document may contain zero Invariant
Sections. If the Document does not identify any Invariant Sections
then there are none.

The Cover Texts are certain short passages of text that are listed, as
Front-Cover Texts or Back-Cover Texts, in the notice that says that
the Document is released under this License. A Front-Cover Text may be
at most 5 words, and a Back-Cover Text may be at most 25 words.

A Transparent copy of the Document means a machine-readable copy,
represented in a format whose specification is available to the
general public, that is suitable for revising the document
straightforwardly with generic text editors or (for images composed of
pixels) generic paint programs or (for drawings) some widely available
drawing editor, and that is suitable for input to text formatters or
for automatic translation to a variety of formats suitable for input
to text formatters. A copy made in an otherwise Transparent file
format whose markup, or absence of markup, has been arranged to thwart
or discourage subsequent modification by readers is not
Transparent. An image format is not Transparent if used for any
substantial amount of text. A copy that is not Transparent is called
Opaque.

Examples of suitable formats for Transparent copies include plain
ASCII without markup, Texinfo input format, \LaTeX{} input format,
SGML or XML using a publicly available DTD, and standard-conforming
simple HTML, PostScript or PDF designed for human
modification. Examples of transparent image formats include PNG, XCF
and JPG. Opaque formats include proprietary formats that can be read
and edited only by proprietary word processors, SGML or XML for which
the DTD and/or processing tools are not generally available, and the
machine-generated HTML, PostScript or PDF produced by some word
processors for output purposes only.

The Title Page means, for a printed book, the title page itself, plus
such following pages as are needed to hold, legibly, the material this
License requires to appear in the title page. For works in formats
which do not have any title page as such, Title Page means the text
near the most prominent appearance of the work's title, preceding the
beginning of the body of the text.

The publisher means any person or entity that distributes copies of
the Document to the public.  A section Entitled XYZ means a named
subunit of the Document whose title either is precisely XYZ or
contains XYZ in parentheses following text that translates XYZ in
another language. (Here XYZ stands for a specific section name
mentioned below, such as Acknowledgements, Dedications, Endorsements,
or History.) To Preserve the Title of such a section when you modify
the Document means that it remains a section Entitled XYZ according to
this definition.

The Document may include Warranty Disclaimers next to the notice which
states that this License applies to the Document. These Warranty
Disclaimers are considered to be included by reference in this
License, but only as regards disclaiming warranties: any other
implication that these Warranty Disclaimers may have is void and has
no effect on the meaning of this License.

** 2. VERBATIM COPYING

You may copy and distribute the Document in any medium, either
commercially or noncommercially, provided that this License, the
copyright notices, and the license notice saying this License applies
to the Document are reproduced in all copies, and that you add no
other conditions whatsoever to those of this License. You may not use
technical measures to obstruct or control the reading or further
copying of the copies you make or distribute. However, you may accept
compensation in exchange for copies. If you distribute a large enough
number of copies you must also follow the conditions in section 3.

You may also lend copies, under the same conditions stated above, and
you may publicly display copies.

** 3. COPYING IN QUANTITY

If you publish printed copies (or copies in media that commonly have
printed covers) of the Document, numbering more than 100, and the
Document's license notice requires Cover Texts, you must enclose the
copies in covers that carry, clearly and legibly, all these Cover
Texts: Front-Cover Texts on the front cover, and Back-Cover Texts on
the back cover. Both covers must also clearly and legibly identify you
as the publisher of these copies. The front cover must present the
full title with all words of the title equally prominent and
visible. You may add other material on the covers in addition. Copying
with changes limited to the covers, as long as they preserve the title
of the Document and satisfy these conditions, can be treated as
verbatim copying in other respects.

If the required texts for either cover are too voluminous to fit
legibly, you should put the first ones listed (as many as fit
reasonably) on the actual cover, and continue the rest onto adjacent
pages.

If you publish or distribute Opaque copies of the Document numbering
more than 100, you must either include a machine-readable Transparent
copy along with each Opaque copy, or state in or with each Opaque copy
a computer-network location from which the general network-using
public has access to download using public-standard network protocols
a complete Transparent copy of the Document, free of added
material. If you use the latter option, you must take reasonably
prudent steps, when you begin distribution of Opaque copies in
quantity, to ensure that this Transparent copy will remain thus
accessible at the stated location until at least one year after the
last time you distribute an Opaque copy (directly or through your
agents or retailers) of that edition to the public.

It is requested, but not required, that you contact the authors of the
Document well before redistributing any large number of copies, to
give them a chance to provide you with an updated version of the
Document.

** 4. MODIFICATIONS

You may copy and distribute a Modified Version of the Document under
the conditions of sections 2 and 3 above, provided that you release
the Modified Version under precisely this License, with the Modified
Version filling the role of the Document, thus licensing distribution
and modification of the Modified Version to whoever possesses a copy
of it. In addition, you must do these things in the Modified Version:

A. Use in the Title Page (and on the covers, if any) a title distinct
from that of the Document, and from those of previous versions (which
should, if there were any, be listed in the History section of the
Document). You may use the same title as a previous version if the
original publisher of that version gives permission.

B. List on the Title Page, as authors, one or more persons or entities
responsible for authorship of the modifications in the Modified
Version, together with at least five of the principal authors of the
Document (all of its principal authors, if it has fewer than five),
unless they release you from this requirement.

C. State on the Title page the name of the publisher of the Modified
Version, as the publisher.

D. Preserve all the copyright notices of the Document. 

E. Add an appropriate copyright notice for your modifications adjacent
to the other copyright notices.

F. Include, immediately after the copyright notices, a license notice
giving the public permission to use the Modified Version under the
terms of this License, in the form shown in the Addendum below.

G. Preserve in that license notice the full lists of Invariant
Sections and required Cover Texts given in the Document's license
notice.

H. Include an unaltered copy of this License. 

I. Preserve the section Entitled History, Preserve its Title, and add
to it an item stating at least the title, year, new authors, and
publisher of the Modified Version as given on the Title Page. If there
is no section Entitled History in the Document, create one stating the
title, year, authors, and publisher of the Document as given on its
Title Page, then add an item describing the Modified Version as stated
in the previous sentence.

J. Preserve the network location, if any, given in the Document for
public access to a Transparent copy of the Document, and likewise the
network locations given in the Document for previous versions it was
based on. These may be placed in the History section. You may omit a
network location for a work that was published at least four years
before the Document itself, or if the original publisher of the
version it refers to gives permission.

K. For any section Entitled Acknowledgements or Dedications, Preserve
the Title of the section, and preserve in the section all the
substance and tone of each of the contributor acknowledgements and/or
dedications given therein.  L. Preserve all the Invariant Sections of
the Document, unaltered in their text and in their titles. Section
numbers or the equivalent are not considered part of the section
titles.

M. Delete any section Entitled Endorsements. Such a section may not be
included in the Modified Version.

N. Do not retitle any existing section to be Entitled Endorsements or
to conflict in title with any Invariant Section.

O. Preserve any Warranty Disclaimers.

If the Modified Version includes new front-matter sections or
appendices that qualify as Secondary Sections and contain no material
copied from the Document, you may at your option designate some or all
of these sections as invariant. To do this, add their titles to the
list of Invariant Sections in the Modified Version's license
notice. These titles must be distinct from any other section titles.

You may add a section Entitled Endorsements, provided it contains
nothing but endorsements of your Modified Version by various
parties--for example, statements of peer review or that the text has
been approved by an organization as the authoritative definition of a
standard.

You may add a passage of up to five words as a Front-Cover Text, and a
passage of up to 25 words as a Back-Cover Text, to the end of the list
of Cover Texts in the Modified Version. Only one passage of
Front-Cover Text and one of Back-Cover Text may be added by (or
through arrangements made by) any one entity. If the Document already
includes a cover text for the same cover, previously added by you or
by arrangement made by the same entity you are acting on behalf of,
you may not add another; but you may replace the old one, on explicit
permission from the previous publisher that added the old one.

The author(s) and publisher(s) of the Document do not by this License
give permission to use their names for publicity for or to assert or
imply endorsement of any Modified Version.

** 5. COMBINING DOCUMENTS

You may combine the Document with other documents released under this
License, under the terms defined in section 4 above for modified
versions, provided that you include in the combination all of the
Invariant Sections of all of the original documents, unmodified, and
list them all as Invariant Sections of your combined work in its
license notice, and that you preserve all their Warranty Disclaimers.

The combined work need only contain one copy of this License, and
multiple identical Invariant Sections may be replaced with a single
copy. If there are multiple Invariant Sections with the same name but
different contents, make the title of each such section unique by
adding at the end of it, in parentheses, the name of the original
author or publisher of that section if known, or else a unique
number. Make the same adjustment to the section titles in the list of
Invariant Sections in the license notice of the combined work.

In the combination, you must combine any sections Entitled History in
the various original documents, forming one section Entitled History;
likewise combine any sections Entitled Acknowledgements, and any
sections Entitled Dedications. You must delete all sections Entitled
Endorsements.

** 6. COLLECTIONS OF DOCUMENTS

You may make a collection consisting of the Document and other
documents released under this License, and replace the individual
copies of this License in the various documents with a single copy
that is included in the collection, provided that you follow the rules
of this License for verbatim copying of each of the documents in all
other respects.

You may extract a single document from such a collection, and
distribute it individually under this License, provided you insert a
copy of this License into the extracted document, and follow this
License in all other respects regarding verbatim copying of that
document.

** 7. AGGREGATION WITH INDEPENDENT WORKS

A compilation of the Document or its derivatives with other separate
and independent documents or works, in or on a volume of a storage or
distribution medium, is called an aggregate if the copyright resulting
from the compilation is not used to limit the legal rights of the
compilation's users beyond what the individual works permit. When the
Document is included in an aggregate, this License does not apply to
the other works in the aggregate which are not themselves derivative
works of the Document.

If the Cover Text requirement of section 3 is applicable to these
copies of the Document, then if the Document is less than one half of
the entire aggregate, the Document's Cover Texts may be placed on
covers that bracket the Document within the aggregate, or the
electronic equivalent of covers if the Document is in electronic
form. Otherwise they must appear on printed covers that bracket the
whole aggregate.

** 8. TRANSLATION

Translation is considered a kind of modification, so you may
distribute translations of the Document under the terms of
section 4. Replacing Invariant Sections with translations requires
special permission from their copyright holders, but you may include
translations of some or all Invariant Sections in addition to the
original versions of these Invariant Sections. You may include a
translation of this License, and all the license notices in the
Document, and any Warranty Disclaimers, provided that you also include
the original English version of this License and the original versions
of those notices and disclaimers. In case of a disagreement between
the translation and the original version of this License or a notice
or disclaimer, the original version will prevail.

If a section in the Document is Entitled Acknowledgements,
Dedications, or History, the requirement (section 4) to Preserve its
Title (section 1) will typically require changing the actual title.

** 9. TERMINATION

You may not copy, modify, sublicense, or distribute the Document
except as expressly provided under this License. Any attempt otherwise
to copy, modify, sublicense, or distribute it is void, and will
automatically terminate your rights under this License.

However, if you cease all violation of this License, then your license
from a particular copyright holder is reinstated (a) provisionally,
unless and until the copyright holder explicitly and finally
terminates your license, and (b) permanently, if the copyright holder
fails to notify you of the violation by some reasonable means prior to
60 days after the cessation.

Moreover, your license from a particular copyright holder is
reinstated permanently if the copyright holder notifies you of the
violation by some reasonable means, this is the first time you have
received notice of violation of this License (for any work) from that
copyright holder, and you cure the violation prior to 30 days after
your receipt of the notice.

Termination of your rights under this section does not terminate the
licenses of parties who have received copies or rights from you under
this License. If your rights have been terminated and not permanently
reinstated, receipt of a copy of some or all of the same material does
not give you any rights to use it.

** 10. FUTURE REVISIONS OF THIS LICENSE

The Free Software Foundation may publish new, revised versions of the
GNU Free Documentation License from time to time. Such new versions
will be similar in spirit to the present version, but may differ in
detail to address new problems or concerns. See
http://www.gnu.org/copyleft/.

Each version of the License is given a distinguishing version
number. If the Document specifies that a particular numbered version
of this License or any later version applies to it, you have the
option of following the terms and conditions either of that specified
version or of any later version that has been published (not as a
draft) by the Free Software Foundation. If the Document does not
specify a version number of this License, you may choose any version
ever published (not as a draft) by the Free Software Foundation. If
the Document specifies that a proxy can decide which future versions
of this License can be used, that proxy's public statement of
acceptance of a version permanently authorizes you to choose that
version for the Document.

** 11. RELICENSING

Massive Multiauthor Collaboration Site (or MMC Site) means any World
Wide Web server that publishes copyrightable works and also provides
prominent facilities for anybody to edit those works. A public wiki
that anybody can edit is an example of such a server. A Massive
Multiauthor Collaboration (or MMC) contained in the site means any set
of copyrightable works thus published on the MMC site.

CC-BY-SA means the Creative Commons Attribution-Share Alike 3.0
license published by Creative Commons Corporation, a not-for-profit
corporation with a principal place of business in San Francisco,
California, as well as future copyleft versions of that license
published by that same organization.

Incorporate means to publish or republish a Document, in whole or in
part, as part of another Document.

An MMC is eligible for relicensing if it is licensed under this
License, and if all works that were first published under this License
somewhere other than this MMC, and subsequently incorporated in whole
or in part into the MMC, (1) had no cover texts or invariant sections,
and (2) were thus incorporated prior to November 1, 2008.

The operator of an MMC Site may republish an MMC contained in the site
under CC-BY-SA on the same site at any time before August 1, 2009,
provided the MMC is eligible for relicensing.

** ADDENDUM: How to use this License for your documents

To use this License in a document you have written, include a copy of
the License in the document and put the following copyright and
license notices just after the title page:

#+BEGIN_quote
Copyright (c) YEAR YOUR NAME. Permission is granted to copy,
distribute and/or modify this document under the terms of the GNU Free
Documentation License, Version 1.3 or any later version published by
the Free Software Foundation; with no Invariant Sections, no
Front-Cover Texts, and no Back-Cover Texts. A copy of the license is
included in the section entitled GNU Free Documentation License.
#+END_quote

If you have Invariant Sections, Front-Cover Texts and Back-Cover
Texts, replace the with...Texts. line with this:

#+BEGIN_quote
with the Invariant Sections being LIST THEIR TITLES, with the
Front-Cover Texts being LIST, and with the Back-Cover Texts being
LIST.
#+END_quote

If you have Invariant Sections without Cover Texts, or some other
combination of the three, merge those two alternatives to suit the
situation.

If your document contains nontrivial examples of program code, we
recommend releasing these examples in parallel under your choice of
free software license, such as the GNU General Public License, to
permit their use in free software.

* History                                                          :appendix:
:PROPERTIES:
:tangle: R/17-appendix.R
:CUSTOM_ID: cha-History
:END:

| Title:     | Introduction to Probability and Statistics Using \(\mathsf{R}\), Second Edition |
| Year:      | 2014                                                                            |
| Authors:   | G. Jay Kerns                                                                    |
| Publisher: | G. Jay Kerns                                                                    |

| Title:     | Introduction to Probability and Statistics Using \(\mathsf{R}\), First Edition |
| Year:      | 2010                                                                           |
| Authors:   | G. Jay Kerns                                                                   |
| Publisher: | G. Jay Kerns                                                                   |

#+LaTeX: \vfill{}

* Data                                                             :appendix:
:PROPERTIES:
:tangle: R/17-appendix.R
:CUSTOM_ID: cha-data
:END:

This appendix is a reference of sorts regarding some of the data
structures a statistician is likely to encounter. We discuss their
salient features and idiosyncrasies.

** Data Structures
:PROPERTIES:
:CUSTOM_ID: sec-Data-Structures
:END: 

*** Vectors

See the "Vectors and Assignment" section of /An Introduction to/
\(\mathsf{R}\). A vector is an ordered sequence of elements, such as
numbers, characters, or logical values, and there may be =NA='s
present. We usually make vectors with the assignment operator =<-=.

#+BEGIN_SRC R :exports both :results output pp 
x <- c(3, 5, 9)
#+END_SRC

#+RESULTS:

Vectors are atomic in the sense that if you try to mix and match
elements of different modes then all elements will be coerced to the
most convenient common mode.

#+BEGIN_SRC R :exports both :results output pp 
y <- c(3, "5", TRUE)
#+END_SRC

#+RESULTS:

In the example all elements were coerced to /character/ mode. We can
test whether a given object is a vector with =is.vector= and can
coerce an object (if possible) to a vector with =as.vector=.

*** Matrices and Arrays

See the "Arrays and Matrices" section of /An Introduction to/
\(\mathsf{R}\). Loosely speaking, a matrix is a vector that has been
reshaped into rectangular form, and an array is a multidimensional
matrix. Strictly speaking, it is the other way around: an array is a
data vector with a dimension attribute (=dim=), and a matrix is the
special case of an array with only two dimensions. We can construct a
matrix with the =matrix= function.

#+BEGIN_SRC R :exports both :results output pp 
matrix(letters[1:6], nrow = 2, ncol = 3)
#+END_SRC

#+RESULTS:
:      [,1] [,2] [,3]
: [1,] "a"  "c"  "e" 
: [2,] "b"  "d"  "f"

Notice the order of the matrix entries, which shows how the matrix is
populated by default. We can change this with the =byrow= argument:

#+BEGIN_SRC R :exports both :results output pp 
matrix(letters[1:6], nrow = 2, ncol = 3, byrow = TRUE)
#+END_SRC

#+RESULTS:
:      [,1] [,2] [,3]
: [1,] "a"  "b"  "c" 
: [2,] "d"  "e"  "f"

We can test whether a given object is a matrix with =is.matrix= and
can coerce an object (if possible) to a matrix with =as.matrix=. As a
final example watch what happens when we mix and match types in the
first argument:

#+BEGIN_SRC R :exports both :results output pp 
matrix(c(1,"2",NA, FALSE), nrow = 2, ncol = 3)
#+END_SRC

#+RESULTS:
:      [,1] [,2]    [,3]
: [1,] "1"  NA      "1" 
: [2,] "2"  "FALSE" "2"

Notice how all of the entries were coerced to character for the final
result (except =NA=). Also notice how the four values were /recycled/
to fill up the six entries of the matrix.

The standard arithmetic operations work element-wise with matrices.

#+BEGIN_SRC R :exports both :results output pp 
A <- matrix(1:6, 2, 3)
B <- matrix(2:7, 2, 3)
A + B
A * B
#+END_SRC

#+RESULTS:
:      [,1] [,2] [,3]
: [1,]    3    7   11
: [2,]    5    9   13
:      [,1] [,2] [,3]
: [1,]    2   12   30
: [2,]    6   20   42

If you want the standard definition of matrix multiplication then use
the =%*%= function. If we were to try =A %*% B= we would get an error
because the dimensions do not match correctly, but for fun, we could
transpose =B= to get conformable matrices. The transpose function =t=
only works for matrices (and data frames).

#+BEGIN_SRC R :exports both :results output pp 
try(A * B)     # an error
A %*% t(B)     # this is alright
#+END_SRC

#+RESULTS:
:      [,1] [,2] [,3]
: [1,]    2   12   30
: [2,]    6   20   42
:      [,1] [,2]
: [1,]   44   53
: [2,]   56   68

To get the ordinary matrix inverse use the =solve= function: 

#+BEGIN_SRC R :exports both :results output pp 
solve(A %*% t(B))     # input matrix must be square
#+END_SRC

#+RESULTS:
:           [,1]      [,2]
: [1,]  2.833333 -2.208333
: [2,] -2.333333  1.833333

Arrays more general than matrices, and some functions (like transpose)
do not work for the more general array. Here is what an array looks
like:

#+BEGIN_SRC R :exports both :results output pp 
array(LETTERS[1:24], dim = c(3,4,2))
#+END_SRC

#+RESULTS:
#+BEGIN_example
, , 1

     [,1] [,2] [,3] [,4]
[1,] "A"  "D"  "G"  "J" 
[2,] "B"  "E"  "H"  "K" 
[3,] "C"  "F"  "I"  "L" 

, , 2

     [,1] [,2] [,3] [,4]
[1,] "M"  "P"  "S"  "V" 
[2,] "N"  "Q"  "T"  "W" 
[3,] "O"  "R"  "U"  "X"
#+END_example

We can test with =is.array= and may coerce with =as.array=.

*** Data Frames

A data frame is a rectangular array of information with a special
status in \(\mathsf{R}\). It is used as the fundamental data structure
by many of the modeling functions. It is like a matrix in that all of
the columns must be the same length, but it is more general than a
matrix in that columns are allowed to have different modes.

#+BEGIN_SRC R :exports both :results output pp 
x <- c(1.3, 5.2, 6)
y <- letters[1:3]
z <- c(TRUE, FALSE, TRUE)
A <- data.frame(x, y, z)
A
#+END_SRC

#+RESULTS:
:     x y     z
: 1 1.3 a  TRUE
: 2 5.2 b FALSE
: 3 6.0 c  TRUE

Notice the =names= on the columns of =A=. We can change those with the
=names= function.

#+BEGIN_SRC R :exports both :results output pp 
names(A) <- c("Fred","Mary","Sue")
A
#+END_SRC

#+RESULTS:
:   Fred Mary   Sue
: 1  1.3    a  TRUE
: 2  5.2    b FALSE
: 3  6.0    c  TRUE

Basic command is =data.frame=. You can test with =is.data.frame= and
you can coerce with =as.data.frame=.

*** Lists
A list is more general than a data frame.

*** Tables
The word "table" has a special meaning in \(\mathsf{R}\). More
precisely, a contingency table is an object of class =table= which is
an array.

Suppose you have a contingency table and would like to do descriptive
or inferential statistics on it. The default form of the table is
usually inconvenient to use unless we are working with a function
specially tailored for tables. Here is how to transform your data to a
more manageable form, namely, the raw data used to make the table.

First, we coerce the table to a data frame with: 

#+BEGIN_SRC R :exports both :results output pp 
A <- as.data.frame(Titanic)
head(A)
#+END_SRC

#+RESULTS:
:   Class    Sex   Age Survived Freq
: 1   1st   Male Child       No    0
: 2   2nd   Male Child       No    0
: 3   3rd   Male Child       No   35
: 4  Crew   Male Child       No    0
: 5   1st Female Child       No    0
: 6   2nd Female Child       No    0

Note that there are as many preliminary columns of =A= as there are
dimensions to the table. The rows of =A= contain every possible
combination of levels from each of the dimensions. There is also a
=Freq= column, which shows how many observations there were at that
particular combination of levels.

The form of =A= is often sufficient for our purposes, but more often
we need to do more work: we would usually like to repeat each row of
=A= exactly the number of times shown in the =Freq= column. The
=reshape= package \cite{reshape} has the function =untable= designed
for that very purpose:

#+BEGIN_SRC R :exports both :results output pp 
B <- with(A, untable(A, Freq))
head(B)
#+END_SRC

#+RESULTS:
:     Class  Sex   Age Survived Freq
: 3     3rd Male Child       No   35
: 3.1   3rd Male Child       No   35
: 3.2   3rd Male Child       No   35
: 3.3   3rd Male Child       No   35
: 3.4   3rd Male Child       No   35
: 3.5   3rd Male Child       No   35

Now, this is more like it. Note that we slipped in a call to the
=with= function, which was done to make the call to =untable= more
pretty; we could just as easily have done
:  untable(TitanicDF, A$Freq)


The only fly in the ointment is the lingering =Freq= column which has
repeated values that do not have any meaning any more. We could just
ignore it, but it would be better to get rid of the meaningless column
so that it does not cause trouble later. While we are at it, we could
clean up the =rownames=, too.

#+BEGIN_SRC R :exports both :results output pp 
C <- B[, -5]
rownames(C) <- 1:dim(C)[1]
head(C)
#+END_SRC

#+RESULTS:
:   Class  Sex   Age Survived
: 1   3rd Male Child       No
: 2   3rd Male Child       No
: 3   3rd Male Child       No
: 4   3rd Male Child       No
: 5   3rd Male Child       No
: 6   3rd Male Child       No

*** More about Tables
Suppose you want to make a table that looks like this:

There are at least two ways to do it.

- Using a matrix:
  #+BEGIN_SRC R :exports both :results output pp 
  tab <- matrix(1:6, nrow = 2, ncol = 3)
  rownames(tab) <- c('first', 'second')
  colnames(tab) <- c('A', 'B', 'C')
  tab  # Counts
  #+END_SRC

  #+RESULTS:
  :        A B C
  : first  1 3 5
  : second 2 4 6

   - note that the columns are filled in consecutively by default. If
     you want to fill the data in by rows then do =byrow = TRUE= in
     the =matrix= command.

- Using a dataframe
  #+BEGIN_SRC R :exports both :results output pp 
  p <- c("milk","tea")
  g <- c("milk","tea")
  catgs <- expand.grid(poured = p, guessed = g)
  cnts <- c(3, 1, 1, 3)
  D <- cbind(catgs, count = cnts)
  xtabs(count ~ poured + guessed, data = D)
  #+END_SRC

  #+RESULTS:
  :       guessed
  : poured milk tea
  :   milk    3   1
  :   tea     1   3

   - again, the data are filled in column-wise.
   - the object is a dataframe
   - if you want to store it as a table then do =A <- xtabs(count ~
     poured + guessed, data = D)=

** Importing Data
:PROPERTIES:
:CUSTOM_ID: sec-Importing-A-Data
:END: 

Statistics is the study of data, so the statistician's first step is
usually to obtain data from somewhere or another and read them into
\(\mathsf{R}\). In this section we describe some of the most common
sources of data and how to get data from those sources into a running
\(\mathsf{R}\) session.

For more information please refer to the \(\mathsf{R}\) /Data
Import/Export Manual/, \cite{rstatenv} and /An Introduction to/
\(\mathsf{R}\), \cite{Venables2010}.

*** Data in Packages

There are many data sets stored in the =datasets= package
\cite{datasets} of base \(\mathsf{R}\). To see a list of them all
issue the command =data(package = "datasets")=. The output is omitted
here because the list is so long. The names of the data sets are
listed in the left column. Any data set in that list is already on the
search path by default, which means that a user can use it immediately
without any additional work.

There are many other data sets available in the thousands of
contributed packages. To see the data sets available in those packages
that are currently loaded into memory issue the single command
=data()=. If you would like to see all of the data sets that are
available in all packages that are installed on your computer (but not
necessarily loaded), issue the command

:  data(package = .packages(all.available = TRUE))

To load the data set =foo= in the contributed package =bar= issue the
commands =library("bar")= followed by =data(foo)=, or just the single
command

:  data(foo, package = "bar")

*** Text Files
Many sources of data are simple text files. The entries in the file
are separated by delimeters such as TABS (tab-delimeted), commas
(comma separated values, or =.csv=, for short) or even just white
space (no special name). A lot of data on the Internet are stored with
text files, and even if they are not, a person can copy-paste
information from a web page to a text file, save it on the computer,
and read it into \(\mathsf{R}\).

*** Other Software Files
Often the data set of interest is stored in some other, proprietary,
format by third-party software such as Minitab, SAS, or SPSS. The
=foreign= package \cite{foreign} supports import/conversion from many
of these formats. Please note, however, that data sets from other
software sometimes have properties with no direct analogue in
\(\mathsf{R}\). In those cases the conversion process may lose some
information which will need to be reentered manually from within
\(\mathsf{R}\). See the /Data Import/Export Manual/.

As an example, suppose the data are stored in the SPSS file =foo.sav=
which the user has copied to the working directory; it can be imported
with the commands

#+BEGIN_SRC R :exports code :eval never
library("foreign")
read.spss("foo.sav")
#+END_SRC

See =?read.spss= for the available options to customize the file
import. Note that the \(\mathsf{R}\) Commander will import many of the
common file types with a menu driven interface.

*** Importing a Data Frame

The basic command is =read.table=.

** Creating New Data Sets
:PROPERTIES:
:CUSTOM_ID: sec-Creating-New-Data
:END: 

Using =c=
Using =scan=
Using the \(\mathsf{R}\) Commander.

** Editing Data
:PROPERTIES:
:CUSTOM_ID: sec-Editing-Data-Sets
:END: 

*** Editing Data Values
*** Inserting Rows and Columns
*** Deleting Rows and Columns
*** Sorting Data

We can sort a vector with the =sort= function. Normally we have a data
frame of several columns (variables) and many, many rows
(observations). The goal is to shuffle the rows so that they are
ordered by the values of one or more columns. This is done with the
=order= function.

For example, we may sort all of the rows of the =Puromycin= data (in
ascending order) by the variable =conc= with the following:

#+BEGIN_SRC R :exports both :results output pp 
Tmp <- Puromycin[order(Puromycin$conc), ]
head(Tmp)
#+END_SRC

#+RESULTS:
:    conc rate     state
: 1  0.02   76   treated
: 2  0.02   47   treated
: 13 0.02   67 untreated
: 14 0.02   51 untreated
: 3  0.06   97   treated
: 4  0.06  107   treated

We can accomplish the same thing with the command 

#+BEGIN_SRC R :exports code :eval never
with(Puromycin, Puromycin[order(conc), ])
#+END_SRC

We can sort by more than one variable. To sort first by =state= and
next by =conc= do

#+BEGIN_SRC R :exports code :eval never
with(Puromycin, Puromycin[order(state, conc), ])
#+END_SRC

If we would like to sort a numeric variable in descending order then
we put a minus sign in front of it.

#+BEGIN_SRC R :exports both :results output pp 
Tmp <- with(Puromycin, Puromycin[order(-conc), ])
head(Tmp)
#+END_SRC

#+RESULTS:
:    conc rate     state
: 11 1.10  207   treated
: 12 1.10  200   treated
: 23 1.10  160 untreated
: 9  0.56  191   treated
: 10 0.56  201   treated
: 21 0.56  144 untreated

If we would like to sort by a character (or factor) in decreasing
order then we can use the =xtfrm= function which produces a numeric
vector in the same order as the character vector.

#+BEGIN_SRC R :exports both :results output pp 
Tmp <- with(Puromycin, Puromycin[order(-xtfrm(state)), ])
head(Tmp)
#+END_SRC

#+RESULTS:
:    conc rate     state
: 13 0.02   67 untreated
: 14 0.02   51 untreated
: 15 0.06   84 untreated
: 16 0.06   86 untreated
: 17 0.11   98 untreated
: 18 0.11  115 untreated

** Exporting Data
:PROPERTIES:
:CUSTOM_ID: sec-Exporting-a-Data
:END: 

The basic function is =write.table=. The =MASS= package \cite{MASS}
also has a =write.matrix= function.

** Reshaping Data
:PROPERTIES:
:CUSTOM_ID: sec-Reshaping-a-Data
:END: 

- Aggregation
- Convert Tables to data frames and back

=rbind=, =cbind=
=ab[order(ab[,1]),]=
=complete.cases=
=aggregate=
=stack=

* Mathematical Machinery                                           :appendix:
:PROPERTIES:
:tangle: R/17-appendix.R
:CUSTOM_ID: cha-Mathematical-Machinery
:END:

This appendix houses many of the standard definitions and theorems
that are used at some point during the narrative. It is targeted for
someone reading the book who forgets the precise definition of
something and would like a quick reminder of an exact statement. No
proofs are given, and the interested reader should consult a good text
on Calculus (say, Stewart \cite{Stewart2008} or Apostol
\cite{Apostol1967,ApostolI1967}), Linear Algebra (say, Strang
\cite{Strang1988} and Magnus \cite{Magnus1999}), Real Analysis (say,
Folland \cite{Folland1999}, or Carothers \cite{Carothers2000}), or
Measure Theory (Billingsley \cite{Billingsley1995}, Ash
\cite{Ash2000}, Resnick \cite{Resnick1999}) for details.

** Set Algebra
:PROPERTIES:
:CUSTOM_ID: sec-The-Algebra-of
:END: 


We denote sets by capital letters, \(A\), \(B\), \(C\), /etc/. The
letter \(S\) is reserved for the sample space, also known as the
universe or universal set, the set which contains all possible
elements. The symbol \(\emptyset\) represents the empty set, the set
with no elements.

*** Set Union, Intersection, and Difference

Given subsets \(A\) and \(B\), we may manipulate them in an algebraic
fashion. To this end, we have three set operations at our disposal:
union, intersection, and difference. Below is a table summarizing the
pertinent information about these operations.

#+NAME: tab-Set-Operations
#+CAPTION[Set operations]: Set operations.
| Name         | Denoted             | Defined by elements   | \(\mathsf{R}\) syntax |
|--------------+---------------------+-----------------------+-------------------|
| Union        | \(A\cup B\)           | in \(A\) or \(B\) or both | =union(A, B)=     |
| Intersection | \(A\cap B\)           | in both \(A\) and \(B\)   | =intersect(A, B)= |
| Difference   | \(A\backslash B\)     | in \(A\) but not in \(B\) | =setdiff(A, B)=   |
| Complement   | \(A^{c}\)             | in \(S\) but not in \(A\) | =setdiff(S, A)=   |

*** Identities and Properties

1. \(A\cup\emptyset=A,\quad A\cap\emptyset=\emptyset\)
1. \(A\cup S=S,\quad A\cap S=A\)
1. \(A\cup A^{c}=S\), \(A\cap A^{c}=\emptyset\)
1. \((A{}^{c})^{c}=A\)
1. The Commutative Property: 
   \begin{equation}
   A \cup B = B\cup A,\quad A\cap B = B\cap A
   \end{equation}
1. The Associative Property: 
   \begin{equation}
   (A\cup B)\cup C=A\cup(B\cup C),\quad (A\cap B)\cap C=A\cap(B\cap C)
   \end{equation}
1. The Distributive Property: 
   \begin{equation}
   A\cup(B\cap C)=(A\cup B)\cap(A\cup C),\quad A\cap(B\cup C)=(A\cap B)\cup(A\cap C)
   \end{equation}
1. DeMorgan's Laws
   \begin{equation}
   (A\cup B)^{c}=A^{c}\cap B^{c}\quad \mbox{and}\quad (A\cap B)^{c}=A^{c}\cup B^{c},
   \end{equation}
   or more generally,
   \begin{equation}
   \left(\bigcup_{\alpha}A_{\alpha}\right)^{c}=\bigcap_{\alpha}A_{\alpha}^{c},\quad \mbox{and}\quad \left(\bigcap_{\alpha}A_{\alpha}\right)^{c}=\bigcup_{\alpha}A_{\alpha}^{c}
   \end{equation}

** Differential and Integral Calculus
:PROPERTIES:
:CUSTOM_ID: sec-Differential-and-Integral
:END: 

A function \(f\) of one variable is said to be one-to-one if no two
distinct \(x\) values are mapped to the same \(y=f(x)\) value. To show
that a function is one-to-one we can either use the horizontal line
test or we may start with the equation \(f(x_{1}) = f(x_{2})\) and use
algebra to show that it implies \(x_{1} = x_{2}\).

*** Limits and Continuity
#+BEGIN_defn
Let \(f\) be a function defined on some open interval that contains
the number \(a\), except possibly at \(a\) itself. Then we say the
/limit of/ \(f(x)\) /as/ \(x\) /approaches/ \(a\) /is/ \(L\), and we
write
\begin{equation}
\lim_{x \to a}f(x) = L,
\end{equation}
if for every \(\epsilon > 0\) there exists a number \(\delta > 0\) such that \(0 < |x-a| < \delta\) implies \(|f(x) - L| < \epsilon\).
#+END_defn

#+BEGIN_defn
A function \(f\) is /continuous at a number/ \(a\) if 
\begin{equation}
\lim_{x \to a} f(x) = f(a).
\end{equation}
The function \(f\) is /right-continuous at the number/ \(a\) if
\(\lim_{x\to a^{+}}f(x)=f(a)\), and /left-continuous/ at \(a\) if
\(\lim_{x\to a^{-}}f(x)=f(a)\). Finally, the function \(f\) is
/continuous on an interval/ \(I\) if it is continuous at every number
in the interval.
#+END_defn

*** Differentiation
#+BEGIN_defn
The /derivative of a function/ \(f\) /at a number/ \(a\), denoted by
\(f'(a)\), is
\begin{equation}
f'(a)=\lim_{h\to0}\frac{f(a+h)-f(a)}{h},
\end{equation}
provided this limit exists.  A function is /differentiable at/ \(a\)
if \(f'(a)\) exists. It is /differentiable on an open interval/
\((a,b)\) if it is differentiable at every number in the interval.
#+END_defn

**** Differentiation Rules
In the table that follows, \(f\) and \(g\) are differentiable
functions and \(c\) is a constant.

#+NAME: tab-Differentiation-Rules
#+CAPTION[Differentiation rules]: Differentiation rules.
| \(\frac{\mathrm{d}}{\mathrm{d} x}c=0\) | \(\frac{\mathrm{d}}{\mathrm{d} x}x^{n}=nx^{n-1}\) | \((cf)'=cf'\)                                       |
| \((f\pm g)'=f'\pm g'\)       | \((fg)'=f'g+fg'\)                       | \(\left(\frac{f}{g}\right)'=\frac{f'g-fg'}{g^{2}}\) |

#+ATTR_LATEX: :options [\textbf{Chain Rule}]
#+BEGIN_thm
If \(f\) and \(g\) are both differentiable and \(F=f\circ g\) is the
composite function defined by \(F(x)=f[g(x)]\), then \(F\) is
differentiable and \(F'(x) = f'[ g(x) ] \cdot g'(x)\).
#+END_thm

**** Useful Derivatives

#+NAME: tab-Useful-Derivatives
#+CAPTION[Some derivatives]: Some derivatives.
| \(\frac{\mathrm{d}}{\mathrm{d} x}\mathrm{e}^{x}=\mathrm{e}^{x}\) | \(\frac{\mathrm{d}}{\mathrm{d} x}\ln x=x^{-1}\)     | \(\frac{\mathrm{d}}{\mathrm{d} x}\sin x=\cos x\)             |
| \(\frac{\mathrm{d}}{\mathrm{d} x}\cos x=-\sin x\)  | \(\frac{\mathrm{d}}{\mathrm{d} x}\tan x=\sec^{2}x\) | \(\frac{\mathrm{d}}{\mathrm{d} x}\tan^{-1}x=(1+x^{2})^{-1}\) |
|                                        |                                         |                                                  |

*** Optimization
#+BEGIN_defn
A /critical number/ of the function \(f\) is a value \(x^{\ast}\) for
which \(f'(x^{\ast})=0\) or for which \(f'(x^{\ast})\) does not exist.
#+END_defn

#+ATTR_LATEX: :options [\textbf{First Derivative Test}]
#+BEGIN_thm
<<thm-First-Derivative-Test>> If \(f\) is differentiable and if
\(x^{\ast}\) is a critical number of \(f\) and if \(f'(x)\geq0\) for
\(x\leq x^{\ast}\) and \(f'(x)\leq0\) for \(x\geq x^{\ast}\), then
\(x^{\ast}\) is a local maximum of \(f\). If \(f'(x)\leq0\) for
\(x\leq x^{\ast}\) and \(f'(x)\geq0\) for \(x\geq x^{\ast}\) , then
\(x^{\ast}\) is a local minimum of \(f\).
#+END_thm

#+ATTR_LATEX: :options [\textbf{Second Derivative Test}]
#+BEGIN_thm
If \(f\) is twice differentiable and if \(x^{\ast}\) is a critical
number of \(f\), then \(x^{\ast}\) is a local maximum of \(f\) if
\(f''(x^{\ast})<0\) and \(x^{\ast}\) is a local minimum of \(f\) if
\(f''(x^{\ast})>0\).
#+END_thm

*** Integration
As it turns out, there are all sorts of things called "integrals",
each defined in its own idiosyncratic way. There are /Riemann/
integrals, /Lebesgue/ integrals, variants of these called /Stieltjes/
integrals, /Daniell/ integrals, /Ito/ integrals, and the list
continues. Given that this is an introductory book, we will use the
Riemannian integral with the caveat that the Riemann integral is /not/
the integral that will be used in more advanced study.

#+BEGIN_defn
Let \(f\) be defined on \([a,b]\), a closed interval of the real
line. For each \(n\), divide \([a,b]\) into subintervals
\([x_{i},x_{i+1}]\), \(i=0,1,\ldots,n-1\), of length \(\Delta
x_{i}=(b-a)/n\) where \(x_{0}=a\) and \(x_{n}=b\), and let
\(x_{i}^{\ast}\) be any points chosen from the respective
subintervals. Then the /definite integral/ of \(f\) from \(a\) to
\(b\) is defined by
\begin{equation}
\int_{a}^{b}f(x)\,\mathrm{d} x=\lim_{n\to\infty}\sum_{i=0}^{n-1}f(x_{i}^{\ast})\,\Delta x_{i},
\end{equation}
provided the limit exists, and in that case, we say that \(f\) is
/integrable/ from \(a\) to \(b\).
#+END_defn

#+ATTR_LATEX: :options [\textbf{The Fundamental Theorem of Calculus}]
#+BEGIN_thm
Suppose \(f\) is continuous on \([a,b]\). Then
1. the function \(g\) defined by \(g(x)=\int_{a}^{x}f(t)\:\mathrm{d}
   t\), \(a\leq x\leq b\), is continuous on \([a,b]\) and
   differentiable on \((a,b)\) with \(g'(x)=f(x)\).
1. \(\int_{a}^{b}f(x)\,\mathrm{d} x=F(b)-F(a)\), where \(F\) is any
   /antiderivative/ of \(f\), that is, any function \(F\) satisfying
   \(F'=f\).
#+END_thm

**** Change of Variables

#+BEGIN_thm
If \(g\) is a differentiable function whose range is the interval
\([a,b]\) and if both \(f\) and \(g'\) are continuous on the range of
\(u = g(x)\), then
\begin{equation}
\int_{g(a)}^{g(b)}f(u)\:\mathrm{d} u=\int_{a}^{b}f[g(x)]\: g'(x)\:\mathrm{d} x.
\end{equation}
#+END_thm

**** Useful Integrals

#+NAME: tab-Useful-Integrals
#+CAPTION[Some integrals (constants of integration omitted)]: Some integrals (constants of integration omitted).
| \(\int x^{n}\,\mathrm{d} x=x^{n+1}/(n+1),\ n \neq - 1\) | \(\int\mathrm{e}^{x}\,\mathrm{d} x=\mathrm{e}^{x}\) | \(\int x^{-1}\,\mathrm{d} x=\ln \mathrm{abs}(x) \) |
| \(\int\tan x\:\mathrm{d} x=\ln \mathrm{abs}(\sec x)\) | \(\int a^{x}\,\mathrm{d} x=a^{x}/\ln a\)            | \(\int(x^{2}+1)^{-1}\,\mathrm{d} x=\tan^{-1}x\) |

**** Integration by Parts

\begin{equation}
\int u\:\mathrm{d} v=uv-\int v\:\mathrm{d} u
\end{equation}

#+ATTR_LATEX: :options [\textbf{L'H\^ opital's Rule}]
#+BEGIN_thm
Suppose \(f\) and \(g\) are differentiable and \(g'(x)\neq0\) near
\(a\), except possibly at \(a\). Suppose that the limit
\begin{equation}
\lim_{x\to a}\frac{f(x)}{g(x)}
\end{equation}
is an indeterminate form of type \(\frac{0}{0}\) or
\(\infty/\infty\). Then
\begin{equation}
\lim_{x\to a}\frac{f(x)}{g(x)}=\lim_{x\to a}\frac{f'(x)}{g'(x)},
\end{equation}
provided the limit on the right-hand side exists or is infinite.
#+END_thm

**** Improper Integrals

If \(\int_{a}^{t}f(x)\mathrm{d} x\) exists for every number \(t\geq
a\), then we define
\begin{equation}
\int_{a}^{\infty}f(x)\,\mathrm{d} x=\lim_{t\to\infty}\int_{a}^{t}f(x)\,\mathrm{d} x,
\end{equation}
provided this limit exists as a finite number, and in that case we say
that \(\int_{a}^{\infty}f(x)\,\mathrm{d} x\) is
/convergent/. Otherwise, we say that the improper integral is
/divergent/.

If \(\int_{t}^{b}f(x)\,\mathrm{d} x\) exists for every number \(t\leq
b\), then we define
\begin{equation}
\int_{-\infty}^{b}f(x)\,\mathrm{d} x=\lim_{t\to-\infty}\int_{t}^{b}f(x)\,\mathrm{d} x,
\end{equation}
provided this limit exists as a finite number, and in that case we say
that \(\int_{-\infty}^{b}f(x)\,\mathrm{d} x\) is
/convergent/. Otherwise, we say that the improper integral is
/divergent/.

If both \(\int_{a}^{\infty}f(x)\,\mathrm{d} x\) and
\(\int_{-\infty}^{a}f(x)\,\mathrm{d} x\) are convergent, then we
define
\begin{equation}
\int_{-\infty}^{\infty}f(x)\,\mathrm{d} x=\int_{-\infty}^{a}f(x)\,\mathrm{d} x+\int_{a}^{\infty}f(x)\mathrm{d} x,
\end{equation}
and we say that \(\int_{-\infty}^{\infty}f(x)\,\mathrm{d} x\) is
/convergent/. Otherwise, we say that the improper integral is
/divergent/.

** Sequences and Series
:PROPERTIES:
:CUSTOM_ID: sec-Sequences-and-Series
:END: 

A /sequence/ is an ordered list of numbers, \(a_{1}\), \(a_{2}\),
\(a_{3}\), ..., \(a_{n} =\left(a_{k}\right)_{k=1}^{n}\). A sequence
may be finite or infinite. In the latter case we write \(a_{1}\),
\(a_{2}\), \(a_{3}\), ... \( =\left(a_{k}\right)_{k=1}^{\infty}\). We
say that /the infinite sequence/ \(\left(a_{k}\right)_{k=1}^{\infty}\)
/converges to the finite limit/ L, and we write
\begin{equation}
\lim_{k\to\infty}a_{k} = L,
\end{equation}
if for every \(\epsilon > 0\) there exists an integer \(N \geq 1\)
such that \(|a_{k} - L| < \epsilon\) for all \(k \geq N\). We say that
/the infinite sequence/ \(\left(a_{k}\right)_{k=1}^{\infty}\)
/diverges to/ \(+\infty\) (or \( -\infty\)) if for every \(M\geq0\)
there exists an integer \(N\geq1\) such that \(a_{k} \geq M\) for all
\(k \geq N\) (or \(a_{k} \leq - M\) for all \(k \geq N\)).

*** Finite Series

\begin{equation}
\label{eq-gauss-series}
\sum_{k=1}^{n}k=1+2+\cdots+n=\frac{n(n+1)}{2}
\end{equation}
\begin{equation}
\label{eq-gauss-series-sq}
\sum_{k=1}^{n}k^{2}=1^{2}+2^{2}+\cdots+n^{2}=\frac{n(n+1)(2n+3)}{6}
\end{equation}

**** The Binomial Series
\begin{equation}
\label{eq-binom-series}
\sum_{k=0}^{n}{n \choose k}\, a^{n-k}b^{k}=(a+b)^{n}
\end{equation}

*** Infinite Series

Given an infinite sequence of numbers \(a_{1}\), \(a_{2}\), \(a_{3}\),
...\(=\left(a_{k}\right)_{k=1}^{\infty}\), let \(s_{n}\) denote the
/partial sum/ of the first \(n\) terms:
\begin{equation}
s_{n}=\sum_{k=1}^{n}a_{k}=a_{1}+a_{2}+\cdots+a_{n}.
\end{equation}
If the sequence \(\left(s_{n}\right)_{n=1}^{\infty}\) converges to a
finite number \(S\) then we say that the infinite series
\(\sum_{k}a_{k}\) is /convergent/ and write
\begin{equation}
\sum_{k=1}^{\infty}a_{k}=S.
\end{equation}
Otherwise we say the infinite series is /divergent/.

*** Rules for Series

Let \(\left(a_{k}\right)_{k=1}^{\infty}\) and
\(\left(b_{k}\right)_{k=1}^{\infty}\) be infinite sequences and let
\(c\) be a constant.

\begin{equation}
\sum_{k=1}^{\infty}ca_{k}=c\sum_{k=1}^{\infty}a_{k}
\end{equation}
\begin{equation}
\sum_{k=1}^{\infty}(a_{k}\pm b_{k})=\sum_{k=1}^{\infty}a_{k}\pm\sum_{k=1}^{\infty}b_{k}
\end{equation}

In both of the above the series on the left is convergent if the
series on the right is (are) convergent.

**** The Geometric Series
\begin{equation}
\label{eq-geom-series}
\sum_{k=0}^{\infty} x^{k} = \frac{1}{1 - x},\quad |x| < 1.
\end{equation}

**** The Exponential Series
\begin{equation}
\label{eq-exp-series}
\sum_{k=0}^{\infty}\frac{x^{k}}{k!} = \mathrm{e}^{x},\quad -\infty < x < \infty.
\end{equation}

Other Series
\begin{equation}
\label{eq-negbin-series}
\sum_{k=0}^{\infty}{m+k-1 \choose m-1}x^{k}=\frac{1}{(1-x)^{m}},\quad |x|<1.
\end{equation}

\begin{equation}
\label{eq-log-series}
-\sum_{k=1}^{\infty}\frac{x^{n}}{n}=\ln(1-x),\quad |x| < 1.
\end{equation}
\begin{equation}
\label{eq-binom-series-infinite}
\sum_{k=0}^{\infty}{n \choose k}x^{k}=(1+x)^{n},\quad |x| < 1.
\end{equation}

*** Taylor Series

If the function \(f\) has a /power series/ representation at the point
\(a\) with radius of convergence \(R>0\), that is, if
\begin{equation}
f(x)=\sum_{k=0}^{\infty}c_{k}(x-a)^{k},\quad |x - a| < R,
\end{equation}
for some constants \(\left(c_{k}\right)_{k=0}^{\infty}\), then \(c_{k}\) must be
\begin{equation}
c_{k}=\frac{f^{(k)}(a)}{k!},\quad k=0,1,2,\ldots
\end{equation}
Furthermore, the function \(f\) is differentiable on the open interval
\((a-R,\, a+R)\) with
\begin{equation}
f'(x)=\sum_{k=1}^{\infty}kc_{k}(x-a)^{k-1},\quad |x-a| < R,
\end{equation}
\begin{equation}
\int f(x)\,\mathrm{d} x=C+\sum_{k=0}^{\infty}c_{k}\frac{(x-a)^{k+1}}{k+1},\quad |x-a| < R,
\end{equation}
in which case both of the above series have radius of convergence
\(R\).

** The Gamma Function
:PROPERTIES:
:CUSTOM_ID: sec-The-Gamma-Function
:END: 

The /Gamma function/ \(\Gamma\) will be defined in this book according
to the formula
\begin{equation}
\Gamma(\alpha)=\int_{0}^{\infty}x^{\alpha-1}\mathrm{e}^{-x}\:\mathrm{d} x,\quad \mbox{for }\alpha > 0.
\end{equation}

#+BEGIN_fact
Properties of the Gamma Function:
- \(\Gamma(\alpha)=(\alpha - 1)\Gamma(\alpha - 1)\) for any \(\alpha >
  1\), and so \(\Gamma(n)=(n-1)!\) for any positive integer \(n\).
- \(\Gamma(1/2)=\sqrt{\pi}\).
#+END_fact

** Linear Algebra
:PROPERTIES:
:CUSTOM_ID: sec-Linear-Algebra
:END: 

*** Matrices
A /matrix/ is an ordered array of numbers or expressions; typically we
write \(\mathbf{A}=\begin{pmatrix}a_{ij}\end{pmatrix}\) or
\(\mathbf{A}=\begin{bmatrix}a_{ij}\end{bmatrix}\). If \(\mathbf{A}\)
has \(m\) rows and \(n\) columns then we write
\begin{equation}
\mathbf{A}_{\mathrm{m}\times\mathrm{n}}=\begin{bmatrix}a_{11} & a_{12} & \cdots & a_{1n}\\
a_{21} & a_{22} & \cdots & a_{2n}\\
\vdots & \vdots & \ddots & \vdots\\
a_{m1} & a_{m2} & \cdots & a_{mn}\end{bmatrix}.
\end{equation}
The /identity matrix/ \(\mathbf{I}_{\mathrm{n}\times\mathrm{n}}\) is
an \(\mathrm{n}\times\mathrm{n}\) matrix with zeros everywhere except
for 1's along the main diagonal:
\begin{equation}
\mathbf{I}_{\mathrm{n}\times\mathrm{n}}=\begin{bmatrix}1 & 0 & \cdots & 0\\
0 & 1 & \cdots & 0\\
\vdots & \vdots & \ddots & \vdots\\
0 & 0 & \cdots & 1\end{bmatrix}.
\end{equation}
and the matrix with ones everywhere is denoted
\(\mathbf{J}_{\mathrm{n}\times\mathrm{n}}\):
\begin{equation}
\mathbf{J}_{\mathrm{n}\times\mathrm{n}}=\begin{bmatrix}1 & 1 & \cdots & 1\\
1 & 1 & \cdots & 1\\
\vdots & \vdots & \ddots & \vdots\\
1 & 1 & \cdots & 1\end{bmatrix}.
\end{equation}

A /vector/ is a matrix with one of the dimensions equal to one, such
as \(\mathbf{A}_{\mathrm{m}\times1}\) (a column vector) or
\(\mathbf{A}_{\mathrm{1}\times\mathrm{n}}\) (a row vector). The /zero
vector/ \(\mathbf{0}_{\mathrm{n}\times1}\) is an \(\mathrm{n}\times1\)
matrix of zeros:
\begin{equation}
\mathbf{0}_{\mathrm{n}\times1}=\begin{bmatrix}0 & 0 & \cdots & 0\end{bmatrix}^{\mathrm{T}}.
\end{equation}

The /transpose/ of a matrix
\(\mathbf{A}=\begin{pmatrix}a_{ij}\end{pmatrix}\) is the matrix
\(\mathbf{A}^{\mathrm{T}}=\begin{pmatrix}a_{ji}\end{pmatrix}\), which
is just like \(\mathbf{A}\) except the rows are columns and the
columns are rows. The matrix \(\mathbf{A}\) is said to be /symmetric/
if \(\mathbf{A}^{\mathrm{T}}=\mathbf{A}\). Note that
\(\left(\mathbf{A}\mathbf{B}\right)^{\mathrm{T}}=\mathbf{B}^{\mathrm{T}}\mathbf{A}^{\mathrm{T}}\).

The /trace/ of a square matrix \(\mathbf{A}\) is the sum of its
diagonal elements: \(\mathrm{tr}(\mathbf{A})=\sum_{i}a_{ii}\).

The /inverse/ of a square matrix
\(\mathbf{A}_{\mathrm{n}\times\mathrm{n}}\) (when it exists) is the
unique matrix denoted \(\mathbf{A}^{-1}\) which satisfies
\(\mathbf{A}\mathbf{A}^{-1}=\mathbf{A}^{-1}\mathbf{A}=\mathbf{I}_{\mathrm{n}\times\mathrm{n}}\). If
\(\mathbf{A}^{-1}\) exists then we say \(\mathbf{A}\) is /invertible/,
or /nonsingular/. Note that
\(\left(\mathbf{A}^{\mathrm{T}}\right)^{-1}=\left(\mathbf{A}^{\mathrm{-1}}\right)^{\mathrm{T}}\).
#+BEGIN_fact
The inverse of the \(2\times2\) matrix
\begin{equation}
\mathbf{A}=\begin{bmatrix}a & b\\
c & d\end{bmatrix}\quad \mbox{is}\quad \mathbf{A}^{-1}=\frac{1}{ad-bc}\begin{bmatrix}d & -b\\
-c & a\end{bmatrix},
\end{equation}
provided \(ad-bc\neq0\).
#+END_fact

*** Determinants
#+BEGIN_defn
The /determinant/ of a square matrix \(\mathbf{A}_{\mathrm{n}\times
n}\) is denoted \(\mathrm{det}(\mathbf{A})\) or \(|\mathbf{A}|\) and
is defined recursively by
\begin{equation}
\mathrm{det}(\mathbf{A})=\sum_{i=1}^{n}(-1)^{i+j}a_{ij}\,\mathrm{det}(\mathbf{M}_{ij}),
\end{equation}
where \(\mathbf{M}_{ij}\) is the submatrix formed by deleting the
\(i^{\mathrm{th}}\) row and \(j^{\mathrm{th}}\) column of
\(\mathbf{A}\). We may choose any fixed \(1\leq j\leq n\) we wish to
compute the determinant; the final result is independent of the \(j\)
chosen.
#+END_defn
#+BEGIN_fact
The determinant of the \(2\times2\) matrix
\begin{equation}
\mathbf{A}=\begin{bmatrix}a & b\\
c & d\end{bmatrix}\quad \mbox{is} \quad |\mathbf{A}|=ad-bc.
\end{equation}
#+END_fact

#+BEGIN_fact
A square matrix \(\mathbf{A}\) is nonsingular if and only if
\(\mathrm{det}(\mathbf{A})\neq0\).
#+END_fact

*** Positive (Semi)Definite
If the matrix \(\mathbf{A}\) satisfies
\(\mathbf{x^{\mathrm{T}}}\mathbf{A}\mathbf{x}\geq0\) for all vectors
\(\mathbf{x}\neq\mathbf{0}\), then we say that \(\mathbf{A}\) is
/positive semidefinite/. If strict inequality holds for all
\(\mathbf{x}\neq\mathbf{0}\), then \(\mathbf{A}\) is /positive
definite/. The connection to statistics is that covariance matrices
(see Chapter [[#cha-Multivariable-Distributions]]) are always positive
semidefinite, and many of them are even positive definite.

** Multivariable Calculus
:PROPERTIES:
:CUSTOM_ID: sec-Multivariable-Calculus
:END: 

*** Partial Derivatives
If \(f\) is a function of two variables, its /first-order partial
derivatives/ are defined by
\begin{equation}
\frac{\partial f}{\partial x}=\frac{\partial}{\partial x}f(x,y)=\lim_{h\to0}\frac{f(x+h,\, y)-f(x,y)}{h}
\end{equation}
and
\begin{equation}
\frac{\partial f}{\partial y}=\frac{\partial}{\partial y}f(x,y)=\lim_{h\to0}\frac{f(x,\, y+h)-f(x,y)}{h},
\end{equation}
provided these limits exist. The /second-order partial derivatives/ of
\(f\) are defined by
\begin{equation}
\frac{\partial^{2}f}{\partial x^{2}}=\frac{\partial}{\partial x}\left(\frac{\partial f}{\partial x}\right),\quad \frac{\partial^{2}f}{\partial y^{2}}=\frac{\partial}{\partial y}\left(\frac{\partial f}{\partial y}\right),\quad \frac{\partial^{2}f}{\partial x\partial y}=\frac{\partial}{\partial x}\left(\frac{\partial f}{\partial y}\right),\quad \frac{\partial^{2}f}{\partial y\partial x}=\frac{\partial}{\partial y}\left(\frac{\partial f}{\partial x}\right).
\end{equation}
In many cases (and for all cases in this book) it is true that
\begin{equation}
\frac{\partial^{2}f}{\partial x\partial y}=\frac{\partial^{2}f}{\partial y\partial x}.
\end{equation}

*** Optimization
An function \(f\) of two variables has a /local maximum/ at \((a,b)\)
if \(f(x,y)\geq f(a,b)\) for all points \((x,y)\) near \((a,b)\), that
is, for all points in an open disk centered at \((a,b)\). The number
\(f(a,b)\) is then called a /local maximum value/ of \(f\). The
function \(f\) has a /local minimum/ if the same thing happens with
the inequality reversed.

Suppose the point \((a,b)\) is a /critical point/ of \(f\), that is,
suppose \((a,b)\) satisfies
\begin{equation}
\frac{\partial f}{\partial x}(a,b)=\frac{\partial f}{\partial y}(a,b)=0.
\end{equation}
Further suppose \(\frac{\partial^{2}f}{\partial x^{2}}\) and
\(\frac{\partial^{2}f}{\partial y^{2}}\) are continuous near
\((a,b)\). Let the /Hessian matrix/ \(H\) (not to be confused with the
/hat matrix/ \(\mathbf{H}\) of Chapter [[#cha-multiple-linear-regression]]) be
defined by
\begin{equation}
H = 
\begin{bmatrix}
\frac{\partial^{2}f}{\partial x^{2}} & \frac{\partial^{2}f}{\partial x\partial y}\\
\frac{\partial^{2}f}{\partial y\partial x} & \frac{\partial^{2}f}{\partial y^{2}}
\end{bmatrix}.
\end{equation}
We use the following rules to decide whether \((a,b)\) is an
/extremum/ (that is, a local minimum or local maximum) of \(f\).
- If \(\mbox{det}(H)>0\) and \(\frac{\partial^{2}f}{\partial
  x^{2}}(a,b)>0\), then \((a,b)\) is a local minimum of \(f\).
- If \(\mbox{det}(H)>0\) and \(\frac{\partial^{2}f}{\partial
  x^{2}}(a,b)<0\), then \((a,b)\) is a local maximum of \(f\).
- If \(\mbox{det}(H)<0\), then \((a,b)\) is a /saddle point/ of \(f\)
  and so is not an extremum of \(f\).
- If \(\mbox{det}(H)=0\), then we do not know the status of \((a,b)\);
  it might be an extremum or it might not be.

*** Double and Multiple Integrals
Let \(f\) be defined on a rectangle \(R=[a,b]\times[c,d]\), and for
each \(m\) and \(n\) divide \([a,b]\) (respectively \([c,d]\)) into
subintervals \([x_{j},x_{j+1}]\), \(i=0,1,\ldots,m-1\) (respectively
\([y_{i},y_{i+1}]\)) of length \(\Delta x_{j}=(b-a)/m\) (respectively
\(\Delta y_{i}=(d-c)/n\)) where \(x_{0}=a\) and \(x_{m}=b\) (and
\(y_{0}=c\) and \(y_{n}=d\) ), and let \(x_{j}^{\ast}\)
(\(y_{i}^{\ast}\)) be any points chosen from their respective
subintervals. Then the /double integral/ of \(f\) over the rectangle
\(R\) is
\begin{equation}
\iintop_{R}f(x,y)\,\mathrm{d} A=\intop_{c}^{d}\!\!\!\intop_{a}^{b}f(x,y)\,\mathrm{d} x\mathrm{d} y=\lim_{m,n\to\infty}\sum_{i=1}^{n}\sum_{j=1}^{m}f(x_{j}^{\ast},y_{i}^{\ast})\Delta x_{j}\Delta y_{i},
\end{equation}
provided this limit exists. Multiple integrals are defined in the same
way just with more letters and sums.

*** Bivariate and Multivariate Change of Variables
Suppose we have a transformation[fn:fn-trans] \(T\) that maps points
\((u,v)\) in a set \(A\) to points \((x,y)\) in a set \(B\). We
typically write \(x=x(u,v)\) and \(y=y(u,v)\), and we assume that
\(x\) and \(y\) have continuous first-order partial derivatives. We
say that \(T\) is /one-to-one/ if no two distinct \((u,v)\) pairs get
mapped to the same \((x,y)\) pair; in this book, all of our
multivariate transformations \(T\) are one-to-one.

[fn:fn-trans] For our purposes \(T\) is in fact the /inverse/ of a
one-to-one transformation that we are initially given. We usually
start with functions that map \((x,y) \longmapsto (u,v)\), and one of
our first tasks is to solve for the inverse transformation that maps
\((u,v)\longmapsto(x,y)\). It is this inverse transformation which we
are calling \(T\).

The /Jacobian/ (pronounced "yah-KOH-bee-uhn") of \(T\) is denoted by
\(\partial(x,y)/\partial(u,v)\) and is defined by the determinant of
the following matrix of partial derivatives:
\begin{equation}
\frac{\partial(x,y)}{\partial(u,v)}=\left|
\begin{array}{cc}
\frac{\partial x}{\partial u} & \frac{\partial x}{\partial v}\\
\frac{\partial y}{\partial u} & \frac{\partial y}{\partial v}
\end{array}
\right|=\frac{\partial x}{\partial u}\frac{\partial y}{\partial v}-\frac{\partial x}{\partial v}\frac{\partial y}{\partial u}.
\end{equation}

If the function \(f\) is continuous on \(A\) and if the Jacobian of
\(T\) is nonzero except perhaps on the boundary of \(A\), then
\begin{equation}
\iint_{B}f(x,y)\,\mathrm{d} x\,\mathrm{d} y=\iint_{A}f\left[x(u,v),\, y(u,v)\right]\ \left|\frac{\partial(x,y)}{\partial(u,v)}\right|\mathrm{d} u\,\mathrm{d} v.
\end{equation} 

A multivariate change of variables is defined in an analogous way: the
one-to-one transformation \(T\) maps points
\((u_{1},u_{2},\ldots,u_{n})\) to points
\((x_{1},x_{2},\ldots,x_{n})\), the Jacobian is the determinant of the
\(\mathrm{n}\times\mathrm{n}\) matrix of first-order partial
derivatives of \(T\) (lined up in the natural manner), and instead of
a double integral we have a multiple integral over multidimensional
sets \(A\) and \(B\).

* Writing Reports with \(\mathsf{R}\)                              :appendix:
:PROPERTIES:
:tangle: R/17-appendix.R
:CUSTOM_ID: cha-Writing-Reports-with
:END:

Perhaps the most important part of a statistician's job once the
analysis is complete is to communicate the results to others. This is
usually done with some type of report that is delivered to the client,
manager, or administrator. Other situations that call for reports
include term papers, final projects, thesis work, /etc/. This chapter
is designed to pass along some tips about writing reports once the
work is completed with \(\mathsf{R}\).

** What to Write
:PROPERTIES:
:CUSTOM_ID: sec-What-to-Write
:END: 

It is possible to summarize this entire appendix with only one
sentence: /the statistician's goal is to communicate with others/. To
this end, there are some general guidelines that I give to students
which are based on an outline originally written and shared with me by
Dr. G. Andy Chang.

*** Basic Outline for a Statistical Report
1. Executive Summary (a one page description of the study and
   conclusion)
1. Introduction 
   1. What is the question, and why is it important?
   1. Is the study observational or experimental?
   1. What are the hypotheses of interest to the researcher?
   1. What are the types of analyses employed? (one sample \(t\)-test,
      paired-sample \(t\)-test, ANOVA, chi-square test, regression,
      ...)
1. Data Collection 
   1. Describe how the data were collected in detail.
   1. Identify all variable types: quantitative, qualitative, ordered
      or nominal (with levels), discrete, continuous.
   1. Discuss any limitations of the data collection procedure. Look
      carefully for any sources of bias.
1. Summary Information
   1. Give numeric summaries of all variables of interest.
      1. Discrete: (relative) frequencies, contingency tables, odds
         ratios, /etc/.
      1. Continuous: measures of center, spread, shape.
   1. Give visual summaries of all variables of interest.
      1. Side-by-side boxplots, scatterplots, histograms, /etc/.
   1. Discuss any unusual features of the data (outliers, clusters,
      granularity, /etc/.)
   1. Report any missing data and identify any potential problems or
      bias.
1. Analysis 
   1. State any hypotheses employed, and check the assumptions. 
   1. Report test statistics, /p/-values, and confidence intervals. 
   1. Interpret the results in the context of the study.
   1. Attach (labeled) tables and/or graphs and make reference to them
      in the report as needed.
1. Conclusion
   1. Summarize the results of the study. What did you learn?
   1. Discuss any limitations of the study or inferences.
   1. Discuss avenues of future research suggested by the study. 

** How to Write It with R
:PROPERTIES:
:CUSTOM_ID: sec-How-to-Write
:END: 

Once the decision has been made what to write, the next task is to
typeset the information to be shared. To do this the author will need
to select software to use to write the documents. There are many
options available, and choosing one over another is sometimes a matter
of taste. But not all software were created equal, and \(\mathsf{R}\)
plays better with some applications than it does with others.  In
short, \(\mathsf{R}\) does great with \LaTeX{} and there are many
resources available to make writing a document with \(\mathsf{R}\) and
\LaTeX{} easier. But \LaTeX{} is not for the beginner, and there are
other word processors which may be acceptable depending on the
circumstances.

*** Microsoft\(\circledR\) Word
It is a fact of life that Microsoft\(\circledR\) Windows is currently
the most prevalent desktop operating system on the planet. Those who
own Windows also typically own some version of Microsoft Office, thus
Microsoft Word is the default word processor for many, many people.

The standard way to write an \(\mathsf{R}\) report with
Microsoft\(\circledR\) Word is to generate material with
\(\mathsf{R}\) and then copy-paste the material at selected places in
a Word document. An advantage to this approach is that Word is nicely
designed to make it easy to copy-and-paste from =RGui= to the Word
document.

A disadvantage to this approach is that the R input/output needs to be
edited manually by the author to make it readable for others. Another
disadvantage is that the approach does not work on all operating
systems (not on Linux, in particular). Yet another disadvantage is
that Microsoft\(\circledR\) Word is proprietary, and as a result,
\(\mathsf{R}\) does not communicate with Microsoft\(\circledR\) Word
as well as it does with other software as we shall soon see.

Nevertheless, if you are going to write a report with Word there are
some steps that you can take to make the report more amenable to the
reader.

1. Copy and paste graphs into the document. You can do this by right
   clicking on the graph and selecting =Copy as bitmap=, or =Copy as
   metafile=, or one of the other options. Then move the cursor to the
   document where you want the picture, right-click, and select
   =Paste=.
1. Resize (most) pictures so that they take up no more than 1/2
   page. You may want to put graphs side by side; do this by inserting
   a table and placing the graphs inside the cells.
1. Copy selected \(\mathsf{R}\) input and output to the Word
   document. All code should be separated from the rest of the
   writing, except when specifically mentioning a function or object
   in a sentence.
1. The font of \(\mathsf{R}\) input/output should be Courier New, or
   some other monowidth font (not Times New Roman or Calibri); the
   default font size of =12= is usually too big for \(\mathsf{R}\)
   code and should be reduced to, for example, =10pt=.

It is also possible to communicate with \(\mathsf{R}\) through
OpenOffice.org, which can export to the proprietary (=.doc=) format.

*** OpenOffice.org and \texttt{odfWeave}
OpenOffice.org (OO.o) is an open source desktop productivity suite
which mirrors Microsoft\(\circledR\) Office. It is especially nice
because it works on all operating systems. OO.o can read most document
formats, and in particular, it will read =.doc= files. The standard
OO.o file extension for documents is =.odt=, which stands for "open
document text".

The =odfWeave= package \cite{odfWeave} provides a way to generate an
=.odt= file with \(\mathsf{R}\) input and output code formatted
correctly and inserted in the correct places, without any additional
work. In this way, one does not need to worry about all of the trouble
of typesetting \(\mathsf{R}\) output. Another advantage of =odfWeave=
is that it allows you to generate the report dynamically; if the data
underlying the report change or are updated, then a few clicks (or
commands) will generate a brand new report.

One disadvantage is that the source =.odt= file is not easy to read,
because it is difficult to visually distinguish the noweb parts (where
the \(\mathsf{R}\) code is) from the non-noweb parts. This can be
fixed by manually changing the font of the noweb sections to, for
instance, Courier font, size =10pt=. But it is extra work. It would be
nice if a program would discriminate between the two different
sections and automatically typeset the respective parts in their
correct fonts. This is one of the advantages to \LyX{}.

Another advantage of OO.o is that even after you have generated the
outfile, it is fully editable just like any other =.odt= document. If
there are errors or formatting problems, they can be fixed at any
time.

Here are the basic steps to typeset a statistical report with OO.o.

1. Write your report as an =.odt= document in OO.o just as you would
   any other document. Call this document =infile.odt=, and make sure
   that it is saved in your working directory.
1. At the places you would like to insert \(\mathsf{R}\) code in the
   document, write the code chunks in the following format:
   \texttt{<\textcompwordmark{}<>\textcompwordmark{}>=}~\\
   \texttt{x <- rnorm(10)}~\\
   \texttt{mean(x)}~\\
   \texttt{@} or write whatever code you want between the symbols
   \texttt{<\textcompwordmark{}<>\textcompwordmark{}>=} and
   \texttt{@}.
1. Open \(\mathsf{R}\) and type the following:
   #+BEGIN_SRC R :exports code :eval never
   library("odfWeave")
   odfWeave(file = "infile.odt", dest = "outfile.odt")
   #+END_SRC
1. The compiled (=.odt=) file, complete with all of the \(\mathsf{R}\)
   output automatically inserted in the correct places, will now be
   the file =outfile.odt= located in the working directory. Open
   =outfile.odt=, examine it, modify it, and repeat if desired.

There are all sorts of extra things that you can do. For example, the
\(\mathsf{R}\) commands can be suppressed with the tag
\texttt{<\textcompwordmark{}<echo = FALSE>\textcompwordmark{}>=}, and
the \(\mathsf{R}\) output may be hidden with
\texttt{<\textcompwordmark{}<results =
hide>\textcompwordmark{}>=}. See the =odfWeave= package documentation
for details.

*** Sweave and \protect\LaTeX{}

This approach is nice because it works for all operating systems. One
can quite literally typeset /anything/ with \LaTeX{}. All of this
power comes at a price, however. The writer must learn the \LaTeX{}
language which is a nontrivial enterprise. Even given the language, if
there is a single syntax error, or a single delimeter missing in the
entire document, then the whole thing breaks.

\LaTeX{} can do anything, but it is relatively difficult to learn and
very grumpy about syntax errors and delimiter matching. There are
however programs useful for formatting LaTeX.

A disadvantage is that you cannot see the mathematical formulas until
you run the whole file with \LaTeX{}.

A disadvantage is that figures and tables are relatively difficult.

There are programs to make the process easier: AUC\TeX{}.

dev.copy2eps, also dev.copy2pdf

[[http://www.stat.uni-muenchen.de/~leisch/Sweave/]]

*** Sweave and LyX
This approach is nice because it works for all operating systems. It
gives you everything from the last section and makes it easier to use
\LaTeX{}. That being said, it is better to know \LaTeX{} already when
migrating to \LyX{}, because you understand all of the machinery going
on under the hood.

Program Listings and the \(\mathsf{R}\) language
[[http://gregor.gorjanc.googlepages.com/lyx-sweave]]

** Formatting Tables
:PROPERTIES:
:CUSTOM_ID: sec-Formatting-Tables
:END: 

The prettyR package \cite{prettyR}.

The Hmisc package \cite{Hmisc}.

#+BEGIN_SRC R :exports both :results output pp 
summary(cbind(Sepal.Length, Sepal.Width) ~ Species, data = iris)
#+END_SRC

#+RESULTS:
#+BEGIN_example
cbind(Sepal.Length, Sepal.Width)    N=150

+-------+----------+---+------------+-----------+
|       |          |N  |Sepal.Length|Sepal.Width|
+-------+----------+---+------------+-----------+
|Species|setosa    | 50|5.006000    |3.428000   |
|       |versicolor| 50|5.936000    |2.770000   |
|       |virginica | 50|6.588000    |2.974000   |
+-------+----------+---+------------+-----------+
|Overall|          |150|5.843333    |3.057333   |
+-------+----------+---+------------+-----------+
#+END_example

There is a =method= argument to =summary=, which is set to
\texttt{method = "response"} by default. There are two other methods
for summarizing data: =reverse= and =cross=. See =?summary.formula= or
the [[http://biostat.mc.vanderbilt.edu/twiki/bin/view/Main/StatReport][following document]] from Frank Harrell for more details.

** Other Formats
:PROPERTIES:
:CUSTOM_ID: sec-Other-Formats
:END: 

HTML and prettyR
R2HTML

* Instructions for Instructors					   :appendix:
:PROPERTIES:
:tangle: R/18-appendix.R
:CUSTOM_ID: cha-instruct-for-instruct
:END:

** Course schedule by Module
| Module (begin/end) | IPSUR Reading  | Assessments | Videos          |
|--------------------+----------------+-------------+-----------------|
| 1  (Week 1)        | Chapter 1 - 2  |             | 082310 - 082510 |
| 2  (Weeks 2/4)     | Chapter 3      |             | 083010 - 091310 |
|                    |                | Exam One    |                 |
| 3  (Weeks 4/5)     | Chapter 4      |             | 091510 - 092010 |
| 4  (Weeks 5/6)     | Chapter 5      |             | 092210 - 092710 |
| 5  (Weeks 6/8)     | Chapter 6      |             | 092910 - 101110 |
|                    |                | Exam Two    |                 |
| 6  (Weeks 8/9)     | 7.1,2,4, 8.1-5 |             | 101310 - 102010 |
| 7  (Weeks 10/11)   | 9.1-4,7        |             | 102510 - 110110 |
| 8  (Weeks 11/13)   | 10.1 - 10.4    |             | 110310 - 111510 |
|                    |                | Exam Three  |                 |
| 9  (Weeks 13/14)   | Chapter 11     |             | 111710 - 112410 |
| 10  (Week 15)      | 10.5 - 10.6    |             | 112910 - 120110 |
| Final  (Week 16)   | Everything     | Final Exam  | Everything      |
|--------------------+----------------+-------------+-----------------|

** Course Schedule by Week

| Dates  | Video          | IPSUR     |
|--------+----------------+-----------|
| Week 1 |                |           |
|        | 082310/        | 1.1 - 1.2 |
|        | 082510/        | 1.1 - 1.2 |
|        | 01-briefintroR | 2.1 - 2.6 |
| Week 2 |                |           |
|        | 083010/        | 3.1 - 3.2 |
|        | 090110/        | 3.2 - 3.3 |
| Week 3 |                |           |
|        | 090610*        | Nothing   |
|        | 090810/        | 3.3 - 3.5 |
| Week 4 |                |           |
|        | 091310/        | 3.5 - 4.1 |
|        | 091510/        | 4.1 - 4.5 |
|--------+----------------+-----------|

Exam I covers 082310 - 090810.

|        | Video   | IPSUR        |
|--------+---------+--------------|
| Week 5 |         |              |
|        | 092010/ | 4.5 - 4.8    |
|        | 092210/ | 4.8 - 5.3    |
| Week 6 |         |              |
|        | 092710/ | 5.4 - 5.6    |
|        | 092910/ | 5.6 - 6.1    |
| Week 7 |         |              |
|        | 100410/ | 6.1 - 6.3    |
|        | 100610/ | 6.3 - 6.5    |
| Week 8 |         |              |
|        | 101110/ | 6.5 - 7.1    |
|        | 101310/ | 7.1,2,4, 8.1 |
|--------+---------+--------------|

Exam II covers 091310 - 100610
(note: exam coverage is cumulative)

|         | Video   | IPSUR        |
|---------+---------+--------------|
| Week 9  |         |              |
|         | 101810/ | 8.1 - 8.3    |
|         | 102010/ | 8.3 - 9.1    |
| Week 10 |         |              |
|         | 102510/ | 9.1 - 9.2    |
|         | 102710/ | 9.2 - 9.3    |
| Week 11 |         |              |
|         | 110110/ | 9.3 - 9.4,7  |
|         | 110310/ | 10.1 - 10.3  |
| Week 12 |         |              |
|         | 110810/ | Project work |
|         | 111010/ | Project work |
|---------+---------+--------------|

Exam III covers 101110 - 110310.
(note: exam coverage is cumulative)

|         | Video   | IPSUR        |
|---------+---------+--------------|
| Week 13 |         |              |
|         | 111510/ | 10.3 - 10.4  |
|         | 111710/ | 10.4, 11.1   |
| Week 14 |         |              |
|         | 112210* | Project work |
|         | 112410/ | 11.2         |
|---------+---------+--------------|

Turn in Term Project

|         | Video   | IPSUR        |
|---------+---------+--------------|
| Week 15 |         |              |
|         | 112910/ | 11.2-5, 10.5 |
|         | 120110/ | 10.6         |
| Week 16 |         |              |
|         | Study!  | Everything   |
|---------+---------+--------------|

Final Exam covers everything.

* =RcmdrTestDrive= Story                                           :appendix:
:PROPERTIES:
:tangle: R/17-appendix.R
:CUSTOM_ID: cha-RcmdrTestDrive-Story
:END:

The goal of =RcmdrTestDrive= was to have a data set sufficiently rich
in the types of data represented such that a person could load it into
the \(\mathsf{R}\) Commander and be able to explore all of =Rcmdr='s
menu options at once. I decided early-on that an efficient way to do
this would be to generate the data set randomly, and later add to the
list of variables as more =Rcmdr= menu options became
available. Generating the data was easy, but generating a story that
related all of the respective variables proved to be less so.

In the Summer of 2006 I gave a version of the raw data and variable
names to my STAT 3743 Probability and Statistics class and invited
each of them to write a short story linking all of the variables
together in a coherent narrative. No further direction was given.

The most colorful of those I received was written by Jeffery
Cornfield, submitted July 12, 2006, and is included below with his
permission. It was edited slightly by the present author and updated
to respond dynamically to the random generation of =RcmdrTestDrive=;
otherwise, the story has been unchanged.

** Case File: ALU-179 "Murder Madness in Toon Town"
#+BEGIN_quote
*WARNING* This file is not for the faint of heart, dear reader,
because it is filled with horrible images that will haunt your
nightmares. If you are weak of stomach, have irritable bowel syndrome,
or are simply paranoid, DO NOT READ FURTHER! Otherwise, read at your
own risk.
#+END_quote

One fine sunny day, Police Chief R. Runner called up the forensics
department at Acme-Looney University. There had been
SRC_R[:eval no-export]{dim(RcmdrTestDrive)[ 1 ] - 2} 166 murders in the past
SRC_R[:eval no-export]{round((dim(RcmdrTestDrive)[ 1 ] - 2)/24)} 7 days, approximately
one murder every hour, of many of the local Human workers, shop
keepers, and residents of Toon Town. These alarming rates threatened
to destroy the fragile balance of Toon and Human camaraderie that had
developed in Toon Town.

Professor Twee T. Bird, a world-renowned forensics specialist and a
Czechoslovakian native, received the call. "Professor, we need your
expertise in this field to identify the pattern of the killer or
killers," Chief Runner exclaimed. "We need to establish a link between
these people to stop this massacre."

"Yes, Chief Runner, please give me the details of the case," Professor
Bird declared with a heavy native accent, (though, for the sake of the
case file, reader, I have decided to leave out the accent due to the
fact that it would obviously drive you -- if you will forgive the pun
-- looney!)

"All prints are wiped clean and there are no identifiable marks on the
bodies of the victims. All we are able to come up with is the
possibility that perhaps there is some kind of alternative method of
which we are unaware. We have sent a secure e-mail with a listing of
all of the victims =races=, =genders=, locations of the bodies, and
the sequential =order= in which they were killed. We have also
included other information that might be helpful," said Chief Runner.

"Thank you very much. Perhaps I will contact my colleague in the
Statistics Department here, Dr. Elmer Fudd-Einstein," exclaimed
Professor Bird. "He might be able to identify a pattern of attack with
mathematics and statistics."

"Good luck trying to find him, Professor. Last I heard, he had a
bottle of scotch and was in the Hundred Acre Woods hunting rabbits,"
Chief Runner declared in a manner that questioned the beloved doctors
credibility.

"Perhaps I will take a drive to find him. The fresh air will do me
good."

#+BEGIN_quote
I will skip ahead, dear reader, for much occurred during this
time. Needless to say, after a fierce battle with a mountain cat that
the Toon-ology Department tagged earlier in the year as "Sylvester,"
Professor Bird found Dr. Fudd-Einstein and brought him back, with much
bribery of alcohol and the promise of the future slaying of those
"wascally wabbits" (it would help to explain that Dr. Fudd-Einstein
had a speech impediment which was only worsened during the consumption
of alcohol.)
#+END_quote

Once our two heroes returned to the beautiful Acme-Looney University,
and once Dr. Fudd-Einstein became sober and coherent, they set off to
examine the case and begin solving these mysterious murders.

"First off," Dr. Fudd-Einstein explained, "these people all worked at
the University at some point or another. Also, there also seems to be
a trend in the fact that they all had a =salary= between
SRC_R[:eval no-export]{round(min(RcmdrTestDrive$salary))} 377 and
SRC_R[:eval no-export]{round(max(RcmdrTestDrive$salary))} 1156 when they retired."

"Thats not really a lot to live off of," explained Professor Bird.

"Yes, but you forget that the Looney Currency System works differently
than the rest of the American Currency System. One Looney is
equivalent to Ten American Dollars. Also, these faculty members are
the ones who faced a cut in their salary, as denoted by
=reduction=. Some of them dropped quite substantially when the
University had to fix that little /faux pas/ in the Chemistry
Department. You remember: when Dr. D. Duck tried to create that
Everlasting Elixir? As a result, these faculty left the
university. Speaking of which, when is his memorial service?" inquired
Dr. Fudd-Einstein.

"This coming Monday. But if there were all of these killings, how in
the world could one person do it? It just doesnt seem to be possible;
stay up SRC_R[:eval no-export]{round((dim(RcmdrTestDrive)[ 1 ] - 2)/24)} 7 days
straight and be able to kill all of these people and have the energy
to continue on," Professor Bird exclaimed, doubting the guilt of only
one person.

"Perhaps then, it was a group of people, perhaps there was more than
one killer placed throughout Toon Town to commit these crimes. If I
feed in these variables, along with any others that might have a
pattern, the Acme Computer will give us an accurate reading of
suspects, with a scant probability of error. As you know, the Acme
Computer was developed entirely in house here at Acme-Looney
University," Dr. Fudd-Einstein said as he began feeding the numbers
into the massive server.

"Hey, look at this," Professor Bird exclaimed, "Whats with this
=before= and =after= information?"

"Scroll down; it shows it as a note from the coroners
office. Apparently Toon Town Coroner Marvin -- that strange fellow
from Mars, Pennsylvania -- feels, in his opinion, that given the fact
that the cadavers were either =smokers= or non-smokers, and given
their personal health, and family medical history, that this was their
life expectancy before contact with cigarettes or second-hand smoke
and after," Dr. Fudd-Einstein declared matter-of-factly.

"Well, would race or gender have something to do with it, Elmer?"
inquired Professor Bird.

"Maybe, but I would bet my money on somebody was trying to quiet these
faculty before they made a big ruckus about the secret
money-laundering of Old Man Acme. You know, most people think that is
how the University receives most of its funds, through the mob
families out of Chicago. And I would be willing to bet that these
faculty figured out the connection and were ready to tell the Looney
Police." Dr. Fudd-Einstein spoke lower, fearing that somebody would
overhear their conversation.

Dr. Fudd-Einstein then pressed =Enter= on the keyboard and waited for
the results. The massive computer roared to life... and when I say
roared, I mean it literally /roared/. All the hidden bells, whistles,
and alarm clocks in its secret compartments came out and created such
a loud racket that classes across the university had to come to a
stand-still until it finished computing.

Once it was completed, the computer listed 4 names:

#+BEGIN_example
****************************SUSPECTS****************************
- Yosemite Sam ("Looney" Insane Asylum) 
- Wile E. Coyote (deceased) 
- Foghorn Leghorn (whereabouts unknown) 
- Granny (1313 Mockingbird Lane, Toon Town USA)
#+END_example

Dr. Fudd-Einstein and Professor Bird looked on in silence. They could
not believe their eyes. The greatest computer on the Gulf of Mexico
seaboard just released the most obscure results imaginable.

"There seems to be a mistake. Perhaps something is off," Professor
Bird asked, still unable to believe the results.

"Not possible; the Acme Computer takes into account every kind of
connection available. It considers affiliations to groups, and
affiliations those groups have to other groups. It checks the FBI,
CIA, British intelligence, NAACP, AARP, NSA, JAG, TWA, EPA, FDA, USWA,
\(\mathsf{R}\), MAPLE, SPSS, SAS, and Ben & Jerrys files to identify
possible links, creating the most powerful computer in the
world... with a tweak of Toon fanaticism," Dr. Fudd-Einstein
proclaimed, being a proud co-founder of the Acme Computer Technology.

"Wait a minute, Ben & Jerry? What would eating ice cream have to do
with anything?" Professor Bird inquired.

"It is in the works now, but a few of my fellow statistician
colleagues are trying to find a mathematical model to link the type of
ice cream consumed to the type of person they might become. Assassins
always ate vanilla with chocolate sprinkles, a little known fact they
would tell you about Oswald and Booth," Dr. Fudd-Einstein declared.

"Ive heard about this. My forensics graduate students are trying to
identify car thieves with either rocky road or mint chocolate chip so
far, the pattern is showing a clear trend with chocolate chip,"
Professor Bird declared.  "Well, what do we know about these suspects,
Twee?" Dr. Fudd-Einstein asked.

"Yosemite Sam was locked up after trying to rob that bank in the West
Borough. Apparently his guns were switched and he was sent the Acme
Kids Joke Gun and they blew up in his face. The containers of peroxide
they contained turned all of his facial hair red. Some little child is
running around Toon Town with a pair of .38s to this day.

"Wile E. Coyote was that psychopath working for the Yahtzee - the
fanatics who believed that Toons were superior to Humans. He strapped
sticks of Acme Dynamite to his chest to be a martyr for the cause, but
before he got to the middle of Toon Town, this defective TNT blew him
up. Not a single other person -- Toon or Human -- was even close.

"Foghorn Leghorn is the most infamous Dog Kidnapper of all times. He
goes to the homes of prominent Dog citizens and holds one of their
relatives for ransom. If they refuse to pay, he sends them to the
pound. Either way, theyre sure stuck in the dog house," Professor
Bird laughed. Dr. Fudd-Einstein didnt seem amused, so Professor Bird
continued.

"Granny is the most beloved alumnus of Acme-Looney University. She was
in the first graduating class and gives graciously each year to the
university. Without her continued financial support, we wouldnt have
the jobs we do. She worked as a parking attendant at the University
lots... wait a minute, take a look at this," Professor Bird said as he
scrolled down in the police information. "Grannys signature is on
each of these faculty members =parking= tickets. Kind of odd,
considering the Chief-of-Parking signed each personally. The deceased
had from as few as SRC_R[:eval no-export]{min(RcmdrTestDrive$parking)} 1 ticket to as
many as SRC_R[:eval no-export]{max(RcmdrTestDrive$parking)} 8. All tickets were unpaid.

"And look at this, Granny married Old Man Acme after graduation. He
was a resident of Chicago and rumored to be a consigliere to one of
the most prominent crime families in Chicago, the Chuck Jones/Warner
Crime Family," Professor Bird read from the screen as a cold feeling
of terror rose from the pit of his stomach.

"Say, dont you live at her house? Wow, youre living under the same
roof as one of the greatest criminals/murderers of all time!"
Dr. Fudd-Einstein said in awe and sarcasm.

"I would never have suspected her, but I guess it makes sense. She is
older, so she doesnt need near the amount of sleep as a younger
person. She has access to all of the vehicles so she can copy license
plate numbers and follow them to their houses. She has the finances to
pay for this kind of massive campaign on behalf of the Mob, and she
hates anyone that even remotely smells like smoke," Professor Bird
explained, wishing to have his hit of nicotine at this time.

"Well, I guess there is nothing left to do but to call Police Chief
Runner and have him arrest her," Dr. Fudd-Einstein explained as he
began dialing. "What I cant understand is how in the world the Police
Chief sent me all of this information and somehow seemed to screw it
up."

"What do you mean?" inquired Professor Bird.

"Well, look here. The data file from the Chief's email shows
SRC_R[:eval no-export]{dim(RcmdrTestDrive)[ 1 ]} 168 murders, but there have only been
SRC_R[:eval no-export]{dim(RcmdrTestDrive)[ 1 ] - 2} 166. This doesnt make any
sense. Ill have to straighten it out. Hey, wait a minute. Look at
this, Person number SRC_R[:eval no-export]{dim(RcmdrTestDrive)[ 1 ] - 1} 167 and Person
number SRC_R[:eval no-export]{dim(RcmdrTestDrive)[ 1 ]} 168 seem to match our
stats. But how can that be?"

It was at this moment that our two heroes were shot from behind and
fell over the computer, dead. The killer hit =Delete= on the computer
and walked out slowly (considering they had arthritis) and cackling
loudly in the now quiet computer lab.

And so, I guess my question to you the reader is, did Granny murder
SRC_R[:eval no-export]{dim(RcmdrTestDrive)[ 1 ]} 168 people, or did the murderer slip
through the cracks of justice? You be the statistician and come to
your own conclusion.

Detective Pyork E. Pig 

: ***End File***

#+LaTeX: \vfill{}

#+BEGIN_latex
\cleardoublepage
\phantomsection
\addcontentsline{toc}{chapter}{\bibname}
\bibliographystyle{plainurl}
\nocite{*}
\bibliography{include/IPSUR,include/Rpackages}
\vfill{}
\cleardoublepage
\phantomsection
\addcontentsline{toc}{chapter}{\indexname} 
\printindex{}
#+END_latex

#+BEGIN_SRC R :exports none :results silent
save.image("R/IPSUR.RData")
#+END_SRC
