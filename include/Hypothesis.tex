\chapter{Hypothesis Testing}
\label{sec-10}

\textbf{What do I want them to know?}
\begin{itemize}
\item basic terminology and philosophy of the Neyman-Pearson paradigm
\item classical hypothesis tests for the standard one and two sample
problems with means, variances, and proportions
\item the notion of between versus within group variation and how it plays
out with one-way ANOVA
\item the concept of statistical power and its relation to sample size
\end{itemize}

\section{Introduction}
\label{sec-10-1}

I spent a week during the summer of 2005 at the University of Nebraska
at Lincoln grading Advanced Placement Statistics exams, and while I
was there I attended a presentation by Dr. Roxy Peck. At the end of
her talk she described an activity she had used with students to
introduce the basic concepts of hypothesis testing. I was impressed by
the activity and have used it in my own classes several times since.

\begin{quote}
The instructor (with a box of cookies in hand) enters a class of
fifteen or more students and produces a brand-new, sealed deck of
ordinary playing cards. The instructor asks for a student volunteer to
break the seal, and then the instructor prominently shuffles the
deckseveral times in front of the class, after which
time the students are asked to line up in a row. They are going to
play a game. Each student will draw a card from the top of the deck,
in turn. If the card is black, then the lucky student will get a
cookie. If the card is red, then the unlucky student will sit down
empty-handed. Let the game begin.
\end{quote}

Keep the preceding experiment in the back of your mind as you read the
following sections. When you have finished the entire chapter, come
back and read this introduction again. All of the mathematical jargon
that follows is connected to the above paragraphs. In the meantime, I
will get you started:

\begin{description}
\item[{Null hypothesis:}] it is an ordinary deck of playing cards,
shuffled thoroughly.
\item[{Alternative hypothesis:}] either it is a trick deck of cards, or
the instructor did some fancy shufflework.
\item[{Observed data:}] a sequence of draws from the deck, five reds in a row.
\end{description}

If it were truly an ordinary, well-shuffled deck of cards, the
probability of observing zero blacks out of a sample of size five
(without replacement) from a deck with 26 black cards and 26 red cards
would be

\begin{verbatim}
dhyper(0, m = 26, n = 26, k = 5)
\end{verbatim}

\begin{verbatim}
: [1] 0.02531012
\end{verbatim}

There are two very important final thoughts. First, everybody gets a
cookie in the end. Second, the students invariably (and aggressively)
attempt to get me to open up the deck and reveal the true nature of
the cards. I never do.

\section{Tests for Proportions}
\label{sec-10-2}

\label{exa-widget-machine}
We have a machine that makes widgets. 
\begin{itemize}
\item Under normal operation, about 0.10 of the widgets produced are
defective.
\item Go out and purchase a torque converter.
\item Install the torque converter, and observe \(n=100\) widgets from the
machine.
\item Let \(Y=\mbox{number of defective widgets observed}\).
\end{itemize}

If
\begin{itemize}
\item \(Y=0\), then the torque converter is great!
\item \(Y=4\), then the torque converter seems to be helping.
\item \(Y=9\), then there is not much evidence that the torque converter helps.
\item \(Y=17\), then throw away the torque converter.
\end{itemize}

Let \(p\) denote the proportion of defectives produced by the
machine. Before the installation of the torque converter \(p\) was
\(0.10\). Then we installed the torque converter. Did \(p\) change?
Did it go up or down? We use statistics to decide. Our method is to
observe data and construct a 95\% confidence interval for \(p\),
\begin{equation}
\hat{p} \pm z_{\alpha/2}\sqrt{\frac{\hat{p}(1 - \hat{p})}{n}}.
\end{equation}
If the confidence interval is 
\begin{itemize}
\item \([0.01,\,0.05]\), then we are 95\% confident that \(0.01\leq
  p \leq 0.05\), so there is evidence that the torque converter is
helping.
\item \([0.15,\,0.19]\), then we are 95\% confident that \(0.15\leq
  p \leq 0.19\), so there is evidence that the torque converter is
hurting.
\item \([0.07,\,0.11]\), then there is not enough evidence to conclude
that the torque converter is doing anything at all, positive or
negative.
\end{itemize}

\subsection{Terminology}
\label{sec-10-2-1}

The \emph{null hypothesis} \(H_{0}\) is a "nothing" hypothesis, whose
interpretation could be that nothing has changed, there is no
difference, there is nothing special taking place, \emph{etc}. In Example
\ref{exa-widget-machine} the null hypothesis would be \(H_{0}:\, p = 0.10.\)
The \emph{alternative hypothesis} \(H_{1}\) is the hypothesis that
something has changed, in this case, \(H_{1}:\, p \neq 0.10\). Our
goal is to statistically \emph{test} the hypothesis \(H_{0}:\, p = 0.10\)
versus the alternative \(H_{1}:\, p \neq 0.10\). Our procedure will
be:
\begin{enumerate}
\item Go out and collect some data, in particular, a simple random sample
of observations from the machine.
\item Suppose that \(H_{0}\) is true and construct a \(100(1-\alpha)\%\)
   confidence interval for \(p\).
\item If the confidence interval does not cover \(p = 0.10\), then we
\emph{reject} \(H_{0}\). Otherwise, we \emph{fail to reject} \(H_{0}\).
\end{enumerate}

\begin{rem}
Every time we make a decision it is possible to be wrong, and there
are two possible mistakes we can make. We have committed a
\begin{description}
\item[{Type I Error}] if we reject \(H_{0}\) when in fact \(H_{0}\) is
true. This would be akin to convicting an innocent
person for a crime (s)he did not commit.
\item[{Type II Error}] if we fail to reject \(H_{0}\) when in fact
\(H_{1}\) is true. This is analogous to a guilty
person escaping conviction.
\end{description}
\end{rem}

Type I Errors are usually considered worse, and we
design our statistical procedures to control the probability of making
such a mistake. We define the
\begin{equation}
\mbox{significance level of the test} = \mathbb{P}(\mbox{Type I Error}) = \alpha.
\end{equation}
We want \(\alpha\) to be small which conventionally means, say,
\(\alpha=0.05\), \(\alpha=0.01\), or \(\alpha=0.005\) (but could mean
anything, in principle).
\begin{itemize}
\item The \emph{rejection region} (also known as the \emph{critical region}) for the
test is the set of sample values which would result in the rejection
of \(H_{0}\). For Example \ref{exa-widget-machine}, the rejection region
would be all possible samples that result in a 95\% confidence
interval that does not cover \(p = 0.10\).
\item The above example with \(H_{1}:\,p \neq 0.10\) is called a
\emph{two-sided} test. Many times we are interested in a \emph{one-sided}
test, which would look like \(H_{1}:\,p < 0.10\) or \(H_{1}:\,p >
  0.10\).
\end{itemize}

\label{exa-prop-test-pvalue-A} Suppose \(p = \mathrm{the\ proportion\ of\
students}\) who are admitted to the graduate school of the University
of California at Berkeley, and suppose that a public relations officer
boasts that UCB has historically had a 40\% acceptance rate for its
graduate school. Consider the data stored in the table \texttt{UCBAdmissions}
from 1973. Assuming these observations constituted a simple random
sample, are they consistent with the officer's claim, or do they
provide evidence that the acceptance rate was significantly less than
40\%? Use an \(\alpha = 0.01\) significance level.

Our null hypothesis in this problem is \(H_{0}:\,p = 0.4\) and the
alternative hypothesis is \(H_{1}:\,p < 0.4\). We reject the null
hypothesis if \(\hat{p}\) is too small, that is, if
\begin{equation}
\frac{\hat{p} - 0.4}{\sqrt{0.4(1 - 0.4)/n}} < -z_{\alpha},
\end{equation}
where \(\alpha = 0.01\) and \(-z_{0.01}\) is 
\begin{verbatim}
-qnorm(0.99)
\end{verbatim}

\begin{verbatim}
: [1] -2.326348
\end{verbatim}

Our only remaining task is to find the value of the test statistic and
see where it falls relative to the critical value. We can find the
number of people admitted and not admitted to the UCB graduate school
with the following.

\begin{verbatim}
A <- as.data.frame(UCBAdmissions)
head(A)
xtabs(Freq ~ Admit, data = A)
\end{verbatim}

\begin{verbatim}
     Admit Gender Dept Freq
1 Admitted   Male    A  512
2 Rejected   Male    A  313
3 Admitted Female    A   89
4 Rejected Female    A   19
5 Admitted   Male    B  353
6 Rejected   Male    B  207
Admit
Admitted Rejected 
    1755     2771
\end{verbatim}

Now we calculate the value of the test statistic.

\begin{verbatim}
phat <- 1755/(1755 + 2771)
(phat - 0.4)/sqrt(0.4 * 0.6/(1755 + 2771)) 
\end{verbatim}

\begin{verbatim}
: [1] -1.680919
\end{verbatim}

Our test statistic is not less than \(-2.32\), so it does not fall
into the critical region. Therefore, we \emph{fail} to reject the null
hypothesis that the true proportion of students admitted to graduate
school is less than 40\% and say that the observed data are consistent
with the officer's claim at the \(\alpha = 0.01\) significance level.



\label{exa-prop-test-pvalue-B} We are going to do Example
\ref{exa-prop-test-pvalue-A} all over again. Everything will be exactly the
same except for one change. Suppose we choose significance level
\(\alpha = 0.05\) instead of \(\alpha = 0.01\). Are the 1973 data
consistent with the officer's claim?

Our null and alternative hypotheses are the same. Our observed test
statistic is the same: it was approximately \(-1.68\). But notice that
our critical value has changed: \(\alpha = 0.05\) and \(-z_{0.05}\) is

\begin{verbatim}
-qnorm(0.95)
\end{verbatim}

\begin{verbatim}
: [1] -1.644854
\end{verbatim}

Our test statistic is less than \(-1.64\) so it now falls into the
critical region! We now \emph{reject} the null hypothesis and conclude that
the 1973 data provide evidence that the true proportion of students
admitted to the graduate school of UCB in 1973 was significantly less
than 40\%. The data are \emph{not} consistent with the officer's claim at
the \(\alpha = 0.05\) significance level.

What is going on, here? If we choose \(\alpha = 0.05\) then we reject
the null hypothesis, but if we choose \(\alpha = 0.01\) then we fail
to reject the null hypothesis. Our final conclusion seems to depend on
our selection of the significance level. This is bad; for a particular
test, we never know whether our conclusion would have been different
if we had chosen a different significance level.

Or do we?

Clearly, for some significance levels we reject, and for some
significance levels we do not. Where is the boundary? That is, what is
the significance level for which we would \emph{reject} at any significance
level \emph{bigger}, and we would \emph{fail to reject} at any significance
level \emph{smaller}? This boundary value has a special name: it is called
the \emph{p-value} of the test.

\begin{defn}
The \emph{p-value}, or \emph{observed significance level}, of a hypothesis test
is the probability when the null hypothesis is true of obtaining the
observed value of the test statistic (such as \(\hat{p}\)) or values
more extreme -- meaning, in the direction of the alternative
hypothesis.
\end{defn}

\begin{verbatim}
pnorm(-1.680919)
\end{verbatim}

\begin{verbatim}
: [1] 0.04638932
\end{verbatim}

We see that the \(p\)-value is strictly between the significance
levels \(\alpha = 0.01\) and \(\alpha = 0.05\). This makes sense: it
has to be bigger than \(\alpha = 0.01\) (otherwise we would have
rejected \(H_{0}\) in Example \ref{exa-prop-test-pvalue-A}) and it must also
be smaller than \(\alpha = 0.05\) (otherwise we would not have
rejected \(H_{0}\) in Example \ref{exa-prop-test-pvalue-B}). Indeed,
\(p\)-values are a characteristic indicator of whether or not we would
have rejected at assorted significance levels, and for this reason a
statistician will often skip the calculation of critical regions and
critical values entirely. If (s)he knows the \(p\)-value, then (s)he
knows immediately whether or not (s)he would have rejected at \emph{any}
given significance level.

Thus, another way to phrase our significance test procedure is: we
will reject \(H_{0}\) at the \(\alpha\)-level of significance if the
\(p\)-value is less than \(\alpha\).

\begin{rem}
If we have two populations with proportions \(p_{1}\) and \(p_{2}\)
then we can test the null hypothesis \(H_{0}:\,p_{1} = p_{2}\). In
that which follows,
\begin{itemize}
\item we observe independent simple random samples of size \(n_{1}\) and
\(n_{2}\) from Bernoulli populations with respective probabilities
of success \(p_{1}\) and \(p_{2}\),
\item we write \(y_{1}\) and \(y_{2}\) for the respective numbers of
successes from each of the two groups,
\item we estimate \(p_{1}\) and \(p_{2}\) with \(\hat{p}_{1} = y_{1}/n_{1}
  \) and \(\hat{p}_{2} = y_{2}/n_{2}\), while we estimate the pooled
probability \(\hat{p}\) with \((y_{1} + y_{2})/(n_{1} + n_{2}\), and
\item finally, \[z = \frac{\hat{p}_{1} - \hat{p}_{2}}{\sqrt{\hat{p}(1 - \hat{p})\left( \frac{1}{n_{1}} + \frac{1}{n_{2}} \right)}}. \]
\end{itemize}
\end{rem}


\begin{table}[htb]
\caption[Hypothesis tests, difference in population proportions, large sample]{\label{tab-ztest-two-sample-prop}Hypothesis tests, difference in population proportions, large sample.}
\centering
\begin{tabular}{lll}
\(H_{0}\) & \(H_{a}\) & Rejection Region\\
\hline
\(p_{1} = p_{2}\) & \(p_{1} - p_{2} > 0\) & \(z > z_{\alpha}\)\\
\(p_{1} = p_{2}\) & \(p_{1} - p_{2} < 0\) & \(z < -z_{\alpha}\)\\
\(p_{1} = p_{2}\) & \(p_{1} - p_{2} \neq 0\) & \( \vert z \vert > z_{\alpha/2}\)\\
\end{tabular}
\end{table}


Example.



\subsubsection{How to do it with \(\mathsf{R}\)}
\label{sec-10-2-1-1}

The following does the test.

\begin{verbatim}
prop.test(1755, 1755 + 2771, p = 0.4, alternative = "less", 
          conf.level = 0.99, correct = FALSE)
\end{verbatim}

\begin{verbatim}
	1-sample proportions test without continuity correction

data:  1755 out of 1755 + 2771, null probability 0.4
X-squared = 2.8255, df = 1, p-value = 0.04639
alternative hypothesis: true p is less than 0.4
99 percent confidence interval:
 0.0000000 0.4047326
sample estimates:
        p 
0.3877596
\end{verbatim}

Do the following to make the plot.


\textbf{Remark}: In the above we set \texttt{correct = FALSE} to tell the computer
that we did not want to use Yates' continuity correction (reference
BLANK).  It is customary to use Yates when the expected frequency of
successes is less than 10. (reference BLANK) You can use it all of the
time, but you will have a decrease in power. For large samples the
correction does not matter.

\subsubsection{With the \(\mathsf{R}\) Commander}
\label{sec-10-2-1-2}

If you already know the number of successes and failures, then you can
use the menu \texttt{Statistics} \(\triangleright\) \texttt{Proportions}
\(\triangleright\) \texttt{IPSUR Enter table for single sample...}

Otherwise, your data -- the raw successes and failures -- should be in
a column of the Active Data Set. Furthermore, the data must be stored
as a "factor" internally. If the data are not a factor but are numeric
then you can use the menu \texttt{Data} \(\triangleright\) \texttt{Manage variables
in active data set} \(\triangleright\) \texttt{Convert numeric variables to
factors...} to convert the variable to a factor. Or, you can always
use the \texttt{factor} function.

Once your unsummarized data is a column, then you can use the menu
\texttt{Statistics} \(\triangleright\) \texttt{Proportions} \(\triangleright\)
\texttt{Single-sample proportion test...}

\section{One Sample Tests for Means and Variances}
\label{sec-10-3}

\subsection{For Means}
\label{sec-10-3-1}

Here, \(X_{1}\), \(X_{2}\), \ldots{}, \(X_{n}\) are a \(SRS(n)\) from a
\(\mathsf{norm}(\mathtt{mean} = \mu,\,\mathtt{sd} = \sigma)\)
distribution. We would like to test \(H_{0}:\mu = \mu_{0}\).

\begin{description}
\item[{Case A:}] Suppose \(\sigma\) is known. Then under \(H_{0}\),
\[
   Z = \frac{\overline{X} - \mu_{0}}{\sigma/\sqrt{n}} \sim \mathsf{norm}(\mathtt{mean} = 0,\,\mathtt{sd} = 1).
   \]

\begin{table}[htb]
\caption[Hypothesis tests, population mean, large sample]{\label{tab-ztest-one-sample}Hypothesis tests, population mean, large sample.}
\centering
\begin{tabular}{lll}
\(H_{0}\) & \(H_{a}\) & Rejection Region\\
\hline
\(\mu = \mu_{0}\) & \(\mu > \mu_{0}\) & \(z > z_{\alpha}\)\\
\(\mu = \mu_{0}\) & \(\mu < \mu_{0}\) & \(z < -z_{\alpha}\)\\
\(\mu = \mu_{0}\) & \(\mu \neq \mu_{0}\) & \( \vert z \vert > z_{\alpha/2}\)\\
\end{tabular}
\end{table}

\item[{Case B:}] When \(\sigma\) is unknown, under \(H_{0}\),
\[
   T = \frac{\overline{X} - \mu_{0}}{S/\sqrt{n}} \sim \mathsf{t}(\mathtt{df} = n - 1).
   \]
\begin{table}[htb]
\caption[Hypothesis tests, population mean, small sample]{\label{tab-ttest-one-sample}Hypothesis tests, population mean, small sample.}
\centering
\begin{tabular}{lll}
\(H_{0}\) & \(H_{a}\) & Rejection Region\\
\hline
\(\mu = \mu_{0}\) & \(\mu > \mu_{0}\) & \(t > t_{\alpha}(n - 1)\)\\
\(\mu = \mu_{0}\) & \(\mu < \mu_{0}\) & \(t < -t_{\alpha}(n - 1\)\\
\(\mu = \mu_{0}\) & \(\mu \neq \mu_{0}\) & \( \vert t \vert > t_{\alpha/2}(n - 1\)\\
\end{tabular}
\end{table}
\end{description}

\begin{rem}
If \(\sigma\) is unknown but \(n\) is large then we can use the
\(z\)-test.
\end{rem}


In this example we
\begin{enumerate}
\item Find the null and alternative hypotheses.
\item Choose a test and find the critical region.
\item Calculate the value of the test statistic and state the conclusion.
\item Find the \(p\)-value.
\end{enumerate}



\begin{rem}
\begin{itemize}
\item Another name for a \(p\)-value is \emph{tail-end probability}. We reject
\(H_{0}\) when the \(p\)-value is small.
\item The quantity \(\sigma/\sqrt{n}\), when \(\sigma\) is known, is
called the \emph{standard error of the sample mean}. In general, if we
have an estimator \(\hat{\theta}\) then \(\sigma_{\hat{\theta}}\) is
called the \emph{standard error} of \(\hat{\theta}\). We usually need to
estimate \(\sigma_{\hat{\theta}}\) with
\(\hat{\sigma_{\hat{\theta}}}\).
\end{itemize}
\end{rem}

\subsubsection{How to do it with \(\mathsf{R}\)}
\label{sec-10-3-1-1}

I am thinking \texttt{z.test} \index{z.test@\texttt{z.test}} in
\texttt{TeachingDemos}, \texttt{t.test} \index{t.test@\texttt{t.test}} in base
\(\mathsf{R}\).

\begin{verbatim}
x <- rnorm(37, mean = 2, sd = 3)
z.test(x, mu = 1, sd = 3, conf.level = 0.90)
\end{verbatim}

\begin{verbatim}

	One Sample z-test

data:  x
z = 1.2964, n = 37.000, Std. Dev. = 3.000, Std. Dev. of the sample mean = 0.493,
p-value = 0.1948
alternative hypothesis: true mean is not equal to 1
90 percent confidence interval:
 0.8281414 2.4506151
sample estimates:
mean of x 
 1.639378
\end{verbatim}

The \texttt{RcmdrPlugin.IPSUR} package \cite{RcmdrPlugin.IPSUR} does not have
a menu for \texttt{z.test} yet.

\begin{verbatim}
x <- rnorm(13, mean = 2, sd = 3)
t.test(x, mu = 0, conf.level = 0.90, alternative = "greater")
\end{verbatim}

\begin{verbatim}
	One Sample t-test

data:  x
t = 1.2904, df = 12, p-value = 0.1106
alternative hypothesis: true mean is greater than 0
90 percent confidence interval:
 -0.05472113         Inf
sample estimates:
mean of x 
 1.072399
\end{verbatim}

\subsubsection{With the \(\mathsf{R}\) Commander}
\label{sec-10-3-1-2}

Your data should be in a single numeric column (a variable) of the
Active Data Set. Use the menu \texttt{Statistics} \(\triangleright\) \texttt{Means}
\(\triangleright\) \texttt{Single-sample t-test...}

\subsection{Tests for a Variance}
\label{sec-10-3-2}

Here, \(X_{1}\), \(X_{2}\), \ldots{}, \(X_{n}\) are a \(SRS(n)\) from a
\(\mathsf{norm}(\mathtt{mean} = \mu,\,\mathtt{sd} = \sigma)\)
distribution. We would like to test \(H_{0}:\,\sigma^{2} =
\sigma_{0}\). We know that under \(H_{0}\), \[ X^{2} = \frac{(n -
1)S^{2}}{\sigma^{2}} \sim \mathsf{chisq}(\mathtt{df} = n - 1).  \]
Table here.


Give some data and a hypothesis.
\begin{itemize}
\item Give an \(\alpha\)-level and test the critical region way.
\item Find the \(p\)-value for the test.
\end{itemize}

\subsubsection{How to do it with \(\mathsf{R}\)}
\label{sec-10-3-2-1}
I am thinking about \texttt{sigma.test}
\index{sigma.test@\texttt{sigma.test}} in the
\texttt{TeachingDemos} package \cite{TeachingDemos}.

\begin{verbatim}
sigma.test(women$height, sigma = 8)
\end{verbatim}

\begin{verbatim}

	One sample Chi-squared test for variance

data:  women$height
X-squared = 4.375, df = 14, p-value = 0.01449
alternative hypothesis: true variance is not equal to 64
95 percent confidence interval:
 10.72019 49.74483
sample estimates:
var of women$height 
                 20
\end{verbatim}

\section{Two-Sample Tests for Means and Variances}
\label{sec-10-4}

The basic idea for this section is the following. We have
\(X\sim\mathsf{norm}(\mathtt{mean} = \mu_{X},\,\mathtt{sd} =
\sigma_{X})\) and \(Y\sim\mathsf{norm}(\mathtt{mean} =
\mu_{Y},\,\mathtt{sd} = \sigma_{Y})\) distributed independently. We
would like to know whether \(X\) and \(Y\) come from the same
population distribution, in other words, we would like to know:
\begin{equation}
\mbox{Does }X\overset{\mathrm{d}}{=}Y?
\end{equation}
where the symbol \(\overset{\mathrm{d}}{=}\) means equality of
probability distributions.  Since both \(X\) and \(Y\) are normal, we
may rephrase the question:
\begin{equation}
\mbox{Does }\mu_{X} = \mu_{Y}\mbox{ and }\sigma_{X} = \sigma_{Y}?
\end{equation}
Suppose first that we do not know the values of \(\sigma_{X}\) and
\(\sigma_{Y}\), but we know that they are equal,
\(\sigma_{X}=\sigma_{Y}\). Our test would then simplify to
\(H_{0}:\,\mu_{X} = \mu_{Y}\). We collect data \(X_{1}\), \(X_{2}\),
\ldots{}, \(X_{n}\) and \(Y_{1}\), \(Y_{2}\), \ldots{}, \(Y_{m}\), both simple
random samples of size \(n\) and \(m\) from their respective normal
distributions. Then under \(H_{0}\) (that is, assuming \(H_{0}\) is
true) we have \(\mu_{X} = \mu_{Y}\), or rewriting, \(\mu_{X} - \mu_{Y}
= 0\), so
\begin{equation}
T = \frac{\overline{X} - \overline{Y}}{S_{p}\sqrt{\frac{1}{n} + \frac{1}{m}}} = \frac{\overline{X} - \overline{Y} - (\mu_{X} - \mu_{Y})}{S_{p}\sqrt{\frac{1}{n} + \frac{1}{m}}}\sim\mathsf{t}(\mathtt{df} = n + m - 2).
\end{equation}

\subsection{Independent Samples}
\label{sec-10-4-1}

\begin{rem}
If the values of \(\sigma_{X}\) and \(\sigma_{Y}\) are known, then we
can plug them in to our statistic:
\begin{equation} 
Z = \frac{\overline{X} - \overline{Y}}{\sqrt{\sigma_{X}^{2}/n + \sigma_{Y}^{2}/m}};
\end{equation}
the result will have a \(\mathsf{norm}(\mathtt{mean} = 0,\,\mathtt{sd}
= 1)\) distribution when \(H_{0}:\,\mu_{X} = \mu_{Y}\) is true.
\end{rem}

\begin{rem}
Even if the values of \(\sigma_{X}\) and \(\sigma_{Y}\) are not known,
if both \(n\) and \(m\) are large then we can plug in the sample
estimates and the result will have approximately a
\(\mathsf{norm}(\mathtt{mean} = 0,\,\mathtt{sd} = 1)\) distribution when
\(H_{0}:\,\mu_{X} = \mu_{Y}\) is true.
\begin{equation} 
Z = \frac{\overline{X} - \overline{Y}}{\sqrt{S_{X}^{2}/n + S_{Y}^{2}/m}}.
\end{equation}
\end{rem}

\begin{rem}
It is often helpful to construct side-by-side boxplots and other
visual displays in concert with the hypothesis test. This gives a
visual comparison of the samples and helps to identify departures from
the test's assumptions -- such as outliers.
\end{rem}

\begin{rem}
WATCH YOUR ASSUMPTIONS.
\begin{itemize}
\item The normality assumption can be relaxed as long as the population
distributions are not highly skewed.
\item The equal variance assumption can be relaxed as long as both sample
sizes \(n\) and \(m\) are large. However, if one (or both) samples
is small, then the test does not perform well; we should instead use
the methods of Chapter \ref{sec-13}.
\end{itemize}
\end{rem}

For a nonparametric alternative to the two-sample \(F\) test see
Chapter \ref{sec-14}.

\begin{table}[htb]
\caption[Rejection regions, difference in means, pooled t-test]{\label{tab-two-t-test-pooled}Rejection regions, difference in means, pooled t-test.}
\centering
\begin{tabular}{lll}
\(H_{0}\) & \(H_{a}\) & Rejection Region\\
\hline
\(\mu_{D} = D_{0}\) & \(\mu_{D} > D_{0}\) & \(t > t_{\alpha}(n + m - 2)\)\\
\(\mu_{D} = D_{0}\) & \(\mu_{D} < D_{0}\) & \(t < -t_{\alpha}(n + m -  2)\)\\
\(\mu_{D} = D_{0}\) & \(\mu_{D} \neq D_{0}\) & \( \vert t \vert > t_{\alpha/2}(n + m - 2)\)\\
\end{tabular}
\end{table}


\subsection{Paired Samples}
\label{sec-10-4-2}

\begin{table}[htb]
\caption[Rejection regions, difference in means, pairled t-test]{\label{tab-two-t-test-paired}Rejection regions, difference in means, pairled t-test.}
\centering
\begin{tabular}{lll}
\(H_{0}\) & \(H_{a}\) & Rejection Region\\
\hline
\(\mu_{D} = D_{0}\) & \(\mu_{D} > D_{0}\) & \(t > t_{\alpha}(n_{D} - 1)\)\\
\(\mu_{D} = D_{0}\) & \(\mu_{D} < D_{0}\) & \(t < -t_{\alpha}(n_{D} - 2)\)\\
\(\mu_{D} = D_{0}\) & \(\mu_{D} \neq D_{0}\) & \( \vert t \vert > t_{\alpha/2}(n_{D} - 1)\)\\
\end{tabular}
\end{table}


\subsubsection{How to do it with \(\mathsf{R}\)}
\label{sec-10-4-2-1}

\begin{verbatim}
t.test(extra ~ group, data = sleep, paired = TRUE)
\end{verbatim}

\begin{verbatim}
	Paired t-test

data:  extra by group
t = -4.0621, df = 9, p-value = 0.002833
alternative hypothesis: true difference in means is not equal to 0
95 percent confidence interval:
 -2.4598858 -0.7001142
sample estimates:
mean of the differences 
                  -1.58
\end{verbatim}

\section{Other Hypothesis Tests}
\label{sec-10-5}

\subsection{Kolmogorov-Smirnov Goodness-of-Fit Test}
\label{sec-10-5-1}

\subsubsection{How to do it with \(\mathsf{R}\)}
\label{sec-10-5-1-1}

\begin{verbatim}
with(randu, ks.test(x, "punif"))
\end{verbatim}

\begin{verbatim}
: 
: 	One-sample Kolmogorov-Smirnov test
: 
: data:  x
: D = 0.0555, p-value = 0.1697
: alternative hypothesis: two-sided
\end{verbatim}

\subsection{Shapiro-Wilk Normality Test}
\label{sec-10-5-2}

Most of the small-sample procedures we have studied thus far require
that the target population be normally distributed.  In later chapters
we will be assuming that errors from a proposed model are normally
distributed, and one method we will use to assess the model's adequacy
will be to investigate the plausibility of that assumption. So, you
see, determining whether or not a random sample comes from a normal
distribution is an important problem for us.  We have already learned
graphical methods to address the question, but here we will study a
formal hypothesis test of the same.

The test statistic we will use is Wilk's \(W\), which looks like this:
\begin{equation}
W = \frac{\left(\sum_{i = 1}^{n} a_{i}x_{(i)} \right)^{2}}{\sum_{i = 1}^{n}(x_{i} - \overline{x})^{2}},
\end{equation}
where the \(x_{(i)}\)'s are the order statistics (see Section BLANK)
and the constants \(a_{i}\) form the components of a vector
\(\mathbf{a}_{1\times\mathrm{n}}\) defined by
\begin{equation}
\mathbf{a}=\frac{\mathbf{m}^{\mathrm{T}}\mathbf{V}^{-1}}{\sqrt{\mathbf{m}^{\mathrm{T}}\mathbf{V}^{-1}\mathbf{V}^{-1}\mathbf{m}}},
\end{equation}
where \(\mathbf{m}_{\mathrm{n}\times1}\) and \(\mathbf{V}_{\mathrm{n}
\times \mathrm{n}} \) are the mean and covariance matrix,
respectively, of the order statistics from an \(\mathsf{mvnorm}
\left(\mathtt{mean} = \mathbf{0},\,\mathtt{sigma} =
\mathbf{I}\right)\) distribution.  This test statistic \(W\) was
published in 1965 by (you guessed it): Shapiro and
Wilk\cite{Wilk1965}.  In contrast to most other test statistics we
know, we reject \(H_{0}\) if \(W\) is too \emph{small}.

\subsubsection{How to do it with \(\mathsf{R}\)}
\label{sec-10-5-2-1}

\begin{verbatim}
with(women, shapiro.test(height))
\end{verbatim}

\begin{verbatim}
: 
: 	Shapiro-Wilk normality test
: 
: data:  height
: W = 0.9636, p-value = 0.7545
\end{verbatim}

\section{Analysis of Variance}
\label{sec-10-6}

\subsection{How to do it with \(\mathsf{R}\)}
\label{sec-10-6-1}

I am thinking 
\begin{verbatim}
with(chickwts, by(weight, feed, shapiro.test))
\end{verbatim}

\begin{verbatim}
feed: casein

	Shapiro-Wilk normality test

data:  dd[x, ]
W = 0.9166, p-value = 0.2592

--------------------------------------------------------------------- 
feed: horsebean

	Shapiro-Wilk normality test

data:  dd[x, ]
W = 0.9376, p-value = 0.5264

--------------------------------------------------------------------- 
feed: linseed

	Shapiro-Wilk normality test

data:  dd[x, ]
W = 0.9693, p-value = 0.9035

--------------------------------------------------------------------- 
feed: meatmeal

	Shapiro-Wilk normality test

data:  dd[x, ]
W = 0.9791, p-value = 0.9612

--------------------------------------------------------------------- 
feed: soybean

	Shapiro-Wilk normality test

data:  dd[x, ]
W = 0.9464, p-value = 0.5064

--------------------------------------------------------------------- 
feed: sunflower

	Shapiro-Wilk normality test

data:  dd[x, ]
W = 0.9281, p-value = 0.3603
\end{verbatim}

and
\begin{verbatim}
temp <- lm(weight ~ feed, data = chickwts)
\end{verbatim}
and 
\begin{verbatim}
anova(temp)
\end{verbatim}

\begin{verbatim}
: Analysis of Variance Table
: 
: Response: weight
:           Df Sum Sq Mean Sq F value    Pr(>F)    
: feed       5 231129   46226  15.365 5.936e-10 ***
: Residuals 65 195556    3009                      
: ---
: Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
\end{verbatim}

Plot for the intuition of between versus within group variation.

\begin{verbatim}
y1 <- rnorm(300, mean = c(2,8,22))
plot(y1, xlim = c(-1,25), ylim = c(0,0.45) , type = "n")
f <- function(x){dnorm(x, mean = 2)}
curve(f, from = -1, to = 5, add = TRUE, lwd = 2)
f <- function(x){dnorm(x, mean = 8)}
curve(f, from = 5, to = 11, add = TRUE, lwd = 2)
f <- function(x){dnorm(x, mean = 22)}
curve(f, from = 19, to = 25, add = TRUE, lwd = 2)
rug(y1)
\end{verbatim}

\begin{figure}[ht!]
\centering
\includegraphics[width=0.9\textwidth]{fig/hypoth-between-versus-within.ps}
\caption[Between group versus within group variation]{\label{fig-between-versus-within}\small A plot of between group versus within group variation.}
\end{figure}


\begin{verbatim}
old.omd <- par(omd = c(.05,.88, .05,1))
F.setup(df1 = 5, df2 = 30)
F.curve(df1 = 5, df2 = 30, col='blue')
F.observed(3, df1 = 5, df2 = 30)
par(old.omd)
\end{verbatim}

\begin{figure}[ht!]
\centering
\includegraphics[width=0.9\textwidth]{fig/hypoth-some-F-plots-HH.ps}
\caption[Some \(F\) plots from the \texttt{HH} package]{\label{fig-some-F-plots-HH}\small Some \(F\) plots from the \texttt{HH} package.}
\end{figure}

\section{Sample Size and Power}
\label{sec-10-7}

The power function of a test for a parameter \(\theta\) is
\[
\beta(\theta)=\mathbb{P}_{\theta}(\mbox{Reject }H_{0}),\quad -\infty < \theta < \infty.
\]
Here are some properties of power functions:
\begin{enumerate}
\item \(\beta(\theta)\leq\alpha\) for any \(\theta\in\Theta_{0}\), and
\(\beta(\theta_{0})=\alpha\). We interpret this by saying that no
matter what value \(\theta\) takes inside the null parameter space,
there is never more than a chance of \(\alpha\) of rejecting the
null hypothesis. We have controlled the Type I error rate to be no
greater than \(\alpha\).
\item \(\lim_{n\to\infty}\beta(\theta)=1\) for any fixed
\(\theta\in\Theta_{1}\). In other words, as the sample size grows
without bound we are able to detect a nonnull value of \(\theta\)
with increasing accuracy, no matter how close it lies to the null
parameter space. This may appear to be a good thing at first
glance, but it often turns out to be a curse, because this means
that our Type II error rate grows as the sample size increases.
\end{enumerate}

\subsection{How to do it with \(\mathsf{R}\)}
\label{sec-10-7-1}

I am thinking about \texttt{replicate} \index{replicate@\texttt{replicate}}
here, and also \texttt{power.examp} \index{power.examp@\texttt{power.examp}}
from the \texttt{TeachingDemos} package \cite{TeachingDemos}. There is an
even better plot in upcoming work from the \texttt{HH} package \cite{HH}.

\begin{verbatim}
power.examp()
\end{verbatim}

\begin{figure}[ht!]
\centering
\includegraphics[width=0.9\textwidth]{fig/hypoth-power-examp.ps}
\caption[Plot of significance level and power]{\label{fig-power-examp}\small This graph was generated by the \texttt{power.examp} function from the \texttt{TeachingDemos} package. The plot corresponds to the hypothesis test \(H_{0}:\,\mu=\mu_{0}\) versus \(H_{1}:\,\mu=\mu_{1}\) (where \(\mu_{0}=0\) and \(\mu_{1}=1\), by default) based on a single observation \(X\sim\mathsf{norm}(\mathtt{mean}=\mu,\,\mathtt{sd}=\sigma)\). The top graph is of the \(H_{0}\) density while the bottom is of the \(H_{1}\) density. The significance level is set at \(\alpha=0.05\), the sample size is \(n=1\), and the standard deviation is \(\sigma=1\). The pink area is the significance level, and the critical value \(z_{0.05}\approx1.645\) is marked at the left boundary -- this defines the rejection region. When \(H_{0}\) is true, the probability of falling in the rejection region is exactly \(\alpha=0.05\). The same rejection region is marked on the bottom graph, and the probability of falling in it (when \(H_{1}\)  is true) is the blue area shown at the top of the display to be approximately \(0.26\). This probability represents the \emph{power} to detect a non-null mean value of \(\mu=1\). With the command the \texttt{run.power.examp()} at the command line the same plot opens, but in addition, there are sliders available that allow the user to interactively change the sample size \(n\), the standard deviation \(\sigma\), the true difference between the means \(\mu_{1}-\mu_{0}\), and the significance level \(\alpha\). By playing around the student can investigate the effect each of the aforementioned parameters has on the statistical power. Note that you need the \texttt{tkrplot} package \cite{tkrplot} for \texttt{run.power.examp}.}
\end{figure}

\newpage{}

\section{Exercises}
\label{sec-10-8}

\setcounter{thm}{0}
