\chapter{Sampling Distributions}
\label{sec-8}

\noindent
This is an important chapter; it is the bridge from probability and
descriptive statistics to inferential statistics which
turns raw data into quantifiable insights.
Here is the link: we are presented with a \emph{population} about which we
would like to learn. And while it would be desirable to examine every
single member of the population, we find that it is either impossible
or infeasible for us to do so, thus, we resort to collecting a
\emph{sample} instead. We do not lose heart. Our method will suffice,
provided the sample is \emph{representative} of the population. A good way
to achieve this is to sample \emph{randomly} from the population.

Supposing for the sake of argument that we have collected a random
sample, the next task is to make some \emph{sense} out of the data because
the complete list of sample information is usually cumbersome,
unwieldy. We summarize the data set with a descriptive \emph{statistic}, a
quantity calculated from the data (e.g, mean, variance, standard deviation).
But our sample was
random\ldots{} therefore, it stands to reason that our statistic will be
random, too. How is the statistic distributed?

The probability distribution associated with the population (from
which we sample) is called the \emph{population distribution}, and the
probability distribution associated with our statistic is called its
\emph{sampling distribution}; clearly, the two are interrelated. To learn
about the population distribution, it is imperative to know everything
we can about the sampling distribution. Such is the goal of this
chapter.

We begin by introducing the notion of simple random samples and
cataloguing some of their more convenient mathematical
properties. Next we focus on what happens in the special case of
sampling from the normal distribution (which, again, has several
convenient mathematical properties), and in particular, we meet the
sampling distribution of the mean.
Then we explore what happens to sampling distribution of the mean when
the population is not normal and prove one of the most remarkable
theorems in statistics, the Central Limit Theorem (CLT).

With the CLT in hand, we then investigate the sampling distributions
of several other popular statistics, taking full advantage of those
with a tractable form. We finish the chapter with an exploration of
statistics whose sampling distributions are not quite so tractable,
and to accomplish this goal we will use simulation methods that are
grounded in all of our work in the previous chapters.

\textbf{Highlights:}
\begin{itemize}
\item the notion of population versus simple random sample, parameter
versus statistic, and population distribution versus sampling
distribution
\item the classical sampling distributions of the standard sample statistics
\item the Central Limit Theorem, period.
\item some basic concepts related to sampling distribution utility, such
as bias and variance
\item How to do it with $\mathsf{R}$
\end{itemize}

\section{Simple Random Samples}
\label{sec-8-1}

\begin{defn}
If \(X_{1}\), \(X_{2}\), \ldots{}, \(X_{n}\) are independent with
\(X_{i}\sim f\) for \(i=1,2,\ldots,n\), then we say that \(X_{1}\),
\(X_{2}\), \ldots{}, \(X_{n}\) are \emph{independent and identically
distributed} (IID) from the population \(f\) or alternatively we say
that \(X_{1}\), \(X_{2}\), \ldots{}, \(X_{n}\) are a \emph{simple random sample
of size} \(n\), denoted \(SRS(n)\), from the population \(f\).
\end{defn}

\begin{prop}
\label{pro-mean-sd-xbar} Let \(X_{1}\), \(X_{2}\), \ldots{}, \(X_{n}\) be a
\(SRS(n)\) from a population distribution with mean \(\mu\) and finite
standard deviation \(\sigma\). Then the mean and standard deviation of
\(\overline{X}\) are given by the formulas \(\mu_{\overline{X}}=\mu\)
and \(\sigma_{\overline{X}}=\sigma/\sqrt{n}\).
\end{prop}

\begin{proof}
By definition: \[ \overline{X}=\frac{1}{n}\sum_{i=1}^nX_i .\]
The mean is easy:
\[ \mathbb{E} \overline{X}=\mathbb{E}\left(\frac{1}{n}\sum_{i=1}^{n}X_{i}\right)=\frac{1}{n}\sum_{i=1}^{n}\mathbb{E} X_{i}=\frac{1}{n}\sum_{i=1}^{n}\mu=\frac{1}{n}n\mu=\mu.  \]
To compute the variance, as an intermediate step, we calculate:
\[ \mathbb{E} \overline{X}^{2}=\mathbb{E}\left(\frac{1}{n}\sum_{i=1}^{n}X_{i}\right)^{2}=\frac{1}{n^2}\mathbb{E}\left(\sum_{i=1}^{n}X_{i}^{2}+\sum_{i=1}^{n}\sum_{j\ne i}X_{i}X_{j}\right)=\frac{1}{n^2}\left(\sum_{i=1}^{n}\mathbb{E}X_{i}^{2}+\sum_{i=1}^{n}\sum_{j\ne i}\mathbb{E}X_{i}X_{j}\right). \]
Using linearity of expectation the \(\mathbb{E}\) distributes through the sums.
Now \(\mathbb{E} X_{i}^{2}=\sigma^{2}+\mu^{2}\) and \(\mathbb{E} X_{i}X_{j}=\mathbb{E} X_{i}\mathbb{E} X_{j}=\mu\mu=\mu^2\) when \(i\neq j\) because of independence.
Thus
\begin{eqnarray*}
\mathbb{E} \overline{X}^{2} & = & \frac{1}{n^2} \left( \sum_{i=1}^{n}(\sigma^{2}+\mu^{2})+\sum_{i=1}^{n}\sum_{j\ne i}\mu^2 \right)\\
 & = & \frac{1}{n^2}\left( n(\sigma^2+\mu^2)+n(n-1)\mu^2 \right)\\
 & = & \frac{1}{n^2}\left( n\sigma^2+n^2\mu^2 \right)\\
 & = & \frac{1}{n}\sigma^2+\mu^2
\end{eqnarray*}
To complete the proof, recall the identity
\(\sigma_{\overline{X}}^{2}=\mathbb{E} \overline{X}^{2}-\left(\mathbb{E} \overline{X}\right)^{2}=\sigma^2/n\).
\end{proof}

\section{The Central Limit Theorem}

\subsection{Sampling from a Normal Distribution}
\label{sec-8-2-1}

\begin{thm}
\label{thm-Xbar-andS} Let \(X_{1}\), \(X_{2}\), \ldots{}, \(X_{n}\) be a
\(SRS(n)\) from a
\(\mathsf{norm}(\mathtt{mean}=\mu,\,\mathtt{sd}=\sigma)\)
distribution, and let
\begin{equation}
\overline{X}=\sum_{i=1}^{n}X_{i}\quad \mbox{and}\quad S^{2}=\frac{1}{n-1}\sum_{i=1}^{n}(X_{i}-\overline{X})^{2}.
\end{equation}
Then
\begin{enumerate}
\item
\(\overline{X}\) and \(S^{2}\) are independent.
\item
The sample mean \(\overline{X}\) has a \(\mathsf{norm}(\mathtt{mean}=\mu,\,\mathtt{sd}=\sigma/\sqrt{n})\) sampling distribution.
Therefore
\begin{equation}
Z=\frac{\overline{X}-\mu}{\sigma/\sqrt{n}}
\end{equation}
has a standard normal distribution \(\mathsf{norm}(\mathtt{mean}=0,\,\mathtt{sd}=1)\). 
\item The rescaled sample variance
\begin{equation}
\frac{(n-1)}{\sigma^{2}}S^{2}=\frac{\sum_{i=1}^{n}(X_{i}-\overline{X})^{2}}{\sigma^{2}}
\end{equation}
has a \(\mathsf{chisq}(\mathtt{df}=n-1)\) sampling distribution.
\item
The Student's T statistic
\begin{equation}
T=\frac{\overline{X}-\mu}{S/\sqrt{n}}
\end{equation}
has a \(\mathsf{t}(\mathtt{df}=n-1)\) sampling distribution.
\end{enumerate}
\end{thm}

\begin{proof}
The proof is beyond the scope of the present chapter, but the theorem is
simply too important to be omitted. The interested reader could
consult Casella and Berger \cite{Casella2002}, or Hogg \emph{et al}
\cite{Hogg2005}.
\end{proof}

\subsection{Sampling from a General Distribution}

Now we study the distribution of the sample mean when the underlying distribution is \emph{not} normal.
We saw in Section \ref{sec-8-2-1} that when \(X_{1}\), \(X_{2}\), \ldots{} , \(X_{n}\) is a \(SRS(n)\) from a
\(\mathsf{norm}(\mathtt{mean}=\mu,\,\mathtt{sd}=\sigma)\) distribution
then \(\overline{X} \sim \mathsf{norm}(\mathtt{mean} =
\mu,\,\mathtt{sd} = \sigma/\sqrt{n})\). In other words, we may say
when the underlying
population is normal that the sampling distribution of \(Z\) defined
by
\begin{equation}
Z=\frac{\overline{X}-\mu}{\sigma/\sqrt{n}}
\end{equation}
is \(\mathsf{norm}(\mathtt{mean}=0,\,\mathtt{sd}=1)\). 
However, there are many populations that are \emph{not} normal \ldots{} and the
statistician often needs sampling from such populations.
What can be said in this case?

The answer is contained in the following theorem.

\begin{thm}[\textbf{The Central Limit Theorem}]
\label{thm-central-limit-thrm} Let \(X_{1}\), \(X_{2}\), \ldots{}, \(X_{n}\) be
a \(SRS(n)\) from a population distribution with mean \(\mu\) and
finite standard deviation \(\sigma\). Then the sampling distribution
of
\begin{equation}
Z=\frac{\overline{X}-\mu}{\sigma/\sqrt{n}}
\end{equation}
approaches a \(\mathsf{norm}(\mathtt{mean}=0,\,\mathtt{sd}=1)\) distribution as \(n\to\infty\).
\end{thm}

\begin{rem}
We suppose that \(X_{1}\), \(X_{2}\), \ldots{} , \(X_{n}\) are IID, and we
learned in Section \ref{sec-8-1} that \(\overline{X}\) has
mean \(\mu\) and standard deviation \(\sigma/\sqrt{n}\), so we already
knew that \(Z\) has mean zero and standard deviation one. The beauty
of the CLT is that it addresses the \emph{shape} of \(Z\)'s distribution
when the sample size is large.
\end{rem}

\begin{rem}
Notice that the shape of the underlying population's distribution is
not mentioned in Theorem \ref{thm-central-limit-thrm}; indeed, the result is true for any
population that is well-behaved enough to have a finite standard
deviation. In particular, if the population is normally distributed
then we know from Section \ref{sec-8-2-1} that the distribution
of \(\overline{X}\) (and \(Z\) by extension) is \emph{exactly} normal, for
\emph{every} \(n\).
\end{rem}

\begin{rem}
How large is "sufficiently large"? It is here that the shape of the
underlying population distribution plays a role. For populations with
distributions that are approximately symmetric and mound-shaped, the
samples may need to be only of size four or five, while for highly
skewed or heavy-tailed populations the samples may need to be much
larger for the distribution of the sample means to begin to show a
bell-shape. Regardless, for a given population distribution (with
finite standard deviation) the approximation tends to be better for
larger sample sizes.
\end{rem}

\section{Sampling Distributions of Two-Sample Statistics}
\label{sec-8-4}

There are often two populations under consideration, and it sometimes
of interest to compare properties between groups. To do so we take
independent samples from each population and calculate respective
sample statistics for comparison. In some simple cases the sampling
distribution of the comparison is known and easy to derive; such cases
are the subject of the present section.

\subsection{Difference of Independent Sample Means}
\label{sec-8-4-1}

\begin{prop}
Let \(X_{1}\), \(X_{2}\), \ldots{} , \(X_{n_{1}}\) be an \(SRS(n_{1})\)
from a
\(\mathsf{norm}(\mathtt{mean}=\mu_{X},\,\mathtt{sd}=\sigma_{X})\)
distribution and let \(Y_{1}\), \(Y_{2}\), \ldots{} , \(Y_{n_{2}}\) be an
\(SRS(n_{2})\) from a
\(\mathsf{norm}(\mathtt{mean}=\mu_{Y},\,\mathtt{sd}=\sigma_{Y})\)
distribution. Suppose that \(X_{1}\), \(X_{2}\), \ldots{} , \(X_{n_{1}}\)
and \(Y_{1}\), \(Y_{2}\), \ldots{} , \(Y_{n_{2}}\) are independent
samples. Then the quantity
\begin{equation}
\label{eq-diff-indep-sample-means}
\frac{\overline{X}-\overline{Y}-(\mu_{X}-\mu_{Y})}{\sqrt{\left.\sigma_{X}^{2}\right/ n_{1}+\left.\sigma_{Y}^{2}\right/ n_{2}}}
\end{equation}
has a \(\mathsf{norm}(\mathtt{mean}=0,\,\mathtt{sd}=1)\) sampling
distribution. Equivalently, \(\overline{X}-\overline{Y}\) has a
\(\mathsf{norm}(\mathtt{mean}=\mu_{X}-\mu_{Y},\,\mathtt{sd}=\sqrt{\left.\sigma_{X}^{2}\right/
n_{1}+\left.\sigma_{Y}^{2}\right/ n_{2}})\) sampling distribution.
\end{prop}

\begin{proof}
We know that \(\overline{X}\) is
\(\mathsf{norm}(\mathtt{mean}=\mu_{X},\,\mathtt{sd}=\sigma_{X}/\sqrt{n_{1}})\)
and we also know that \(\overline{Y}\) is
\(\mathsf{norm}(\mathtt{mean}=\mu_{Y},\,\mathtt{sd}=\sigma_{Y}/\sqrt{n_{2}})\). And
since the samples \(X_{1}\), \(X_{2}\), \ldots{}, \(X_{n_{1}}\) and
\(Y_{1}\), \(Y_{2}\), \ldots{}, \(Y_{n_{2}}\) are independent, so too are
\(\overline{X}\) and \(\overline{Y}\). The distribution of their
difference is thus normal as well, and the mean and standard deviation
are as given.
\end{proof}

\begin{rem}
Even if the distribution of one or both of the samples is not normal,
the quantity in Equation \eqref{eq-diff-indep-sample-means} will be
approximately normal provided both sample sizes are large.
\end{rem}

\textbf{Remark:} For the special case of \(\mu_{X}=\mu_{Y}\) we have shown
 that
\begin{equation}
\frac{\overline{X} - \overline{Y}}{\sqrt{ \sigma_{X}^{2}/ n_{1} + \sigma_{Y}^{2}/n_{2}}}
\end{equation}

has a \(\mathsf{norm}(\mathtt{mean}=0,\,\mathtt{sd}=1)\) sampling
distribution, or in other words, \(\overline{X} - \overline{Y}\) has a
\(\mathsf{norm}(\mathtt{mean} = 0,\,\mathtt{sd} = \sqrt{\sigma_{X}^{2}
/ n_{1} + \sigma_{Y}^{2} / n_{2}})\) sampling distribution.


\subsection{Difference of Independent Sample Proportions}
\label{sec-8-4-2}

\begin{prop}
Let \(X_{1}\), \(X_{2}\), \ldots{}, \(X_{n_{1}}\) be an \(SRS(n_{1})\) from
a \(\mathsf{binom}(\mathtt{size}=1,\,\mathtt{prob}=p_{1})\)
distribution and let \(Y_{1}\), \(Y_{2}\), \ldots{}, \(Y_{n_{2}}\) be an
\(SRS(n_{2})\) from a
\(\mathsf{binom}(\mathtt{size}=1,\,\mathtt{prob}=p_{2})\)
distribution. Suppose that \(X_{1}\), \(X_{2}\), \ldots{} , \(X_{n_{1}}\)
and \(Y_{1}\), \(Y_{2}\), \ldots{} , \(Y_{n_{2}}\) are independent
samples. Define
\begin{equation}
\hat{p}_{1}=\frac{1}{n_{1}}\sum_{i=1}^{n_{1}}X_{i}\quad \mbox{and}\quad \hat{p}_{2}=\frac{1}{n_{2}}\sum_{j=1}^{n_{2}}Y_{j}.
\end{equation}
Then the sampling distribution of
\begin{equation}
\frac{\hat{p}_{1}-\hat{p}_{2}-(p_{1}-p_{2})}{\sqrt{\frac{p_{1}(1-p_{1})}{n_{1}}+\frac{p_{2}(1-p_{2})}{n_{2}}}}
\end{equation}
approaches a \(\mathsf{norm}(\mathtt{mean}=0,\,\mathtt{sd}=1)\) distribution as both \(n_{1},\, n_{2}\to\infty\). In other words, the sampling distribution of \(\hat{p}_{1}-\hat{p}_{2}\) is approximately
\begin{equation}
\mathsf{norm}\left(\mathtt{mean}=p_{1}-p_{2},\,\mathtt{sd}=\sqrt{\frac{p_{1}(1-p_{1})}{n_{1}}+\frac{p_{2}(1-p_{2})}{n_{2}}}\right),
\end{equation}
provided both \(n_{1}\) and \(n_{2}\) are sufficiently large.
\end{prop}

\begin{proof}
We know that \(\hat{p}_{1}\) is approximately normal for \(n_{1}\)
sufficiently large by the CLT, and we know that \(\hat{p}_{2}\) is
approximately normal for \(n_{2}\) sufficiently large, also by the
CLT. Further, \(\hat{p}_{1}\) and \(\hat{p}_{2}\) are independent
since they are derived from independent samples. And a difference of
independent (approximately) normal distributions is (approximately)
normal as given.
\end{proof}

\subsection{Ratio of Independent Sample Variances}
\label{sec-8-4-3}

\begin{prop}
Let \(X_{1}\), \(X_{2}\), \ldots{}, \(X_{n_{1}}\) be an \(SRS(n_{1})\) from
a \(\mathsf{norm}(\mathtt{mean}=\mu_{X},\,\mathtt{sd}=\sigma_{X})\)
distribution and let \(Y_{1}\), \(Y_{2}\), \ldots{} , \(Y_{n_{2}}\) be an
\(SRS(n_{2})\) from a
\(\mathsf{norm}(\mathtt{mean}=\mu_{Y},\,\mathtt{sd}=\sigma_{Y})\)
distribution. Suppose that \(X_{1}\), \(X_{2}\), \ldots{} , \(X_{n_{1}}\)
and \(Y_{1}\), \(Y_{2}\), \ldots{} , \(Y_{n_{2}}\) are independent
samples. Then the ratio
\begin{equation}
F=\frac{\sigma_{Y}^{2}S_{X}^{2}}{\sigma_{X}^{2}S_{Y}^{2}}
\end{equation}
has an \(\mathsf{f}(\mathtt{df1}=n_{1}-1,\,\mathtt{df2}=n_{2}-1)\)
sampling distribution.
\end{prop}

\begin{proof}
We know from Theorem \ref{thm-Xbar-andS} that
\((n_{1}-1)S_{X}^{2}/\sigma_{X}^{2}\) is distributed
\(\mathsf{chisq}(\mathtt{df}=n_{1}-1)\) and
\((n_{2}-1)S_{Y}^{2}/\sigma_{Y}^{2}\) is distributed
\(\mathsf{chisq}(\mathtt{df}=n_{2}-1)\). Now write \[
F=\frac{\sigma_{Y}^{2}S_{X}^{2}}{\sigma_{X}^{2}S_{Y}^{2}}=\frac{\left.(n_{1}-1)S_{X}^{2}\right/
(n_{1}-1)}{\left.(n_{2}-1)S_{Y}^{2}\right/
(n_{2}-1)}\cdot\frac{\left.1\right/ \sigma_{X}^{2}}{\left.1\right/
\sigma_{Y}^{2}}, \] by multiplying and dividing the numerator with
\(n_{1}-1\) and doing likewise for the denominator with
\(n_{2}-1\). Now we may regroup the terms into \[
F=\frac{\left.\frac{(n_{1}-1)S_{X}^{2}}{\sigma_{X}^{2}}\right/
(n_{1}-1)}{\left.\frac{(n_{2}-1)S_{Y}^{2}}{\sigma_{Y}^{2}}\right/
(n_{2}-1)}, \] and we recognize \(F\) to be the ratio of independent
\(\mathsf{chisq}\) distributions, each divided by its respective
numerator \(\mathtt{df1}=n_{1}-1\) and denominator \(\mathtt{df2}=n_{2}-1\) degrees of freedom.
This is, indeed, the definition of Snedecor's \(F\) distribution.
\end{proof}

\begin{rem}
For the special case of \(\sigma_{X}=\sigma_{Y}\) we have shown that
\begin{equation}
F=\frac{S_{X}^{2}}{S_{Y}^{2}}
\end{equation}
has an \(\mathsf{f}(\mathtt{df1}=n_{1}-1,\,\mathtt{df2}=n_{2}-1)\)
sampling distribution.
\end{rem}

\section{How to do it with $\mathsf{R}$}
\label{sec-8-5}

Now we simulate the distribution of sample mean from a general distribution, e.g., $\mathsf{exp}(\mathtt{rate}=0.05)$.
Note that the shape of $exp$ distribution looks quite different with that of normal distributions.
This can be demonstrated by:
\begin{Verbatim}
curve(dexp(x, 0.05), from=0, to=100)
\end{Verbatim}
or
\begin{Verbatim}
hist(rexp(1000, 0.05), probability=TRUE)
\end{Verbatim}

In the following we try SRS with increasing sizes:
\begin{Verbatim}
par(mfrow=c(2,2))
hist(replicate(1000, mean(rexp(100, 0.05))))
hist(replicate(1000, mean(rexp(1000, 0.05))))
hist(replicate(1000, mean(rexp(10000, 0.05))))
hist(replicate(1000, mean(rexp(100000, 0.05))))
\end{Verbatim}
Does the distribution of sample mean coverge to the theoretical value when the sample size is increased?

You can use similar procedures to study sampling distribution for other random variables or statistics.
Some statistics are meaningful, but their sampling distribution is
not quite so tidy to describe analytically. What do we do then?
As it turns out, we do not need to know the exact analytical form of
the sampling distribution; sometimes it is enough to approximate it
with a simulated distribution. In this section we will show you
how. Note that \(\mathsf{R}\) is particularly well suited to compute
simulated sampling distributions, much more so than, say, SPSS or SAS.
Just replace the function $\mathsf{mean}$ with the statistics of your interest.

\newpage{}

\section{Exercises}
\label{sec-8-6}
\setcounter{thm}{0}

\begin{xca}
Let \(X_{1}\),\ldots{}, \(X_{25}\) be a random sample from a
\(\mathsf{norm}(\mathtt{mean}=37,\,\mathtt{sd}=45)\) distribution, and
let \(\overline{X}\) be the sample mean of these \(n=25\)
observations.
\begin{enumerate}
\item What is the mean of \(\overline{X}\)?
\item What is the standard deviation of \(\overline{X}\)?
\item How is \(\overline{X}\) distributed? 
\(\mathsf{norm}(\mathtt{mean}=37,\,\mathtt{sd}=45/\sqrt{25})\)
\item Find \(\mathbb{P}(\overline{X} > 43.1)\).
\end{enumerate}
\end{xca}
