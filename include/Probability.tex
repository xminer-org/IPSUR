\chapter{Probability}
\label{sec-4}

\noindent
In this chapter we define the basic terminology associated with
probability and derive some of its properties.
We discuss conditional probability and
independent events, along with Bayes' Theorem. We finish the chapter
with an introduction to random variables, which paves the way for the
next two chapters.

In this book we distinguish between two types of experiments:
\emph{deterministic} and \emph{random}. A \emph{deterministic} experiment is one
whose outcome may be predicted with certainty beforehand, such as
combining Hydrogen and Oxygen, or adding two numbers such as
\(2+3\). A \emph{random} experiment is one whose outcome is determined by
chance. We posit that the outcome of a random experiment may not be
predicted with certainty beforehand, even in principle. Examples of
random experiments include tossing a coin, rolling a die, and throwing
a dart on a board, how many red lights you encounter on the drive
home, how many ants traverse a certain patch of sidewalk over a short
period, \emph{etc}.

\textbf{Highlights:}
\begin{itemize}
\item nuts and bolts of basic probability jargon: sample spaces, events,
probability functions, \emph{etc}.
\item conditional probability and its relationship with independence
\item Bayes' Rule and how it relates to the subjective view of probability
\item what we mean by 'random variables', and where they come from
\end{itemize}

\begin{figure}[ht!]
\centering
\includegraphics[width=0.9\textwidth]{fig/prob-diagram.ps}
\caption[Two types of experiments]{\label{fig-diagram}\small Two types of experiments.}
\end{figure}

\section{Sample Spaces}
\label{sec-4-1}

For a random experiment \(E\), the set of all possible outcomes of
\(E\) is called the \emph{sample space} \index{sample space} and is denoted
by the letter \(S\). For a coin-toss experiment, \(S\) would be the
results "Head" and "Tail", which we may represent by \( S = \{H,T
\} \). Formally, the performance of a random experiment is the
unpredictable selection of an outcome in \(S\).

\subsection{How to do it with \(\mathsf{R}\)}
\label{sec-4-1-1}

A sample space is (usually) represented by a \emph{data frame}, that is, a rectangular collection of variables.
Each row of the data frame corresponds to an outcome of the experiment.
The data frame choice is convenient for its simplicity and compatibility with many R functions and packages.

Consider the random experiment of dropping a Styrofoam cup onto the
floor from a height of four feet. The cup hits the ground and
eventually comes to rest. It could land upside down, right side up, or
it could land on its side. We represent these possible outcomes of the
random experiment by the following.

\begin{Verbatim}
S <- data.frame(lands = c("down","up","side"))
S
\end{Verbatim}

\begin{verbatim}
:   lands
: 1  down
: 2    up
: 3  side
\end{verbatim}

The sample space \texttt{S} contains the column \texttt{lands} which stores the
outcomes \texttt{down}, \texttt{up}, and \texttt{side}.



Some sample spaces are so common that convenience wrappers were
written to set them up with minimal effort. The underlying machinery
that does the work includes the \texttt{expand.grid} function in the \texttt{base}
package, \texttt{combn} in the \texttt{utils} package, both are automatically loaded.

\subsection{Sampling from Urns}
\label{sec-4-1-2}

This is perhaps the most fundamental type of random experiment. We
have an urn that contains a bunch of distinguishable objects (balls)
inside. We shake up the urn, reach inside, grab a ball, and take a
look. That's all.

But there are all sorts of variations on this theme. Maybe we would
like to grab more than one ball -- say, two balls. What are all of the
possible outcomes of the experiment now? It depends on how we
sample. We could select a ball, take a look, put it back, and sample
again. Another way would be to select a ball, take a look -- but do
not put it back -- and sample again (equivalently, just reach in and
grab two balls). There are certainly more possible outcomes of the
experiment in the former case than in the latter. In the first
(second) case we say that sampling is done \emph{with (without)
replacement}.

There is more. Suppose we do not actually keep track of which ball
came first. All we observe are the two balls, and we have no idea
about the order in which they were selected. We call this \emph{unordered
sampling} (in contrast to \emph{ordered}) because the order of the
selections does not matter with respect to what we observe. We might
as well have selected the balls and put them in a bag before looking.

Note that this one general class of random experiments contains as a
special case all of the common elementary random experiments. Tossing
a coin twice is equivalent to selecting two balls labeled \(H\) and
\(T\) from an urn, with replacement. The die-roll experiment is
equivalent to selecting a ball from an urn with six elements, labeled
1 through 6.

\subsubsection{How to do it with \(\mathsf{R}\)}
\label{sec-4-1-2-1}

\begin{Verbatim}
urnsamples <- function (x, size, replace = FALSE, ordered = FALSE, ...){
  nurn <- length(x)
  if (isTRUE(replace)) {
    ind <- as.matrix(expand.grid(rep(list(seq(nurn)), size)))
    if (isTRUE(ordered)) ind <- t(ind)
    else ind <- unique(apply(ind, 1, sort), MARGIN = 2)
  } else {
    ind <- combn(seq(nurn), size)
    if (isTRUE(ordered)) {
      perm <- combinat::permn(size)
      perm <- matrix(unlist(perm), ncol=length(perm))
      res <- matrix(nrow=size, ncol=0)
      for (i in seq(length=ncol(ind))) {
          perms <- matrix(ind[, i][perm], nrow = size)
          res <- cbind(res, perms)
      }
      ind <- res
    }
  }
  return(data.frame(t(matrix(x[ind], nrow = size))))
}
\end{Verbatim}

Note that the \texttt{urnsamples} function uses the package \texttt{combinat}.

We will accomplish sampling from urns with
the \texttt{urnsamples} \index{urnsamples@\texttt{urnsamples}}
function, which has arguments \texttt{x}, \texttt{size}, \texttt{replace}, and
\texttt{ordered}. The argument \texttt{x} represents the urn from which sampling is
to be done. The \texttt{size} argument tells how large the sample will
be. The \texttt{ordered} and \texttt{replace} arguments are logical and specify how
sampling will be performed. We will discuss each in turn.

\label{exa-sample-urn-two-from-three} Let our urn simply contain three
balls, labeled 1, 2, and 3, respectively. We are going to take a
sample of size 2 from the urn.

\subsubsection{Ordered, With Replacement}
\label{sec-4-1-2-2}

If sampling is with replacement, then we can get any outcome 1, 2, or
3 on any draw. Further, by "ordered" we mean that we shall keep
track of the order of the draws that we observe. We can accomplish
this in \(\mathsf{R}\) with

\begin{Verbatim}
urnsamples(1:3, size = 2, replace = TRUE, ordered = TRUE)
\end{Verbatim}

\begin{verbatim}
  X1 X2
1  1  1
2  2  1
3  3  1
4  1  2
5  2  2
6  3  2
7  1  3
8  2  3
9  3  3
\end{verbatim}

Notice that rows 2 and 4 are identical, save for the order in which
the numbers are shown. Further, note that every possible pair of the
numbers 1 through 3 are listed. This experiment is equivalent to
rolling a 3-sided die twice, which we could have accomplished with
\texttt{rolldie(2, nsides = 3)}.

\begin{Verbatim}
rolldie <- function(times, nsides = 6, makespace = FALSE){
    res <- expand.grid(rep(list(seq(nsides)), times))
    names(res) <- c(paste(rep("X", times), 1:times, sep = ""))
    if (makespace) res$probs <- 1/nrow(res)
    return(res)
}
\end{Verbatim}

\subsubsection{Ordered, Without Replacement}
\label{sec-4-1-2-3}

Here sampling is without replacement, so we may not observe the same
number twice in any row. Order is still important, however, so we
expect to see the outcomes \texttt{1,2} and \texttt{2,1} somewhere in our data
frame.

\begin{Verbatim}
urnsamples(1:3, size = 2, replace = FALSE, ordered = TRUE)
\end{Verbatim}

\begin{verbatim}
:   X1 X2
: 1  1  2
: 2  2  1
: 3  1  3
: 4  3  1
: 5  2  3
: 6  3  2
\end{verbatim}

This is just as we expected. Notice that there are less rows in this
answer due to the more restrictive sampling procedure. If the numbers
1, 2, and 3 represented "Fred", "Mary", and "Sue", respectively,
then this experiment would be equivalent to selecting two people of
the three to serve as president and vice-president of a company,
respectively, and the sample space shown above lists all possible ways
that this could be done.

\subsubsection{Unordered, Without Replacement}
\label{sec-4-1-2-4}

Again, we may not observe the same outcome twice, but in this case, we
will only retain those outcomes which (when jumbled) would not
duplicate earlier ones.

\begin{Verbatim}
urnsamples(1:3, size = 2, replace = FALSE, ordered = FALSE) 
\end{Verbatim}

\begin{verbatim}
:   X1 X2
: 1  1  2
: 2  1  3
: 3  2  3
\end{verbatim}

This experiment is equivalent to reaching in the urn, picking a pair,
and looking to see what they are. This is the default setting of
\texttt{urnsamples}, so we would have received the same output by simply
typing \texttt{urnsamples(1:3, 2)}.

\subsubsection{Unordered, With Replacement}
\label{sec-4-1-2-5}

The last possibility is perhaps the most interesting. We replace the
balls after every draw, but we do not remember the order in which the
draws came.

\begin{Verbatim}
urnsamples(1:3, size = 2, replace = TRUE, ordered = FALSE) 
\end{Verbatim}

\begin{verbatim}
:   X1 X2
: 1  1  1
: 2  1  2
: 3  1  3
: 4  2  2
: 5  2  3
: 6  3  3
\end{verbatim}

We may interpret this experiment in a number of alternative ways. One
way is to consider this as simply putting two 3-sided dice in a cup,
shaking the cup, and looking inside -- as in a game of \emph{Liar's Dice},
for instance. Each row of the sample space is a potential pair we
could observe. Another way is to view each outcome as a separate
method to distribute two identical golf balls into three boxes labeled
1, 2, and 3. Regardless of the interpretation, \texttt{urnsamples} lists
every possible way that the experiment can conclude.



Note that the urn does not need to contain numbers; we could have just
as easily taken our urn to be \texttt{x = c("Red","Blue","Green")}. But,
there is an \textbf{important} point to mention before proceeding. Astute
readers will notice that in our example, the balls in the urn were
\emph{distinguishable} in the sense that each had a unique label to
distinguish it from the others in the urn. A natural question would
be, "What happens if your urn has indistinguishable elements, for
example, what if \texttt{x = c("Red","Red","Blue")}?" The answer is that
\texttt{urnsamples} behaves as if each ball in the urn is distinguishable,
regardless of its actual contents. We may thus imagine that while
there are two red balls in the urn, the balls are such that we can
tell them apart (in principle) by looking closely enough at the
imperfections on their surface.
In this way, when the \texttt{x} argument of \texttt{urnsamples} has repeated
elements, the resulting sample space may appear to be \texttt{ordered = TRUE}
even when, in fact, the call to the function was \texttt{urnsamples(...,
ordered = FALSE)}. Similar remarks apply for the \texttt{replace} argument.

\section{Events}
\label{sec-4-2}

An \emph{event} \index{event} \(A\) is merely a collection of outcomes, or
in other words, a subset of the sample space. After the
performance of a random experiment \(E\) we say that the event \(A\)
\emph{occurred} if the experiment's outcome belongs to \(A\). We say that a
bunch of events \(A_{1}\), \(A_{2}\), \(A_{3}\), \ldots{} are \emph{mutually
exclusive} \index{mutually exclusive} or \emph{disjoint} if \(A_{i}\cap
A_{j}=\emptyset\) for any distinct pair \(A_{i}\neq A_{j}\). For
instance, in the coin-toss experiment the events \( A = \{
\mbox{Heads} \}\) and \( B = \{ \mbox{Tails} \} \) would be mutually
exclusive.

\subsection{How to do it with \(\mathsf{R}\)}
\label{sec-4-2-1}

\begin{Verbatim}
tosscoin <- function (times, makespace = FALSE){
    res <- expand.grid(rep(list(c("H", "T")), times))
    names(res) <- c(paste(rep("toss", times), 1:times, sep = ""))
    if (makespace) res$probs <- 1/nrow(res)
    return(res)
}
\end{Verbatim}

Given a data frame sample/probability space \texttt{S}, we may extract rows
using the \texttt{[]} operator:

\begin{Verbatim}
S <- tosscoin(2, makespace = TRUE) 
S[1:3, ] 
\end{Verbatim}

\begin{verbatim}
:   toss1 toss2 probs
: 1     H     H  0.25
: 2     T     H  0.25
: 3     H     T  0.25
\end{verbatim}

\begin{Verbatim}
S[c(2,4), ] 
\end{Verbatim}

\begin{verbatim}
:   toss1 toss2 probs
: 2     T     H  0.25
: 4     T     T  0.25
\end{verbatim}

and so forth. We may also extract rows that satisfy a logical
expression using the \texttt{subset} function, for instance

\begin{Verbatim}
cards <- function (jokers = FALSE, makespace = FALSE){
    x <- c(2:10, "J", "Q", "K", "A")
    y <- c("Club", "Diamond", "Heart", "Spade")
    res <- expand.grid(rank = x, suit = y)
    if (jokers) {
        jokers <- data.frame(rank = c("Joker", "Joker"), suit = c(NA, NA))
        res <- rbind(res, jokers)
    }
    if (makespace) res$probs <- 1/nrow(res)
    return(res)
}
\end{Verbatim}

\begin{Verbatim}
S <- cards() 
\end{Verbatim}

\begin{Verbatim}
subset(S, suit == "Heart") 
\end{Verbatim}

\begin{verbatim}
   rank  suit
27    2 Heart
28    3 Heart
29    4 Heart
30    5 Heart
31    6 Heart
32    7 Heart
33    8 Heart
34    9 Heart
35   10 Heart
36    J Heart
37    Q Heart
38    K Heart
39    A Heart
\end{verbatim}

\begin{Verbatim}
subset(S, rank %in% 7:9)
\end{Verbatim}

\begin{verbatim}
   rank    suit
6     7    Club
7     8    Club
8     9    Club
19    7 Diamond
20    8 Diamond
21    9 Diamond
32    7   Heart
33    8   Heart
34    9   Heart
45    7   Spade
46    8   Spade
47    9   Spade
\end{verbatim}

We could continue indefinitely. Also note that mathematical
expressions are allowed:

\begin{Verbatim}
subset(rolldie(3), X1+X2+X3 > 16) 
\end{Verbatim}

\begin{verbatim}
:     X1 X2 X3
: 180  6  6  5
: 210  6  5  6
: 215  5  6  6
: 216  6  6  6
\end{verbatim}

\subsection{Set Union, Intersection, and Difference}
\label{sec-4-2-3}

Given subsets \(A\) and \(B\), it is often useful to manipulate them
in an algebraic fashion. To this end, we have three set operations at
our disposal: union, intersection, and difference. Below is a table
that summarizes the pertinent information about these operations.

\begin{table}[htb]
\caption[Set operations]{Basic set operations.  The first column lists the name, the second shows the typical notation, the third describes set membership, and the fourth shows how to accomplish it with R.}
\centering
\begin{tabular}{llll}
\hline
Name & Denoted & Defined by elements & Code\\
\hline
Union & \(A\cup B\) & in \(A\) or \(B\) or both & \texttt{union(A,B)}\\
Intersection & \(A\cap B\) & in both \(A\) and \(B\) & \texttt{intersect(A,B)}\\
Difference & \(A\backslash B\) & in \(A\) but not in \(B\) & \texttt{setdiff(A,B)}\\
\hline
\end{tabular}
\end{table}

By default, \texttt{union}, \texttt{intersect}, and \texttt{setdiff} cannot work directly for rows in data frames.
To manipulate data frames, we can combine these functions with the function \texttt{row.names}.
Some examples follow. 

\begin{Verbatim}
S <- cards() 
A <- subset(S, suit == "Heart") 
B <- subset(S, rank %in% 7:9)
\end{Verbatim}

We can now do some set algebra: 

\begin{Verbatim}
S[union(row.names(A), row.names(B)),]
S[sort(union(row.names(A), row.names(B))),]
S[sort(as.numeric(union(row.names(A), row.names(B)))),]
\end{Verbatim}

\begin{verbatim}
   rank    suit
6     7    Club
7     8    Club
8     9    Club
19    7 Diamond
20    8 Diamond
21    9 Diamond
27    2   Heart
28    3   Heart
29    4   Heart
30    5   Heart
31    6   Heart
32    7   Heart
33    8   Heart
34    9   Heart
35   10   Heart
36    J   Heart
37    Q   Heart
38    K   Heart
39    A   Heart
45    7   Spade
46    8   Spade
47    9   Spade
\end{verbatim}

\begin{Verbatim}
S[intersect(row.names(A), row.names(B)),]
\end{Verbatim}

\begin{verbatim}
:    rank  suit
: 32    7 Heart
: 33    8 Heart
: 34    9 Heart
\end{verbatim}

\begin{Verbatim}
S[setdiff(row.names(A),row.names(B)),]
\end{Verbatim}

\begin{verbatim}
   rank  suit
27    2 Heart
28    3 Heart
29    4 Heart
30    5 Heart
31    6 Heart
35   10 Heart
36    J Heart
37    Q Heart
38    K Heart
39    A Heart
\end{verbatim}

\begin{Verbatim}
S[setdiff(row.names(B),row.names(A)),]
\end{Verbatim}

\begin{verbatim}
   rank    suit
6     7    Club
7     8    Club
8     9    Club
19    7 Diamond
20    8 Diamond
21    9 Diamond
45    7   Spade
46    8   Spade
47    9   Spade
\end{verbatim}

Notice that \texttt{setdiff} is not symmetric. Further, note that we can
calculate the \emph{complement} of a set \(A\), denoted \(A^{c}\) and
defined to be the elements of \(S\) that are not in \(A\) simply with
the following code:
\begin{Verbatim}
S[setdiff(row.names(S),row.names(A)),]
\end{Verbatim}

\section{Properties of Probability}
\label{sec-4-4}

\subsection{Probability Functions}
\label{sec-4-4-1}

A \emph{probability function} is a rule that associates with each event
\(A\) of the sample space a single number \(\mathbb{P}(A)=p\), called
the \emph{probability of} \(A\). Any probability function \(\mathbb{P}\)
satisfies the following three Kolmogorov Axioms:

\begin{ax}
\label{ax-prob-nonnegative} \(\mathbb{P}(A)\geq0\) for any event \(A\subset S\).
\end{ax}

\begin{ax}
\label{ax-total-mass-one} \(\mathbb{P}(S)=1\).
\end{ax}

\begin{ax}
\label{ax-countable-additivity} If the events \(A_{1}\), \(A_{2}\),
\(A_{3}\)\ldots{} are disjoint then
\begin{equation}
\mathbb{P}\left(\bigcup_{i=1}^{n}A_{i}\right)=\sum_{i=1}^{n}\mathbb{P}(A_{i})\mbox{ for every }n,
\end{equation}
and furthermore,
\begin{equation}
\mathbb{P}\left(\bigcup_{i=1}^{\infty}A_{i}\right)=\sum_{i=1}^{\infty}\mathbb{P}(A_{i}).
\end{equation}
\end{ax}

The intuition behind the axioms goes like this: first, the probability
of an event should never be negative. Second, since the sample space
contains all possible outcomes, its probability should be one, or
100\%. The last axiom may look intimidating but it simply means that in
a sequence of disjoint events (in other words, sets that do not
overlap), the total probability (measure) should equal the sum of its
parts. For example, the chance of rolling a 1 or a 2 on a die should
be the chance of rolling a 1 plus the chance of rolling a 2.

\subsection{Properties}
\label{sec-4-4-2}

For any events \(A\) and \(B\),

\begin{enumerate}
\item \label{enu-prop-prob-complement} \(\mathbb{P}(A^{c})=1-\mathbb{P}(A)\).
\begin{proof}
Since \(A\cup A^{c}=S\) and \(A\cap A^{c}=\emptyset\), we have
\[
   1=\mathbb{P}(S)=\mathbb{P}(A\cup A^{c})=\mathbb{P}(A)+\mathbb{P}(A^{c}).
   \]
\end{proof}
\item \(\mathbb{P}(\emptyset)=0\).
\begin{proof}
Note that \(\emptyset=S^{c}\), and use Property 1.
\end{proof}
\item If \(A\subset B\) , then \(\mathbb{P}(A)\leq\mathbb{P}(B)\).
\begin{proof}
Write \(B=A\cup\left(B\cap A^{c}\right)\), and notice that \(A\cap\left(B\cap A^{c}\right)=\emptyset\); thus
\[
   \mathbb{P}(B)=\mathbb{P}(A\cup\left(B\cap A^{c}\right))=\mathbb{P}(A)+\mathbb{P}\left(B\cap A^{c}\right)\geq\mathbb{P}(A),
   \]
since \(\mathbb{P}\left(B\cap A^{c}\right)\ge0\). 
\end{proof}
\item \(0\leq\mathbb{P}(A)\leq1\).
\begin{proof}
The left inequality is immediate from Axiom \ref{ax-prob-nonnegative}, and the second inequality follows from Property 3 since \(A\subset S\).
\end{proof}

\item \textbf{The General Addition Rule.}
\begin{equation}
\label{eq-general-addition-rule-1}
\mathbb{P}(A\cup B)=\mathbb{P}(A)+\mathbb{P}(B)-\mathbb{P}(A\cap B).
\end{equation}
More generally, for events \(A_{1}\), \(A_{2}\), \(A_{3}\),\ldots{}, \(A_{n}\),
\begin{equation}
\mathbb{P}\left(\bigcup_{i=1}^{n}A_{i}\right)=\sum_{i=1}^{n}\mathbb{P}(A_{i})-\sum_{i=1}^{n-1}\sum_{j=i+1}^{n}\mathbb{P}(A_{i}\cap A_{j})+\cdots+(-1)^{n-1}\mathbb{P}\left(\bigcap_{i=1}^{n}A_{i}\right)
\end{equation}
\item \textbf{The Theorem of Total Probability.} Let \(B_{1}\), \(B_{2}\), \ldots{},
\(B_{n}\) be mutually exclusive and exhaustive. Then
\begin{equation}
\label{eq-theorem-total-probability}
\mathbb{P}(A)=\mathbb{P}(A\cap B_{1})+\mathbb{P}(A\cap B_{2})+\cdots+\mathbb{P}(A\cap B_{n}).
\end{equation}
\end{enumerate}

\subsection{Assigning Probabilities}
\label{sec-4-4-3}

A model of particular interest is the \emph{equally likely model}. The idea
is to divide the sample space \(S\) into a finite collection of
elementary events \( \{ a_{1},\ a_{2}, \ldots, a_{N} \} \) that are
equally likely in the sense that each \(a_{i}\) has equal chances of
occurring. The probability function associated with this model must
satisfy \(\mathbb{P}(S)=1\), by Axiom 2. On the other hand, it must
also satisfy \[ \mathbb{P}(S)=\mathbb{P}( \{ a_{1},\
a_{2},\ldots,a_{N} \} )=\mathbb{P}(a_{1}\cup a_{2}\cup\cdots\cup
a_{N})=\sum_{i=1}^{N}\mathbb{P}(a_{i}), \] by Axiom 3. Since
\(\mathbb{P}(a_{i})\) is the same for all \(i\), each one necessarily
equals \(1/N\).

For an event \(A\subset S\), we write \(A\) as a collection of
elementary outcomes: if \( A = \{ a_{i_{1}}, a_{i_{2}}, \ldots,
a_{i_{k}} \} \) then \(A\) has \(k\) elements and
\begin{align*}
\mathbb{P}(A) & =\mathbb{P}(a_{i_{1}})+\mathbb{P}(a_{i_{2}})+\cdots+\mathbb{P}(a_{i_{k}}),\\
 & =\frac{1}{N}+\frac{1}{N}+\cdots+\frac{1}{N},\\
 & =\frac{k}{N}=\frac{\#(A)}{\#(S)}.
\end{align*}
In other words, under the equally likely model, the probability of an
event \(A\) is determined by the number of elementary events that
\(A\) contains.


Consider the random experiment \(E\) of tossing a coin. Then the
sample space is \(S=\{H,T\}\), and under the equally likely model,
these two outcomes have \(\mathbb{P}(H)=\mathbb{P}(T)=1/2\). This
model is taken when it is reasonable to assume that the coin is fair.



Suppose the experiment \(E\) consists of tossing a fair coin
twice. The sample space may be represented by \(S=\{HH,\, HT,\, TH,\,
TT\}\). Given that the coin is fair and that the coin is tossed in an
independent and identical manner, it is reasonable to apply the
equally likely model.

What is \(\mathbb{P}(\mbox{at least 1 Head})\)? Looking at the sample
space we see the elements \(HH\), \(HT\), and \(TH\) have at least one
Head; thus, \(\mathbb{P}(\mbox{at least 1 Head})=3/4\).

What is \(\mathbb{P}(\mbox{no Heads})\)? Notice that the event \(\{
\mbox{no Heads} \} = \{ \mbox{at least one Head} \} ^{c}\), which by
Property \ref{enu-prop-prob-complement} means \(\mathbb{P}(\mbox{no
Heads})=1-\mathbb{P}(\mbox{at least one head})=1-3/4=1/4\). It is
obvious in this simple example that the only outcome with no Heads is
\(TT\), however, this complementation trick can be handy in more
complicated problems.


\label{exa-three-child-family} Imagine a three child family, each child
being either Boy (\(B\)) or Girl (\(G\)). An example sequence of
siblings would be \(BGB\). The sample space may be written \[ S =
\left\{ BBB,\ BGB,\ GBB,\ GGB,\ BBG,\ BGG,\ GBG,\ GGG,\ \right\}.\]

Note that for many reasons (for instance, it turns out that girls are
slightly more likely to be born than boys), this sample space is \emph{not}
equally likely. For the sake of argument, however, we will assume that
the elementary outcomes each have probability \(1/8\).

What is \(\mathbb{P}(\mbox{exactly 2 Boys})\)? Inspecting the sample
space reveals three outcomes with exactly two boys: \( \{ BBG,\,
BGB,\, GBB \} \).
Therefore \[ \mathbb{P}(\mbox{exactly 2 Boys}) = 3/8. \]

What is \(\mathbb{P}(\mbox{at most 2 Boys})\)? One way to solve the
problem would be to count the outcomes that have 2 or less Boys, but a
quicker way would be to recognize that the only way that the event
\(\{ \mbox{at most 2 Boys} \}\) does \emph{not} occur is the event \(\{
\mbox{all Boys} \}\).
Thus \[ \mathbb{P}(\mbox{at most 2 Boys}) = 1 - \mathbb{P}(BBB) = 1 -
1/8 = 7/8. \]




Consider the experiment of rolling a six-sided die, and let the
outcome be the face showing up when the die comes to rest. Then \( S =
\{ 1,\,2,\,3,\,4,\,5,\,6 \} \). It is usually reasonable to suppose
that the die is fair, so that the six outcomes are equally likely.



Consider a standard deck of 52 cards. These are usually labeled with
the four \emph{suits}: Clubs, Diamonds, Hearts, and Spades, and the 13
\emph{ranks}: 2, 3, 4, \ldots{}, 10, Jack (J), Queen (Q), King (K), and Ace
(A). Depending on the game played, the Ace may be ranked below 2 or
above King.

Let the random experiment \(E\) consist of drawing exactly one card
from a well-shuffled deck, and let the outcome be the face of the
card. Define the events \( A = \{ \mbox{draw an Ace} \} \) and \( B =
\{ \mbox{draw a Club} \} \). Bear in mind: we are only drawing one
card.

Immediately we have \(\mathbb{P}(A) = 4/52\) since there are four Aces
in the deck; similarly, there are \(13\) Clubs which implies
\(\mathbb{P}(B) = 13/52\).

What is \(\mathbb{P}(A\cap B)\)? We realize that there is only one
card of the 52 which is an Ace and a Club at the same time, namely,
the Ace of Clubs. Therefore \(\mathbb{P}(A\cap B)=1/52\).

To find \(\mathbb{P}(A\cup B)\) we may use the above with the General
Addition Rule to get
\begin{eqnarray*}
\mathbb{P}(A\cup B) & = & \mathbb{P}(A) + \mathbb{P}(B) - \mathbb{P}(A \cap B),\\
 & = & 4/52 + 13/52 - 1/52,\\
 & = & 16/52.
\end{eqnarray*}




Staying with the deck of cards, let another random experiment be the
selection of a five card stud poker hand, where "five card stud"
means that we draw exactly five cards from the deck without
replacement, no more, and no less. It turns out that the sample space
\(S\) is so large and complicated that we will be obliged to settle
for the trivial description \( S = \{ \mbox{all possible 5 card hands}
\} \) for the time being. We will have a more precise description
later.

What is \(\mathbb{P}(\mbox{Royal Flush})\), or in other words,
\(\mathbb{P}(\mbox{A, K, Q, J, 10 all in the same suit})\)?

It should be clear that there are only four possible royal
flushes. Thus, if we could only count the number of outcomes in \(S\)
then we could simply divide four by that number and we would have our
answer under the equally likely model. This is the subject of Section
\ref{sec-4-5}.

\subsubsection{How to do it with \(\mathsf{R}\)}
\label{sec-4-4-3-1}

Consider the experiment of drawing a card from a standard deck of
playing cards. Let's denote the probability space associated with the
experiment as \texttt{S}, and let the subsets \texttt{A} and \texttt{B} be defined by the
following:

\begin{Verbatim}
S <- cards(makespace = TRUE) 
A <- subset(S, suit == "Heart") 
B <- subset(S, rank %in% 7:9)
\end{Verbatim}

Now it is easy to calculate 

\begin{Verbatim}
nrow(A)/nrow(S)
nrow(B)/nrow(S)
\end{Verbatim}

\begin{verbatim}
: [1] 0.25
: [1] 0.2307692
\end{verbatim}

Note that we can get the same answer with 

\begin{Verbatim}
sum(A$probs)
sum(B$probs)
\end{Verbatim}

\section{Counting Methods}
\label{sec-4-5}

The equally-likely model is a convenient and popular way to analyze
random experiments. And when the equally likely model applies, finding
the probability of an event \(A\) amounts to nothing more than
counting the number of outcomes that \(A\) contains (together with the
number of events in \(S\)). Hence, to be a master of probability one
must be skilled at counting outcomes in events of all kinds.

\begin{prop}[\textbf{The Multiplication Principle}]
Suppose that an experiment is composed of two successive
steps. Further suppose that the first step may be performed in
\(n_{1}\) distinct ways while the second step may be performed in
\(n_{2}\) distinct ways. Then the experiment may be performed in
\(n_{1}n_{2}\) distinct ways.

More generally, if the experiment is composed of \(k\) successive
steps which may be performed in \(n_{1}\), \(n_{2}\), \ldots{}, \(n_{k}\)
distinct ways, respectively, then the experiment may be performed in
\(n_{1} n_{2} \cdots n_{k}\) distinct ways.
\end{prop}


We would like to order a pizza. It will be sure to have cheese (and
marinara sauce), but we may elect to add one or more of the following
five (5) available toppings: \[ \mbox{pepperoni, sausage, anchovies,
olives, and green peppers.}  \] How many distinct pizzas are possible?

There are many ways to approach the problem, but the quickest avenue
employs the Multiplication Principle directly. We will separate the
action of ordering the pizza into a series of stages. At the first
stage, we will decide whether or not to include pepperoni on the pizza
(two possibilities). At the next stage, we will decide whether or not
to include sausage on the pizza (again, two possibilities). We will
continue in this fashion until at last we will decide whether or not
to include green peppers on the pizza.

At each stage we will have had two options, or ways, to select a pizza
to be made. The Multiplication Principle says that we should multiply
the 2's to find the total number of possible pizzas: \(2 \cdot 2 \cdot
2 \cdot 2 \cdot 2 = 2^{5} = 32\).




We would like to buy a desktop computer to study statistics. We go to
a website to build our computer our way. Given a line of products we
have many options to customize our computer. In particular, there are
2 choices for a processor, 3 different operating systems, 4 levels of
memory, 4 hard drives of differing sizes, and 10 choices for a
monitor. How many possible types of computer must the company be
prepared to build? \textbf{Answer:} \(2 \cdot 3 \cdot 4 \cdot 4 \cdot 10 = 960\)

\subsection{Ordered Samples}
\label{sec-4-5-1}

Imagine a bag with \(n\) distinguishable balls inside. Now shake up
the bag and select \(k\) balls at random. How many possible sequences
might we observe?

\begin{prop}
The number of ways in which one may select an ordered sample of \(k\)
subjects from a population that has \(n\) distinguishable members is
\begin{itemize}
\item \(n^{k}\) if sampling is done with replacement,
\item \(n(n-1)(n-2)\cdots(n-k+1)\) if sampling is done without
replacement.
\end{itemize}
\end{prop}

Recall from calculus the notation for \emph{factorials}: 
\begin{eqnarray*}
1! & = & 1,\\
2! & = & 2 \cdot 1 = 2,\\
3! & = & 3 \cdot 2 \cdot 1 = 6,\\
 & \vdots\\
n! & = & n(n - 1)(n - 2) \cdots 3 \cdot 2 \cdot 1.
\end{eqnarray*}

\begin{fact}
The number of permutations of \(n\) elements is \(n!\).
\end{fact}


Take a coin and flip it 7 times. How many sequences of Heads and Tails
are possible? \textbf{Answer:} \(2^{7}=128\).



In a class of 20 students, we randomly select a class president, a
class vice-president, and a treasurer. How many ways can this be
done? \textbf{Answer:} \(20\cdot19\cdot18=6840\).



We rent five movies to watch over the span of two nights. We wish to
watch 3 movies on the first night. How many distinct sequences of 3
movies could we possibly watch? \textbf{Answer:} \(5\cdot4\cdot3=60\).

\subsection{Unordered Samples}
\label{sec-4-5-2}

\begin{prop}
The number of ways in which one may select an unordered sample of
\(k\) subjects from a population that has \(n\) distinguishable
members is
\begin{itemize}
\item \((n-1+k)!/[(n-1)!k!]\) if sampling is done with replacement,
\item \(n!/[k!(n-k)!]\) if sampling is done without replacement.
\end{itemize}
\end{prop}

The quantity \(n!/[k!(n-k)!]\) is called a \emph{binomial coefficient} and
plays a special role in mathematics; it is denoted  
\begin{equation}
\label{eq-binomial-coefficient}
{n \choose k}=\frac{n!}{k!(n-k)!}
\end{equation}
and is read "\(n\) choose \(k\)".


You rent five movies to watch over the span of two nights, but only
wish to watch 3 movies the first night. Your friend, Fred, wishes to
borrow some movies to watch at his house on the first night. You owe
Fred a favor, and allow him to select 2 movies from the set of 5. How
many choices does Fred have? \textbf{Answer:} \({5 \choose 2}=10\).



Place 3 six-sided dice into a cup. Next, shake the cup well and pour
out the dice. How many distinct rolls are possible? \textbf{Answer:}
\((6-1+3)!/[(6-1)!3!]={8 \choose 5}=56\).

\subsubsection{How to do it with \(\mathsf{R}\)}
\label{sec-4-5-2-1}

The factorial \(n!\) is computed with the command \texttt{factorial(n)} and
the binomial coefficient \({n \choose k}\) with the command
\texttt{choose(n,k)}.

The sample spaces we have computed so far have been relatively small,
and we can visually study them without much trouble. However, it is
\emph{very} easy to generate sample spaces that are prohibitively
large. And while \(\mathsf{R}\) is wonderful and powerful and does
almost everything except wash windows, even \(\mathsf{R}\) has limits
of which we should be mindful.

But we often do not need to actually generate the sample space; it
suffices to count the number of outcomes. The \texttt{nsamp} function will
calculate the number of rows in a sample space made by \texttt{urnsamples}
without actually devoting the memory resources necessary to generate
the space. The arguments are \texttt{n}, the number of (distinguishable)
objects in the urn, \texttt{k}, the sample size, and \texttt{replace}, \texttt{ordered}, as
above.

\begin{table}[htb]
\caption[Sampling \(k\) from \(n\) objects with \texttt{urnsamples}]{\label{tab-Sampling-k-from-n}Sampling \(k\) from \(n\) objects with \texttt{urnsamples}.}
\centering
\begin{tabular}{lll}
 & \texttt{ordered = TRUE} & \texttt{ordered = FALSE}\\
\hline
\texttt{replace = TRUE} & \(n^{k}\) & \((n-1+k)! / [(n-1)!k!]\)\\
\texttt{replace = FALSE} & \( n! / (n-k)! \) & \( {n \choose k} \)\\
\hline
\end{tabular}
\end{table}

\begin{Verbatim}
nsamp <- function (n, k, replace = FALSE, ordered = FALSE){
  if (ordered) {
    if (replace) return(n^k)
    return(factorial(n)/factorial(n - k))
  }
  if (replace) return(choose(n - 1 + k, k))
  return(choose(n, k))
}
nsamp <- Vectorize(nsamp)
\end{Verbatim}


We will compute the number of outcomes for each of the four
\texttt{urnsamples} examples that we saw in Example
\ref{exa-sample-urn-two-from-three}. Recall that we took a sample of size two from an
urn with three distinguishable elements.


\begin{Verbatim}
nsamp(n=3, k=2, replace = TRUE, ordered = TRUE) 
nsamp(n=3, k=2, replace = FALSE, ordered = TRUE) 
nsamp(n=3, k=2, replace = FALSE, ordered = FALSE) 
nsamp(n=3, k=2, replace = TRUE, ordered = FALSE) 
\end{Verbatim}

\begin{verbatim}
: [1] 9
: [1] 6
: [1] 3
: [1] 6
\end{verbatim}

Compare these answers with the length of the data frames generated above.

\subsubsection{The Multiplication Principle}
\label{sec-4-5-2-2}

A benefit of \texttt{nsamp} is that it is \emph{vectorized} so that entering
vectors instead of numbers for \texttt{n}, \texttt{k}, \texttt{replace}, and \texttt{ordered}
results in a vector of corresponding answers. This becomes
particularly convenient for combinatorics problems.


There are 11 artists who each submit a portfolio containing 7
paintings for competition in an art exhibition. Unfortunately, the
gallery director only has space in the winners' section to accommodate
12 paintings in a row equally spread over three consecutive walls. The
director decides to give the first, second, and third place winners
each a wall to display the work of their choice. The walls boast 31
separate lighting options apiece. How many displays are possible?

\textbf{Answer:} The judges will pick 3 (ranked) winners out of 11 (with \texttt{rep
= FALSE}, \texttt{ord = TRUE}). Each artist will select 4 of his/her
paintings from 7 for display in a row (\texttt{rep = FALSE}, \texttt{ord = TRUE}),
and lastly, each of the 3 walls has 31 lighting possibilities (\texttt{rep =
TRUE}, \texttt{ord = TRUE}). These three numbers can be calculated quickly
with

\begin{Verbatim}
n <- c(11,7,31) 
k <- c(3,4,3) 
r <- c(FALSE,FALSE,TRUE) 
x <- nsamp(n, k, rep = r, ord = TRUE) 
\end{Verbatim}

(Notice that \texttt{ordered} is always \texttt{TRUE}; \texttt{nsamp} will recycle
\texttt{ordered} and \texttt{replace} to the appropriate length.) By the
Multiplication Principle, the number of ways to complete the
experiment is the product of the entries of \texttt{x}:

\begin{Verbatim}
prod(x) 
\end{Verbatim}

\begin{verbatim}
: [1] 24774195600
\end{verbatim}

Compare this with the some other ways to compute the same thing: 

\begin{Verbatim}
(11*10*9)*(7*6*5*4)*31^3 
\end{Verbatim}

\begin{verbatim}
: [1] 24774195600
\end{verbatim}

or alternatively 

\begin{Verbatim}
prod(9:11)*prod(4:7)*31^3 
\end{Verbatim}

\begin{verbatim}
: [1] 24774195600
\end{verbatim}

or even 

\begin{Verbatim}
prod(factorial(c(11,7))/factorial(c(8,3)))*31^3 
\end{Verbatim}

\begin{verbatim}
: [1] 24774195600
\end{verbatim}



As one can guess, in many of the standard counting problems there
aren't substantial savings in the amount of typing; it is about the
same using \texttt{nsamp} versus \texttt{factorial} and \texttt{choose}. But the virtue of
\texttt{nsamp} lies in its collecting the relevant counting formulas in a
one-stop shop. Ultimately, it is up to the user to choose the method
that works best for him/herself.


\textbf{The Birthday Problem.} Suppose that there are \(n\) people together
in a room. Each person announces the date of his/her birthday in
turn. The question is: what is the probability of at least one match?
If we let the event \(A\) represent \[ A = \{ \mbox{there is at least
one match}\}, \] then we are looking for \(\mathbb{P}(A)\), but as we
soon will see, it will be more convenient for us to calculate
\(\mathbb{P}(A^{c})\).

For starters we will ignore leap years and assume that there are only
365 days in a year. Second, we will assume that births are equally
distributed over the course of a year (which is not true due to all
sorts of complications such as hospital delivery schedules). See \href{http://en.wikipedia.org/wiki/Birthday_problem}{here}
for more.

Let us next think about the sample space. There are 365 possibilities
for the first person's birthday, 365 possibilities for the second, and
so forth. The total number of possible birthday sequences is therefore
\(\#(S)=365^{n}\).

Now we will use the complementation trick we saw in Example \ref{exa-three-child-family}. We realize that the only situation in which \(A\) does
\emph{not} occur is if there are \emph{no} matches among all people in the room,
that is, only when everybody's birthday is different, so \[
\mathbb{P}(A)=1-\mathbb{P}(A^{c})=1-\frac{\#(A^{c})}{\#(S)}, \] since
the outcomes are equally likely. Let us then suppose that there are no
matches. The first person has one of 365 possible birthdays. The
second person must not match the first, thus, the second person has
only 364 available birthdays from which to choose. Similarly, the
third person has only 363 possible birthdays, and so forth, until we
reach the \(n^{\mathrm{th}}\) person, who has only \(365-n+1\)
remaining possible days for a birthday. By the Multiplication
Principle, we have \(\#(A^{c})=365\cdot364\cdots(365-n+1)\), and
\begin{equation}
\mathbb{P}(A)=1-\frac{365\cdot364\cdots(365-n+1)}{365^{n}}=1-\frac{364}{365}\cdot\frac{363}{365}\cdots\frac{(365-n+1)}{365}.
\end{equation}
As a surprising consequence, consider this: how many people does it
take to be in the room so that the probability of at least one match
is at least 0.50? Clearly, if there is only \(n=1\) person in the room
then the probability of a match is zero, and when there are \(n=366\)
people in the room there is a 100\% chance of a match (recall that we
are ignoring leap years). So how many people does it take so that
there is an equal chance of a match and no match?

When I have asked this question to students, the usual response is
"somewhere around \(n=180\) people" in the room. The reasoning seems
to be that in order to get a 50\% chance of a match, there should be
50\% of the available days to be occupied. The number of students in a
typical classroom is 25, so as a companion question I ask students to
estimate the probability of a match when there are \(n=25\) students
in the room. Common estimates are a 1\%, or 0.5\%, or even 0.1\% chance
of a match. After they have given their estimates, we go around the
room and each student announces their birthday. More often than not,
we observe a match in the class, to the students' disbelief.

Students are usually surprised to hear that, using the formula above,
one needs only \(n=23\) students to have a greater than 50\% chance of
at least one match. Figure \ref{fig-birthday} shows a graph of the birthday
probabilities:

\begin{figure}[ht!]
\centering
\includegraphics[width=0.9\textwidth]{fig/prob-birthday.ps}
\caption[The birthday problem]{\label{fig-birthday}\small The birthday problem. The horizontal line is at \(p=0.50\) and the vertical line is at \(n=23\).}
\end{figure}

\subsubsection{How to do it with \(\mathsf{R}\)}
\label{sec-4-5-2-3}

We can make the plot in Figure \ref{fig-birthday} with the following
sequence of commands.

\begin{Verbatim}
g <- Vectorize(pbirthday)
plot(1:50, g(1:50), xlab = "Number of people in room", 
  ylab = "Prob(at least one match)" )
abline(h = 0.5)
abline(v = qbirthday(0.5))
\end{Verbatim}

One can use \texttt{pbirthday} and \texttt{qbirthday} to
compute approximate probabilities for the more general case of
probabilities other than 1/2, for differing total number of days in
the year, and even for more than two matches.

\section{Conditional Probability}
\label{sec-4-6}

Consider a full deck of 52 standard playing cards. Now select two
cards from the deck, in succession. Let \( A = \{ \mbox{first card
drawn is an Ace} \} \) and \( B = \{ \mbox{second card drawn is an
Ace} \} \). Since there are four Aces in the deck, it is natural to
assign \( \mathbb{P}(A) = 4/52 \). Suppose we look at the first
card. What now is the probability of \(B\)? Of course, the answer
depends on the value of the first card. If the first card is an Ace,
then the probability that the second also is an Ace should be \( 3/51
\), but if the first card is not an Ace, then the probability that the
second is an Ace should be \( 4/51 \). As notation for these two
situations we write 
\[ \mathbb{P}(B|A)=3/51,\quad
\mathbb{P}(B|A^{c})=4/51.  
\]

\begin{defn}
The conditional probability of \(B\) given \(A\), denoted
\(\mathbb{P}(B|A)\), is defined by
\begin{equation}
\mathbb{P}(B|A)=\frac{\mathbb{P}(A\cap B)}{\mathbb{P}(A)},\quad \mbox{if }\mathbb{P}(A)>0.
\end{equation}
\end{defn}


Toss a coin twice. The sample space is given by \(S=\{ HH,\ HT,\ TH,\
TT \} \). Let \(A= \{ \mbox{a head occurs} \} \) and \(B= \{ \mbox{a
head and tail occur} \} \). It should be clear that
\(\mathbb{P}(A)=3/4\), \(\mathbb{P}(B)=2/4\), and \(\mathbb{P}(A\cap
B)=2/4\). What now are the probabilities \(\mathbb{P}(A|B)\) and
\(\mathbb{P}(B|A)\)?  

\[ \mathbb{P}(A|B)=\frac{\mathbb{P}(A\cap
B)}{\mathbb{P}(B)}=\frac{2/4}{2/4}=1, \] 

in other words, once we know that a Head and Tail occur, we may be
certain that a Head occurs. Next

\[ 
\mathbb{P}(B|A)=\frac{\mathbb{P}(A\cap
B)}{\mathbb{P}(A)}=\frac{2/4}{3/4}=\frac{2}{3}, 
\] 

which means that given the information that a Head has occurred, we no
longer need to account for the outcome \(TT\), and the remaining three
outcomes are equally likely with exactly two outcomes lying in the set
\(B\).



\label{exa-Toss-a-six-sided-die-twice} Toss a six-sided die twice. The
sample space consists of all ordered pairs \((i,j)\) of the numbers
\(1,2,\ldots,6\), that is, \( S = \{ (1,1),\ (1,2),\ldots,(6,6) \}
\). We know from Section \ref{sec-4-5} that \( \# (S) =
6^{2} = 36 \). Let \( A = \{ \mbox{outcomes match} \} \) and \( B = \{
\mbox{sum of outcomes at least 8} \} \). The sample space may be
represented by a matrix:

\begin{Verbatim}
S <- rolldie(2)
A <- subset(S, X1==X2)
B <- subset(S, X1+X2 >= 8)
A$pch <- "X"
B$pch <- "O"
with(rbind(A, B),
    plot(X1, X2, pch=pch, xlab="First roll", ylab="Second roll")
)
\end{Verbatim}

\begin{figure}[ht!]
\centering
\includegraphics[width=0.9\textwidth]{fig/prob-twodiceAB.ps}
\caption[Rolling two dice]{\label{fig-twodiceAB}\small Rolling two dice. The outcomes in A are marked with X, the outcomes in B are marked with O.}
\end{figure}

The outcomes lying in the event \(A\) are marked with the symbol
"X", the outcomes falling in \(B\) are marked with "O", and the
outcomes in \(A\cap B\) are those where the letters overlap. Now it is
clear that \(\mathbb{P}(A)=6/36\), \(\mathbb{P}(B)=15/36\), and
\(\mathbb{P}(A\cap B)=3/36\).  Finally, 

\[
\mathbb{P}(A|B)=\frac{3/36}{15/36}=\frac{1}{5},\quad
\mathbb{P}(B|A)=\frac{3/36}{6/36}=\frac{1}{2}.  
\] 

Again, we see that given the knowledge that \(B\) occurred (the 15
outcomes in the upper right triangle), there are 3 of the 15 that fall
into the set \(A\), thus the probability is \(3/15\). Similarly, given
that \(A\) occurred (we are on the diagonal), there are 3 out of 6
outcomes that also fall in \(B\), thus, the probability of \(B\) given
\(A\) is 1/2.

\subsection{How to do it with \(\mathsf{R}\)}
\label{sec-4-6-1}

Continuing with Example \ref{exa-Toss-a-six-sided-die-twice}, the first thing to do is set
up the probability space with the \texttt{rolldie} function.

\begin{Verbatim}
S <- rolldie(2, makespace = TRUE)  # assumes ELM
head(S)                            #  first few rows
\end{Verbatim}

\begin{verbatim}
:   X1 X2      probs
: 1  1  1 0.02777778
: 2  2  1 0.02777778
: 3  3  1 0.02777778
: 4  4  1 0.02777778
: 5  5  1 0.02777778
: 6  6  1 0.02777778
\end{verbatim}

Next we define the events

\begin{Verbatim}
A <- subset(S, X1 == X2)
B <- subset(S, X1 + X2 >= 8)
AB <- S[intersect(row.names(A), row.names(B)), ]
\end{Verbatim}

And now we are ready to calculate probabilities:

\begin{Verbatim}
sum(AB$probs)/sum(B$probs); nrow(AB)/nrow(B)
sum(AB$probs)/sum(A$probs); nrow(AB)/nrow(A)
\end{Verbatim}

\begin{verbatim}
: [1] 0.2
: [1] 0.5
\end{verbatim}

\subsection{Properties and Rules}
\label{sec-4-6-2}

The following theorem establishes that conditional probabilities
behave just like regular probabilities when the conditioned event is
fixed.

\begin{thm}
For any fixed event \(A\) with \(\mathbb{P}(A)>0\),
\begin{enumerate}
\item \( \mathbb{P} (B|A)\geq 0 \), for all events \( B \subset S\),
\item \( \mathbb{P} (S|A) = 1 \), and
\item If \(B_{1}\), \(B_{2}\), \(B_{3}\),\ldots{} are disjoint events, then
\begin{equation}
\mathbb{P}\left(\left.\bigcup_{k=1}^{\infty}B_{k}\:\right|A\right)=\sum_{k=1}^{\infty}\mathbb{P}(B_{k}|A).
\end{equation}
\end{enumerate}
\end{thm}
In other words, \(\mathbb{P}(\cdot|A)\) is a legitimate probability
function. With this fact in mind, the following properties are
immediate:

\begin{prop}
For any events \(A\), \(B\), and \(C\) with \(\mathbb{P}(A)>0\),
\begin{enumerate}
\item \( \mathbb{P} ( B^{c} | A ) = 1 - \mathbb{P} (B|A).\)
\item If \(B\subset C\) then \(\mathbb{P}(B|A)\leq\mathbb{P}(C|A)\).
\item \( \mathbb{P} [ ( B\cup C ) | A ] = \mathbb{P} (B|A) +
   \mathbb{P}(C|A) - \mathbb{P} [ (B \cap C|A) ].\)
\item \textbf{The Multiplication Rule.} For any two events \(A\) and \(B\),
\begin{equation}
\label{eq-multiplication-rule-short}
\mathbb{P}(A\cap B)=\mathbb{P}(A)\mathbb{P}(B|A).
\end{equation}
And more generally, for events \(A_{1}\), \(A_{2}\), \(A_{3}\),\ldots{},
\(A_{n}\),
\begin{equation}
\label{eq-multiplication-rule-long}
\mathbb{P}(A_{1}\cap A_{2}\cap\cdots\cap A_{n})=\mathbb{P}(A_{1})\mathbb{P}(A_{2}|A_{1})\cdots\mathbb{P}(A_{n}|A_{1}\cap A_{2}\cap\cdots\cap A_{n-1}).
\end{equation}
\end{enumerate}
\end{prop}
The Multiplication Rule is very important because it allows us to find
probabilities in random experiments that have a sequential structure,
as the next example shows.

\label{exa-two-cards-both-aces} At the beginning of the section we drew
two cards from a standard playing deck. Now we may answer our original
question, what is \(\mathbb{P}(\mbox{both Aces})\)?  \[
\mathbb{P}(\mbox{both Aces})=\mathbb{P}(A\cap
B)=\mathbb{P}(A)\mathbb{P}(B|A)=\frac{4}{52}\cdot\frac{3}{51}\approx0.00452.
\]

\label{exa-urn-7-red-3-green} Consider an urn with 10 balls inside, 7 of
which are red and 3 of which are green. Select 3 balls successively
from the urn. Let \( A = \{ 1^{\mathrm{st}} \mbox{ ball is red} \} \),
\( B = \{ 2^{\mathrm{nd}} \mbox{ ball is red} \} \), and \( C = \{
3^{\mathrm{rd}} \mbox{ ball is red} \} \). Then \[
\mathbb{P}(\mbox{all 3 balls are red})=\mathbb{P}(A\cap B\cap
C)=\frac{7}{10}\cdot\frac{6}{9}\cdot\frac{5}{8}\approx 0.2917.  \]

Consider two urns, the first with 5 red balls and 3 green balls, and
the second with 2 red balls and 6 green balls. Your friend randomly
selects one ball from the first urn and transfers it to the second
urn, without disclosing the color of the ball. You select one ball
from the second urn. What is the probability that the selected ball is
red? Let \( A = \{ \mbox{transferred ball is red} \} \) and \( B = \{
\mbox{selected ball is red} \} \). Write
\begin{align*}
B & =S\cap B\\
 & =(A\cup A^{c})\cap B\\
 & =(A\cap B)\cup(A^{c}\cap B)
\end{align*}
and notice that \(A\cap B\) and \(A^{c}\cap B\) are disjoint. Therefore
\begin{align*}
\mathbb{P}(B) & =\mathbb{P}(A\cap B)+\mathbb{P}(A^{c}\cap B)\\
 & =\mathbb{P}(A)\mathbb{P}(B|A)+\mathbb{P}(A^{c})\mathbb{P}(B|A^{c})\\
 & =\frac{5}{8}\cdot\frac{3}{9}+\frac{3}{8}\cdot\frac{2}{9}\\
 & =\frac{21}{72}\ 
\end{align*}
(which is 7/24 in lowest terms).


\subsubsection{Example: smoking v.s. gender}

We have a two-way table of the smoking status versus the gender:

\begin{verbatim}
:            gender
: smoking     Female Male Sum
:   Nonsmoker     61   75 136
:   Smoker         9   23  32
:   Sum           70   98 168
\end{verbatim}

If one person were selected at random from the data set, then we see
from the two-way table that \(\mathbb{P}(\mbox{Female})=70/168\) and
\(\mathbb{P}(\mbox{Smoker})=32/168\). Now suppose that one of the
subjects quits smoking, but we do not know the person's gender. If we
now select one nonsmoker at random, what would be
\(\mathbb{P}(\mbox{Female})\)? This example is just like the last
example, but with different labels. Let \( A = \{ \mbox{the quitter is
a female} \} \) and \( B = \{ \mbox{selected nonsmoker is a female} \}
\). Write

\begin{align*}
B & =S\cap B\\
 & =(A\cup A^{c})\cap B\\
 & =(A\cap B)\cup(A^{c}\cap B)
\end{align*}
and notice that \(A\cap B\) and \(A^{c}\cap B\) are disjoint. Therefore
\begin{align*}
\mathbb{P}(B) & =\mathbb{P}(A\cap B)+\mathbb{P}(A^{c}\cap B),\\
 & =\mathbb{P}(A)\mathbb{P}(B|A)+\mathbb{P}(A^{c})\mathbb{P}(B|A^{c}),\\
 & =\frac{9}{32}\cdot\frac{62}{137}+\frac{23}{32}\cdot\frac{76}{137},\\
 & =\frac{2306}{4384},
\end{align*}
(which is 1153/2192 in lowest terms).


Using the same reasoning, we can return to the example from the
beginning of the section and show that 
\[ \mathbb{P}(\{ \mbox{second
card is an Ace} \} )=4/52.  
\]

\section{Independent Events}
\label{sec-4-7}

Toss a coin twice. The sample space is \(S= \{ HH,\ HT,\ TH,\ TT \}
\). We know that \(\mathbb{P}(1^{\mathrm{st}}\mbox{ toss is }H)=2/4\),
\(\mathbb{P}(2^{\mathrm{nd}}\mbox{ toss is }H)=2/4\), and
\(\mathbb{P}(\mbox{both }H)=1/4\). Then

\begin{align*} 
\mathbb{P}(2^{\mathrm{nd}}\mbox{ toss is }H\ \vert \ 1^{\mathrm{st}}\mbox{ toss is }H) & =\frac{\mathbb{P}(\mbox{both }H)}{\mathbb{P}(1^{\mathrm{st}}\mbox{ toss is }H)}, \\
 & =\frac{1/4}{2/4},\\
 & =\mathbb{P}(2^{\mathrm{nd}}\mbox{ toss is }H).
\end{align*}

Intuitively, this means that the information that the first toss is
\(H\) has no bearing on the probability that the second toss is
\(H\). The coin does not remember the result of the first toss.

\begin{defn}
Events \(A\) and \(B\) are said to be \emph{independent} if 
\begin{equation}
\mathbb{P}(A\cap B)=\mathbb{P}(A)\mathbb{P}(B).
\end{equation}
Otherwise, the events are said to be \emph{dependent}. 
\end{defn}

The connection with the above example stems from the following. We
know from Section \ref{sec-4-6} that when
\(\mathbb{P}(B)>0\) we may write
\begin{equation}
\mathbb{P}(A|B)=\frac{\mathbb{P}(A\cap B)}{\mathbb{P}(B)}.
\end{equation}

In the case that \(A\) and \(B\) are independent, the numerator of the
fraction factors so that \(\mathbb{P}(B)\) cancels with the result:
\begin{equation}
\mathbb{P}(A|B)=\mathbb{P}(A)\mbox{ when \(A\), \(B\) are independent.}
\end{equation}

The interpretation in the case of independence is that the information
that the event \(B\) occurred does not influence the probability of
the event \(A\) occurring. Similarly,
\(\mathbb{P}(B|A)=\mathbb{P}(B)\), and so the occurrence of the event
\(A\) likewise does not affect the probability of event \(B\). It may
seem more natural to define \(A\) and \(B\) to be independent when
\(\mathbb{P}(A|B)=\mathbb{P}(A)\); however, the conditional
probability \(\mathbb{P}(A|B)\) is only defined when
\(\mathbb{P}(B)>0\). Our definition is not limited by this
restriction. It can be shown that when \(\mathbb{P}(A),\
\mathbb{P}(B)>0\) the two notions of independence are equivalent.

\begin{prop}
If the events \(A\) and \(B\) are independent then
\begin{itemize}
\item \(A\) and \(B^{c}\) are independent,
\item \(A^{c}\) and \(B\) are independent,
\item \(A^{c}\) and \(B^{c}\) are independent.
\end{itemize}
\end{prop}

\begin{proof}
Suppose that \(A\) and \(B\) are independent. We will show the second
one; the others are similar. We need to show that \[
\mathbb{P}(A^{c}\cap B)=\mathbb{P}(A^{c})\mathbb{P}(B).  \] To this
end, note that the Multiplication Rule, Equation
\eqref{eq-multiplication-rule-short} implies
\begin{eqnarray*}
\mathbb{P}(A^{c}\cap B) & = & \mathbb{P}(B)\mathbb{P}(A^{c}|B),\\
 & = & \mathbb{P}(B)[1-\mathbb{P}(A|B)],\\
 & = & \mathbb{P}(B)\mathbb{P}(A^{c}).
\end{eqnarray*}
\end{proof}

\begin{defn}
The events \(A\), \(B\), and \(C\) are \emph{mutually independent} if the
following four conditions are met:
\begin{eqnarray*}
\mathbb{P}(A\cap B) & = & \mathbb{P}(A)\mathbb{P}(B),\\
\mathbb{P}(A\cap C) & = & \mathbb{P}(A)\mathbb{P}(C),\\
\mathbb{P}(B\cap C) & = & \mathbb{P}(B)\mathbb{P}(C),
\end{eqnarray*}
and
\[
\mathbb{P}(A\cap B\cap C)=\mathbb{P}(A)\mathbb{P}(B)\mathbb{P}(C).
\]
If only the first three conditions hold then \(A\), \(B\), and \(C\)
are said to be independent \emph{pairwise}. Note that pairwise independence
is not the same as mutual independence when the number of events is
larger than two.
\end{defn}

We can now deduce the pattern for \(n\) events, \(n>3\). The events
will be mutually independent only if they satisfy the product equality
pairwise, then in groups of three, in groups of four, and so forth, up
to all \(n\) events at once. For \(n\) events, there will be
\(2^{n}-n-1\) equations that must be satisfied.
Although these requirements for a set of events to
be mutually independent may seem stringent, the good news is that for
most of the situations considered in this book the conditions will all
be met (or at least we will suppose that they are).

\label{exa-toss-ten-coins} Toss ten coins. What is the probability of
observing at least one Head? Answer: Let \(A_{i}= \{ \mbox{the
}i^{\mathrm{th}}\mbox{ coin shows }H \} ,\
i=1,2,\ldots,10\). Supposing that we toss the coins in such a way that
they do not interfere with each other, this is one of the situations
where all of the \(A_{i}\) may be considered mutually independent due
to the nature of the tossing. Of course, the only way that there will
not be at least one Head showing is if all tosses are
Tails. Therefore,
\begin{align*}
\mathbb{P}(\mbox{at least one }H) & =1-\mathbb{P}(\mbox{all }T),\\
 & =1-\mathbb{P}(A_{1}^{c}\cap A_{2}^{c}\cap\cdots\cap A_{10}^{c}),\\
 & =1-\mathbb{P}(A_{1}^{c})\mathbb{P}(A_{2}^{c})\cdots\mathbb{P}(A_{10}^{c}),\\
 & =1-\left(\frac{1}{2}\right)^{10},
\end{align*}
which is approximately \(0.9990234\).

\subsection{How to do it with \(\mathsf{R}\)}
\label{sec-4-7-1}


Toss ten coins. What is the probability of observing at least one
Head?

\begin{Verbatim}
S <- tosscoin(10, makespace = TRUE)
A <- subset(S, rowSums(S=="T")==10)
1-sum(A$probs)
\end{Verbatim}

\begin{verbatim}
: [1] 0.9990234
\end{verbatim}

\section{Bayes' Rule}
\label{sec-4-8}

In this section we introduce a rule that
allows us to update our probabilities when new information becomes
available.

\begin{thm}[\textbf{Bayes' Rule}]
Let \(B_{1}\), \(B_{2}\), \ldots{}, \(B_{n}\) be mutually exclusive and
exhaustive and let \(A\) be an event with \(\mathbb{P}(A)>0\). Then
\begin{equation}
\label{eq-bayes-rule}
\mathbb{P}(B_{k}|A)=\frac{\mathbb{P}(B_{k})\mathbb{P}(A|B_{k})}{\sum_{i=1}^{n}\mathbb{P}(B_{i})\mathbb{P}(A|B_{i})},\quad k=1,2,\ldots,n.
\end{equation}
\end{thm}

\begin{proof}
The proof follows from looking at \(\mathbb{P}(B_{k}\cap A)\) in two
different ways. For simplicity, suppose that \(P(B_{k})>0\) for all
\(k\). Then \[ \mathbb{P}(A)\mathbb{P}(B_{k}|A)=\mathbb{P}(B_{k}\cap
A)=\mathbb{P}(B_{k})\mathbb{P}(A|B_{k}).  \] Since \(\mathbb{P}(A)>0\)
we may divide through to obtain \[
\mathbb{P}(B_{k}|A)=\frac{\mathbb{P}(B_{k})\mathbb{P}(A|B_{k})}{\mathbb{P}(A)}.
\] Now remembering that \(\{ B_{k} \}\) is a partition, the Theorem of
Total Probability (Equation \eqref{eq-theorem-total-probability} )
gives the denominator of the last expression to be \[
\mathbb{P}(A)=\sum_{k=1}^{n}\mathbb{P}(B_{k}\cap
A)=\sum_{k=1}^{n}\mathbb{P}(B_{k})\mathbb{P}(A|B_{k}).  \]
\end{proof}

What does it mean? Usually in applications we are given (or know) \emph{a
priori} probabilities \(\mathbb{P}(B_{k})\). We go out and collect
some data, which we represent by the event \(A\). We want to know: how
do we \textbf{update} \(\mathbb{P}(B_{k})\) to \(\mathbb{P}(B_{k}|A)\)? The
answer: Bayes' Rule.

\label{exa-misfiling-assistants} \textbf{Misfiling Assistants.} In this problem,
there are three assistants working at a company: Moe, Larry, and
Curly. Their primary job duty is to file paperwork in the filing
cabinet when papers become available. The three assistants have
different work schedules:

\begin{table}[htb]
\caption[Misfiling assistants: workload]{Misfiling assistants: workload.}
\centering
\begin{tabular}{llll}
 & Moe & Larry & Curly\\
\hline
Workload & \(60\%\) & \(30\%\) & \(10\%\)\\
\end{tabular}
\end{table}

That is, Moe works 60\% of the time, Larry works 30\% of the time, and
Curly does the remaining 10\%, and they file documents at approximately
the same speed. Suppose a person were to select one of the documents
from the cabinet at random. Let \(M\) be the event \[ M= \{ \mbox{Moe
filed the document} \} \] and let \(L\) and \(C\) be the events that
Larry and Curly, respectively, filed the document. What are these
events' respective probabilities? In the absence of additional
information, reasonable prior probabilities would just be

\begin{table}[htb]
\caption[Misfiling assistants: prior]{Misfiling assistants: prior.}
\centering
\begin{tabular}{lrrr}
 & Moe & Larry & Curly\\
\hline
Prior Probability & 0.6 & 0.3 & 0.1\\
\end{tabular}
\end{table}

Now, the boss comes in one day, opens up the file cabinet, and selects
a file at random. The boss discovers that the file has been
misplaced. The boss is so angry at the mistake that (s)he threatens to
fire the one who erred. The question is: who misplaced the file?

The boss decides to use probability to decide, and walks straight to
the workload schedule. (S)he reasons that, since the three employees
work at the same speed, the probability that a randomly selected file
would have been filed by each one would be proportional to his
workload. The boss notifies \textbf{Moe} that he has until the end of the day
to empty his desk.

But Moe argues in his defense that the boss has ignored additional
information. Moe's likelihood of having misfiled a document is smaller
than Larry's and Curly's, since he is a diligent worker who pays close
attention to his work. Moe admits that he works longer than the
others, but he doesn't make as many mistakes as they do. Thus, Moe
recommends that -- before making a decision -- the boss should update
the probability (initially based on workload alone) to incorporate the
likelihood of having observed a misfiled document.

And, as it turns out, the boss has information about Moe, Larry, and
Curly's filing accuracy in the past (due to historical performance
evaluations). The performance information may be represented by the
following table:

\begin{table}[htb]
\caption[Misfiling assistants: misfile rate]{Misfiling assistants: misfile rate.}
\centering
\begin{tabular}{lrrr}
 & Moe & Larry & Curly\\
\hline
Misfile Rate & 0.003 & 0.007 & 0.010\\
\end{tabular}
\end{table}

In other words, on the average, Moe misfiles 0.3\% of the documents he
is supposed to file. Notice that Moe was correct: he is the most
accurate filer, followed by Larry, and lastly Curly. If the boss were
to make a decision based only on the worker's overall accuracy,
then \textbf{Curly} should get the axe. But Curly hears this and interjects
that he only works a short period during the day, and consequently
makes mistakes only very rarely; there is only the tiniest chance that
he misfiled this particular document.

The boss would like to use this updated information to update the
probabilities for the three assistants, that is, (s)he wants to use
the additional likelihood that the document was misfiled to update
his/her beliefs about the likely culprit. Let \(A\) be the event that
a document is misfiled. What the boss would like to know are the three
probabilities
\[
\mathbb{P}(M|A),\mbox{ }\mathbb{P}(L|A),\mbox{ and }\mathbb{P}(C|A).
\]
We will show the calculation for \(\mathbb{P}(M|A)\), the other two
cases being similar. We use Bayes' Rule in the form
\[
\mathbb{P}(M|A)=\frac{\mathbb{P}(M\cap A)}{\mathbb{P}(A)}.
\]
Let's try to find \(\mathbb{P}(M\cap A)\), which is just
\(\mathbb{P}(M)\cdot\mathbb{P}(A|M)\) by the Multiplication Rule. We
already know \(\mathbb{P}(M)=0.6\) and \(\mathbb{P}(A|M)\) is nothing
more than Moe's misfile rate, given above to be
\(\mathbb{P}(A|M)=0.003\). Thus, we compute
\[
\mathbb{P}(M\cap A)=(0.6)(0.003)=0.0018.
\]
Using the same procedure we may calculate
\[
\mathbb{P}(L \cap A)=0.0021\mbox{ and }\mathbb{P}(C \cap A)=0.0010.
\]

Now let's find the denominator, \(\mathbb{P}(A)\). The key here is the
notion that if a file is misplaced, then either Moe or Larry or Curly
must have filed it; there is no one else around to do the
misfiling. Further, these possibilities are mutually exclusive. We may
use the Theorem of Total Probability
\eqref{eq-theorem-total-probability} to write \[
\mathbb{P}(A)=\mathbb{P}(A\cap M)+\mathbb{P}(A\cap L)+\mathbb{P}(A\cap
C).  \] Luckily, we have computed these above. Thus \[
\mathbb{P}(A)=0.0018+0.0021+0.0010=0.0049.  \] Therefore, Bayes' Rule
yields \[ \mathbb{P}(M|A)=\frac{0.0018}{0.0049}\approx0.37.  \] This
last quantity is called the posterior probability that Moe misfiled
the document, since it incorporates the observed data that a randomly
selected file was misplaced (which is governed by the misfile
rate). We can use the same argument to calculate

\begin{table}[htb]
\caption[Misfiling assistants: posterior]{Misfiling assistants: posterior.}
\centering
\begin{tabular}{llll}
 & Moe & Larry & Curly\\
\hline
Posterior Probability & \(\approx0.37\) & \(\approx0.43\) & \(\approx0.20\)\\
\end{tabular}
\end{table}

The conclusion: \textbf{Larry} gets the axe. What is happening is an
intricate interplay between the time on the job and the misfile
rate. It is not obvious who the winner (or in this case, loser) will
be, and the statistician needs to consult Bayes' Rule to determine the
best course of action.


\label{exa-misfiling-assistants-multiple} Suppose the boss gets a change
of heart and does not fire anybody. But the next day (s)he randomly
selects another file and again finds it to be misplaced. To decide
whom to fire now, the boss would use the same procedure, with one
small change. (S)he would not use the prior probabilities 60\%, 30\%,
and 10\%; those are old news. Instead, she would replace the prior
probabilities with the posterior probabilities just calculated. After
the math she will have new posterior probabilities, updated even more
from the day before.

In this way, probabilities found by Bayes' rule are always on the
cutting edge, always updated with respect to the best information
available at the time.

\subsection{How to do it with \(\mathsf{R}\)}
\label{sec-4-8-1}

\textbf{Misfiling assistants} (continued from Example \ref{exa-misfiling-assistants}). We store the prior probabilities and the likelihoods in
vectors and go to town.

\begin{Verbatim}
prior <- c(0.6, 0.3, 0.1)
like <- c(0.003, 0.007, 0.010)
post <- prior * like
post / sum(post)
\end{Verbatim}

\begin{verbatim}
: [1] 0.3673469 0.4285714 0.2040816
\end{verbatim}


Compare these answers with what we got in Example \ref{exa-misfiling-assistants}. We would replace \texttt{prior} with \texttt{post} in a future
calculation. We could raise \texttt{like} to a power to see how the posterior
is affected by future document mistakes. (Do you see why? Think back
to Section \ref{sec-4-7}.)



Let us incorporate the posterior probability (\texttt{post}) information from
the last example and suppose that the assistants misfile seven more
documents. Using Bayes' Rule, what would the new posterior
probabilities be?

\begin{Verbatim}
newprior <- post
post <- newprior * like^7
post / sum(post)
\end{Verbatim}

\begin{verbatim}
: [1] 0.0003355044 0.1473949328 0.8522695627
\end{verbatim}

We see that the individual with the highest probability of having
misfiled all eight documents given the observed data is no longer
Larry, but Curly.

There are two important points. First, we did not divide \texttt{post} by the
sum of its entries until the very last step; we do not need to
calculate it, and it will save us computing time to postpone
normalization until absolutely necessary, namely, until we finally
want to interpret them as probabilities.

Second, the reader might be wondering what the boss would get if (s)he
skipped the intermediate step of calculating the posterior after only
one misfiled document. What if she started from the \emph{original} prior,
then observed eight misfiled documents, and calculated the posterior?
What would she get? It must be the same answer, of course.

\begin{Verbatim}
fastpost <- prior * like^8
fastpost / sum(fastpost)
\end{Verbatim}

\begin{verbatim}
: [1] 0.0003355044 0.1473949328 0.8522695627
\end{verbatim}

\section{Random Variables}
\label{sec-4-9}

We already know about experiments, sample spaces, and events. In this
section, we are interested in a \emph{number} that is associated with the
experiment. We conduct a random experiment \(E\) and after learning
the outcome \(\omega\) in \(S\) we calculate a number \(X\). That is,
to each outcome \(\omega\) in the sample space we associate a number
\(X(\omega)=x\).

\begin{defn}
A \emph{random variable} \(X\) is a function \(X:S\to\mathbb{R}\) that
associates to each outcome \(\omega\in S\) exactly one number
\(X(\omega)=x\).
\end{defn}

We usually denote random variables by uppercase letters such as \(X\),
\(Y\), and \(Z\), and we denote their observed values by lowercase
letters \(x\), \(y\), and \(z\). Just as \(S\) is the set of all
possible outcomes of \(E\), we call the set of all possible values of
\(X\) the \emph{support} of \(X\) and denote it by \(S_{X}\).


Let \(E\) be the experiment of flipping a coin twice. We have seen
that the sample space is \( S = \{ HH,\ HT,\ TH,\ TT \} \). Now define
the random variable
\[ X = \mbox{the number of heads} .\]
That is, for
example, \(X(HH)=2\), while \(X(HT)=1\). We may make a table of the
possibilities:

\begin{table}[htb]
\caption[Flipping a coin twice]{\label{tab-flip-coin-twice}Flipping a coin twice.}
\centering
\begin{tabular}{lrrrr}
\(\omega\in S\) & \(HH\) & \(HT\) & \(TH\) & \(TT\)\\
\hline
\(X(\omega)=x\) & 2 & 1 & 1 & 0\\
\end{tabular}
\end{table}

Taking a look at the second row of the table, we see that the support
of \(X\) -- the set of all numbers that \(X\) assumes -- would be \(
S_{X}= \{ 0,1,2 \} \).



Let \(E\) be the experiment of flipping a coin repeatedly until
observing a Head. The sample space would be \(S= \{ H,\ TH,\ TTH,\
TTTH,\ \ldots \} \).
Now define the random variable
\[ Y=\mbox{the number of Tails before the first head} .\]
Then the support of \(Y\)
would be \( S_{Y}= \{ 0,1,2,\ldots \} \).



Let \(E\) be the experiment of tossing a coin in the air, and define
the random variable \( Z = \mbox{the time (in seconds) until the coin
hits the ground} \). In this case, the sample space is inconvenient to
describe. Yet the support of \(Z\) would be \((0,\infty)\). Of course,
it is reasonable to suppose that the coin will return to Earth in a
short amount of time; in practice, the set \((0,\infty)\) is
admittedly too large. However, we will find that in many circumstances
it is mathematically convenient to study the extended set rather than
a restricted one.


There are important differences between the supports of \(X\), \(Y\),
and \(Z\). The support of \(X\) is a finite collection of elements
that can be inspected all at once. And while the support of \(Y\)
cannot be exhaustively written down, its elements can nevertheless be
listed in a naturally ordered sequence. Random variables with supports
similar to those of \(X\) and \(Y\) are called \emph{discrete random
variables}.
In contrast, the support of \(Z\) is a continuous interval, containing
all rational and irrational positive real numbers. For this
reason, random variables with supports like \(Z\) are
called \emph{continuous random variables}.

\subsection{How to do it with \(\mathsf{R}\)}
\label{sec-4-9-1}

The idea is to evaluate an expression, apply or directly call a function defining the random variable.
As an example, let us roll a 4-sided die three times, and
let us define the random variable \(U=X1-X2+X3\), \(V=\max(X1,X2,X3)\) and \(W=X1+X2+X3\).

See \texttt{?with}, \texttt{?apply} and \texttt{?rowSums}.

\begin{Verbatim}
S <- rolldie(3, nsides = 4)
U <- with(S, X1-X2+X3)
V <- apply(S, 1, max)
W <- rowSums(S)
probs <- 1/nrow(S)
S <- cbind(S, U, V, W, probs)
head(S)
\end{Verbatim}

\begin{verbatim}
:   X1 X2 X3 U V W    probs
: 1  1  1  1 1 1 3 0.015625
: 2  2  1  1 2 2 4 0.015625
: 3  3  1  1 3 3 5 0.015625
: 4  4  1  1 4 4 6 0.015625
: 5  1  2  1 0 2 4 0.015625
: 6  2  2  1 1 2 5 0.015625
\end{verbatim}

Note that we only reproduce the first few rows of \(S\) (there are \(4^{3}=64\) rows in total).
Now let's take a look at the values of \(U\).
We see from the \(U\) column it is operating just like it should.
We can now answer questions like

\begin{Verbatim}
sum(subset(S, U > 6)$probs)
\end{Verbatim}

\begin{verbatim}
: [1] 0.015625
\end{verbatim}

\subsection{Marginal Distributions}
\label{sec-4-9-2}

As we can see above, often after adding a random variable \(V\) to a
probability space one will find that \(V\) has values that are
repeated, so that it becomes difficult to understand what the ultimate
behavior of \(V\) actually is.
We can \texttt{aggregate} the sample space by values of \(V\), all the
while accumulating the probability associated with \(V\)'s distinct
values. Continuing our example from above, suppose we would like to
focus entirely on the values and probabilities of
\(V=\max(X1,X2,X3)\).

\begin{Verbatim}
aggregate(S["probs"], by=S["V"], FUN=sum)
\end{Verbatim}

\begin{verbatim}
:   V    probs
: 1 1 0.015625
: 2 2 0.109375
: 3 3 0.296875
: 4 4 0.578125
\end{verbatim}

We could save the probability space of \(V\) in a data frame and study
it further, if we wish. As a final remark, we can calculate the
distributions of multiple variables, e.g., \(V\) and \(W\).

\begin{Verbatim}
aggregate(S["probs"], by=S[c("V","W")], FUN=sum)
\end{Verbatim}

\begin{verbatim}
   V  W    probs
1  1  3 0.015625
2  2  4 0.046875
3  2  5 0.046875
4  3  5 0.046875
5  2  6 0.015625
6  3  6 0.093750
7  4  6 0.046875
8  3  7 0.093750
9  4  7 0.093750
10 3  8 0.046875
11 4  8 0.140625
12 3  9 0.015625
13 4  9 0.140625
14 4 10 0.093750
15 4 11 0.046875
16 4 12 0.015625
\end{verbatim}
