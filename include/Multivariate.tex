\chapter{Multivariate Distributions}
\label{sec-7}

\noindent
We have built up quite a catalogue of distributions, discrete and
continuous. They were all univariate, however, meaning that we only
considered one random variable at a time. We can imagine nevertheless
many random variables associated with a single person: their height,
their weight, their wrist circumference (all continuous), or their
eye/hair color, shoe size, whether they are right handed, left handed,
or ambidextrous (all categorical), and we can even surmise reasonable
probability distributions to associate with each of these variables.
But there is a difference: for a single person, these variables are
related. For instance, a person's height betrays a lot of information
about that person's weight.

The concept we are hinting at is the notion of \emph{dependence} between
random variables. It is the focus of this chapter to study this
concept in some detail. Along the way, we will pick up additional
models to add to our catalogue. Moreover, we will study certain
classes of dependence, and clarify the special case when there is no
dependence, namely, independence.

\textbf{Highlights:}
\begin{itemize}
\item joint versus marginal distributions/expectation (discrete and
continuous)
\item some numeric measures of dependence
\item some details of multivariate model (discrete and
continuous)
\item what it looks like when there are more than two random variables
present
\end{itemize}

\section{Joint and Marginal Probability Distributions}
\label{sec-7-1}

Consider two discrete random variables \(X\) and \(Y\) with PMFs
\(f_{X}\) and \(f_{Y}\) that are supported on the sample spaces
\(S_{X}\) and \(S_{Y}\), respectively. Let \(S_{X,Y}\) denote the set
of all possible observed \emph{pairs} \((x,y)\), called the \emph{joint support
set} of \(X\) and \(Y\). Then the \emph{joint probability mass function} of
\(X\) and \(Y\) is the function \(f_{X,Y}\) defined by
\begin{equation}
\label{eq-joint-pmf}
f_{X,Y}(x,y)=\mathbb{P}(X=x,\, Y=y),\quad \mbox{for }(x,y)\in S_{X,Y}.
\end{equation}
Every joint PMF satisfies
\begin{equation}
f_{X,Y}(x,y)>0\mbox{ for all }(x,y)\in S_{X,Y},
\end{equation}
and
\begin{equation}
\sum_{(x,y)\in S_{X,Y}}f_{X,Y}(x,y)=1.
\end{equation}
It is customary to extend the function \(f_{X,Y}\) to be defined on
all of \(\mathbb{R}^{2}\) by setting \(f_{X,Y}(x,y)=0\) for
\((x,y)\not\in S_{X,Y}\).

In the context of this chapter, the PMFs \(f_{X}\) and \(f_{Y}\) are
called the \emph{marginal PMFs} of \(X\) and \(Y\), respectively. If we are
given only the joint PMF then we may recover each of the marginal PMFs
by using the Theorem of Total Probability:
\begin{eqnarray}
f_{X}(x) & = & \mathbb{P}(X=x),\\
 & = & \sum_{y\in S_{Y}}\mathbb{P}(X=x,\, Y=y),\\
 & = & \sum_{y\in S_{Y}}f_{X,Y}(x,y).
\end{eqnarray}
By interchanging the roles of \(X\) and \(Y\) it is clear that 
\begin{equation}
\label{eq-marginal-pmf}
f_{Y}(y)=\sum_{x\in S_{Y}}f_{X,Y}(x,y).
\end{equation}
Given the joint PMF we may recover the marginal PMFs, but the converse
is not true. Even if we have \emph{both} marginal distributions they are
not sufficient to determine the joint PMF; more information is
needed.

\label{exa-max-sum-two-dice} Let the random experiment again be to roll a
fair die twice, except now let us define the random variables \(U\)
and \(V\) by
\begin{eqnarray*}
U & = & \mbox{the maximum of the two rolls, and }\\
V & = & \mbox{the sum of the two rolls.}
\end{eqnarray*}
We see that the support of \(U\) is \(S_{U}= \{ 1,2,\ldots,6 \} \) and
the support of \(V\) is \(S_{V}= \{ 2,3,\ldots,12 \} \). We may
represent the sample space with a matrix, and for each entry in the
matrix we may calculate the value that \(U\) assumes. The result is in
the left half of Figure \ref{fig-max-and-sum-two-dice}.

We can use the table to calculate the marginal PMF of \(U\), because
we know that each entry in
the matrix has probability \(1/36\) associated with it. For instance,
there is only one outcome in the matrix with \(U=1\), namely, the
bottom left corner. This single entry has probability \(1/36\),
therefore, it must be that
\(f_{U}(1)=\mathbb{P}(U=1)=1/36\). Similarly we see that there are
three entries in the matrix with \(U=2\), thus
\(f_{U}(2)=3/36\).
We may do a similar thing for \(V\); see the right half of Figure
\ref{fig-max-and-sum-two-dice}.

\begin{figure}[ht!]
\centering
\includegraphics[width=0.9\textwidth]{fig/multdist-max-and-sum-two-dice.ps}
\caption[Max and Sum of two dice]{\label{fig-max-and-sum-two-dice}\small Rolling two dice. The value of U is the maximum of the two rolls, while the value of V is the sum of the two rolls.}
\end{figure}

We may collapse the two matrices from Figure \ref{fig-max-and-sum-two-dice} into
one, big matrix of pairs of values \((u,v)\). The result is shown in
Table \ref{fig-max-sum-two-dice-joint}.

\begin{figure}[ht!]
\centering
\includegraphics[width=0.9\textwidth]{fig/multdist-max-sum-two-dice-joint.ps}
\caption[Joint outcomes of Max and Sum]{\label{fig-max-sum-two-dice-joint}\small Rolling two dice. Joint values of U and V are shown as pairs, for each outcome in the sample space.}
\end{figure}

Again, each of these pairs has probability \(1/36\) associated with it
and we are looking at the joint PDF of \((U,V)\) albeit in an unusual
form. Many of the pairs are repeated, but some of them are not:
\((1,2)\) appears only once, but \((2,3)\) appears twice. We can make
more sense out of this by writing a new table with \(U\) on one side
and \(V\) along the top.
We will accumulate the probability in Table \ref{tab-max-sum-joint-pmf}.

\begin{table}[htb]
\caption[The joint PMF of \((U,V)\)]{\label{tab-max-sum-joint-pmf}The outcomes of \(U\) are along the left and the outcomes of \(V\) are along the top. Empty entries in the table have zero probability. The row totals (on the right) and column totals (on the bottom) correspond to the marginal distribution of \(U\) and \(V\), respectively.}
\centering
\begin{tabular}{r|lllllrrrrrr|l}
 & 2 & 3 & 4 & 5 & 6 & 7 & 8 & 9 & 10 & 11 & 12 & Total\\
\hline
1 & \(\frac{1}{36}\) &  &  &  &  &  &  &  &  &  &  & \(\frac{1}{36}\)\\
2 &  & \(\frac{2}{36}\) & \(\frac{1}{36}\) &  &  &  &  &  &  &  &  & \(\frac{3}{36}\)\\
3 &  &  & \(\frac{2}{36}\) & \(\frac{2}{36}\) & \(\frac{1}{36}\) &  &  &  &  &  &  & \(\frac{5}{36}\)\\
4 &  &  &  & \(\frac{2}{36}\) & \(\frac{2}{36}\) & \(\frac{2}{36}\) & \(\frac{1}{36}\) &  &  &  &  & \(\frac{7}{36}\)\\
5 &  &  &  &  & \(\frac{2}{36}\) & \(\frac{2}{36}\) & \(\frac{2}{36}\) & \(\frac{2}{36}\) & \(\frac{1}{36}\) &  &  & \(\frac{9}{36}\)\\
6 &  &  &  &  &  & \(\frac{2}{36}\) & \(\frac{2}{36}\) & \(\frac{2}{36}\) & \(\frac{2}{36}\) & \(\frac{2}{36}\) & \(\frac{1}{36}\) & \(\frac{11}{36}\)\\
\hline
Total & \(\frac{1}{36}\) & \(\frac{2}{36}\) & \(\frac{3}{36}\) & \(\frac{4}{36}\) & \(\frac{5}{36}\) & \(\frac{6}{36}\) & \(\frac{5}{36}\) & \(\frac{4}{36}\) & \(\frac{3}{36}\) & \(\frac{2}{36}\) & \(\frac{1}{36}\) & 1\\
\end{tabular}
\end{table}

The joint support of \((U,V)\) is concentrated along the main
diagonal; note that the nonzero entries do not form a rectangle. Also
notice that if we form row and column totals we are doing exactly the
same thing as Equation \eqref{eq-marginal-pmf}, so that the marginal
distribution of \(U\) is the list of totals in the right "margin" of
the Table \ref{tab-max-sum-joint-pmf}, and the marginal distribution of \(V\) is the
list of totals in the bottom "margin".


Continuing the reasoning for the discrete case, given two continuous
random variables \(X\) and \(Y\) there similarly
exists a function \(f_{X,Y}(x,y)\) associated with
\(X\) and \(Y\) called the \emph{joint probability density function} of
\(X\) and \(Y\). Every joint PDF satisfies
\begin{equation}
f_{X,Y}(x,y)\geq0\mbox{ for all }(x,y)\in S_{X,Y},
\end{equation}
and
\begin{equation}
\iintop_{S_{X,Y}}f_{X,Y}(x,y)\,\mathrm{d} x\,\mathrm{d} y=1.
\end{equation}
In the continuous case there is not such a simple interpretation for
the joint PDF; however, we do have one for the joint CDF, namely, \[
F_{X,Y}(x,y)=\mathbb{P}(X\leq x,\, Y\leq
y)=\int_{-\infty}^{x}\int_{-\infty}^{y}f_{X,Y}(u,v)\,\mathrm{d}
v\,\mathrm{d} u, \] for \((x,y)\in\mathbb{R}^{2}\). If \(X\) and \(Y\)
have the joint PDF \(f_{X,Y}\), then the marginal density of \(X\) may
be recovered by
\begin{equation}
f_{X}(x)=\int_{S_{Y}}f_{X,Y}(x,y)\,\mathrm{d} y,\quad x \in S_{X}
\end{equation}
and the marginal PDF of \(Y\) may be found with
\begin{equation}
f_{Y}(y)=\int_{S_{X}}f_{X,Y}(x,y)\,\mathrm{d} x, \quad y \in S_{Y}.
\end{equation}

\subsection{How to do it with \(\mathsf{R}\)}
\label{sec-7-1-1}

We will show how to do Example \ref{exa-max-sum-two-dice} using \(\mathsf{R}\);
it is much simpler to do it with \(\mathsf{R}\) than without.

\begin{Verbatim}
S <- expand.grid(X1=1:6, X2=1:6)
U <- apply(S, 1, max)
V <- apply(S, 1, sum)
S <- cbind(S, U, V, probs=1/nrow(S))
head(S)
\end{Verbatim}

\begin{verbatim}
:   X1 X2 U V      probs
: 1  1  1 1 2 0.02777778
: 2  2  1 2 3 0.02777778
: 3  3  1 3 4 0.02777778
: 4  4  1 4 5 0.02777778
: 5  5  1 5 6 0.02777778
: 6  6  1 6 7 0.02777778
\end{verbatim}

Yes, the \(U\) and \(V\) columns have been added to the data frame and
have been computed correctly. This result would be fine as it is, but
the data frame has too many rows: there are repeated pairs \((u,v)\)
which show up as repeated rows in the data frame. The goal is to
aggregate the rows of \(S\) such that the result has exactly one row
for each unique pair \((u,v)\) with positive probability.
We may take a look at the joint distribution of \(U\) and
\(V\) by showing the first few rows of the data frame:

\begin{Verbatim}
UV <- aggregate(S["probs"], by=S[c("U","V")], FUN=sum)
head(UV)
\end{Verbatim}

\begin{verbatim}
:   U V      probs
: 1 1 2 0.02777778
: 2 2 3 0.05555556
: 3 2 4 0.02777778
: 4 3 4 0.05555556
: 5 3 5 0.05555556
: 6 4 5 0.05555556
\end{verbatim}

It would be even better to have
a tabular display like Table \ref{tab-max-sum-joint-pmf}. We can do that with
the \texttt{xtabs} function.

\begin{Verbatim}
round(xtabs(probs ~ U + V, data = UV), 3)
\end{Verbatim}

\begin{verbatim}
:    V
: U       2     3     4     5     6     7     8     9    10    11    12
:   1 0.028 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000
:   2 0.000 0.056 0.028 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000
:   3 0.000 0.000 0.056 0.056 0.028 0.000 0.000 0.000 0.000 0.000 0.000
:   4 0.000 0.000 0.000 0.056 0.056 0.056 0.028 0.000 0.000 0.000 0.000
:   5 0.000 0.000 0.000 0.000 0.056 0.056 0.056 0.056 0.028 0.000 0.000
:   6 0.000 0.000 0.000 0.000 0.000 0.056 0.056 0.056 0.056 0.056 0.028
\end{verbatim}

Compare these values to the ones shown in Table \ref{tab-max-sum-joint-pmf}.
Note that, the same output can also be computed with the $table$ and $prop.table$ functions:

\begin{Verbatim}
round(prop.table(table(U, V)), 3)
\end{Verbatim}

We can repeat the process to get the univariate marginal
distributions of \(U\) and \(V\) separately.

\begin{Verbatim}
aggregate(UV["probs"], by=UV["U"], FUN=sum)
aggregate(UV["probs"], by=UV["V"], FUN=sum)
\end{Verbatim}

\begin{verbatim}
  U      probs
1 1 0.02777778
2 2 0.08333333
3 3 0.13888889
4 4 0.19444444
5 5 0.25000000
6 6 0.30555556
  V      probs
1 2 0.02777778
2 3 0.05555556
3 4 0.08333333
4 5 0.11111111
5 6 0.13888889
6 7 0.16666667
7 8 0.13888889
8 9 0.11111111
9 10 0.08333333
10 11 0.05555556
11 12 0.02777778
\end{verbatim}

Another way to do the same thing is with the \texttt{rowSums} and \texttt{colSums}
of the \texttt{xtabs} object. Compare

\begin{Verbatim}
temp <- xtabs(probs ~ U + V, data = UV)
round(rowSums(temp), 3)
round(colSums(temp), 3)
\end{Verbatim}

\begin{verbatim}
:     1     2     3     4     5     6
: 0.028 0.083 0.139 0.194 0.250 0.306
:     2     3     4     5     6     7     8     9    10    11    12
: 0.028 0.056 0.083 0.111 0.139 0.167 0.139 0.111 0.083 0.056 0.028
\end{verbatim}

You should check that the answers that we have obtained exactly match
the same (somewhat laborious) calculations that we completed in
Example \ref{exa-max-sum-two-dice}.

\section{Joint and Marginal Expectation}
\label{sec-7-2}

Given a function \(g\) with arguments \((x,y)\) we would like to know
the long-run average behavior of \(g(X,Y)\) and how to mathematically
calculate it. Expectation in this context is computed in the
pedestrian way. We simply integrate (sum) with respect to the joint
probability density (mass) function.
\begin{equation}
\mathbb{E}\, g(X,Y)=\iintop_{S_{X,Y}}g(x,y)\, f_{X,Y}(x,y)\,\mathrm{d} x\,\mathrm{d} y,
\end{equation}
or in the discrete case,
\begin{equation}
\mathbb{E}\, g(X,Y)=\mathop{\sum\sum}\limits _{(x,y)\in S_{X,Y}}g(x,y)\, f_{X,Y}(x,y).
\end{equation}

\subsection{Covariance and Correlation}
\label{sec-7-2-1}

There are two very special cases of joint expectation: the
\emph{covariance} and the \emph{correlation}. These are measures which help us
quantify the dependence between \(X\) and \(Y\).

\begin{defn}
The \emph{covariance} of \(X\) and \(Y\) is
\begin{equation}
\mbox{Cov}(X,Y)=\mathbb{E}(X-\mathbb{E} X)(Y-\mathbb{E} Y).
\end{equation}
\end{defn}

By the way, there is a shortcut formula for covariance which is almost
as handy as the shortcut for the variance:
\begin{equation}
\mbox{Cov}(X,Y)=\mathbb{E}(XY)-(\mathbb{E} X)(\mathbb{E} Y).
\end{equation}
The proof is left to Exercise \ref{xca-Prove-cov-shortcut}.

The Pearson product moment correlation between \(X\) and \(Y\) is the
covariance between \(X\) and \(Y\) rescaled to fall in the interval
\([-1,1]\). It is formally defined by
\begin{equation}
\mbox{Corr}(X,Y)=\frac{\mbox{Cov}(X,Y)}{\sigma_{X}\sigma_{Y}}.
\end{equation}

The correlation is usually denoted by \(\rho_{X,Y}\) or simply
\(\rho\) if the random variables are clear from context. There are
some important facts about the correlation coefficient:
\begin{enumerate}
\item The range of correlation is \(-1\leq\rho_{X,Y}\leq1\).
\item Equality holds above (\(\rho_{X,Y}=\pm1\)) if and only if \(Y\) is a linear function of \(X\) with probability one.
\end{enumerate}

\label{exa-max-sum-dice-covariance} We will compute the covariance for the
discrete distribution in Example \ref{exa-max-sum-two-dice}. The expected
value of \(U\) is \[ \mathbb{E} U=\sum_{u=1}^{6}u\,
f_{U}(u)=1\left(\frac{1}{36}\right)+2\left(\frac{3}{36}\right)+\cdots+6\left(\frac{11}{36}\right)=\frac{161}{36},
\] and the expected value of \(V\) is \[ \mathbb{E}
V=\sum_{v=2}^{12}v\,f_{V}(v)=2\left(\frac{1}{36}\right)+3\left(\frac{2}{36}\right)+\cdots+12\left(\frac{1}{36}\right)=7,
\] and the expected value of \(UV\) is \[ \mathbb{E}
UV=\sum_{u=1}^{6}\sum_{v=2}^{12}uv\,
f_{U,V}(u,v)=1\cdot2\left(\frac{1}{36}\right)+2\cdot3\left(\frac{2}{36}\right)+\cdots+6\cdot12\left(\frac{1}{36}\right)=\frac{308}{9}.
\] Therefore the covariance of \((U,V)\) is \[
\mbox{Cov}(U,V)=\mathbb{E} UV-\left(\mathbb{E}
U\right)\left(\mathbb{E}
V\right)=\frac{308}{9}-\frac{161}{36}\cdot7=\frac{35}{12}.  \] All we
need now are the standard deviations of \(U\) and \(V\) to calculate
the correlation coefficient (omitted).

\subsubsection{How to do it with \(\mathsf{R}\)}
\label{sec-7-2-1-1}

We just need to keep the definition in mind.
For
instance, we may compute the covariance of \((U,V)\) from Example
\ref{exa-max-sum-dice-covariance}.

\begin{Verbatim}
Eu <- sum(S$U*S$probs)
Ev <- sum(S$V*S$probs)
Euv <- sum(S$U*S$V*S$probs)
Euv - Eu * Ev
\end{Verbatim}

\begin{verbatim}
: [1] 2.916667
\end{verbatim}

Compare this answer to what we got in Example \ref{exa-max-sum-dice-covariance}.

\section{Conditional and Independent Random Variables}
\label{sec-7-3}

\subsection{Conditional Distributions}

If \(x\in S_{X}\) is such that \(f_{X}(x)>0\), then we define the
\emph{conditional density} of \(Y|\, X=x\), denoted \(f_{Y|x}\), by
\begin{equation}
f_{Y|x}(y|x)=\frac{f_{X,Y}(x,y)}{f_{X}(x)},\quad y\in S_{Y}.
\end{equation}
We define \(f_{X|y}\) in a similar fashion.

\subsection{Independent Random Variables}
\label{sec-7-4-1}

We recall that the events \(A\) and \(B\) are
said to be independent when
\begin{equation}
\mathbb{P}(A\cap B)=\mathbb{P}(A)\mathbb{P}(B).
\end{equation}
If it happens that
\begin{equation}
\mathbb{P}(X=x,Y=y)=\mathbb{P}(X=x)\mathbb{P}(Y=y),\quad \mbox{for every }x\in S_{X},\ y\in S_{Y},
\end{equation}
then we say that \(X\) and \(Y\) are \emph{independent random
variables}. Otherwise, we say that \(X\) and \(Y\) are
\emph{dependent}. Using the PMF notation from above, we see that
independent discrete random variables satisfy
\begin{equation}
f_{X,Y}(x,y)=f_{X}(x)f_{Y}(y)\quad \mbox{for every }x\in S_{X},\ y\in S_{Y}.
\end{equation}
Continuing the reasoning, given two continuous random variables \(X\)
and \(Y\) with joint PDF \(f_{X,Y}\) and respective marginal PDFs
\(f_{X}\) and \(f_{Y}\) that are supported on the sets \(S_{X}\) and
\(S_{Y}\), if it happens that
the above equality holds
then we say that \(X\) and \(Y\) are independent.

In Example \ref{exa-max-sum-two-dice} we considered the same experiment but
different random variables \(U\) and \(V\). We can prove that \(U\)
and \(V\) are not independent if we can find a single pair \((u,v)\)
where the independence equality does not hold. There are many such
pairs. One of them is \((6,12)\): \[
f_{U,V}(6,12)=\frac{1}{36}\neq\left(\frac{11}{36}\right)\left(\frac{1}{36}\right)=f_{U}(6)\,
f_{V}(12).  \]


Independent random variables are very useful to the
mathematician. They have many, many, tractable properties. We mention
some of the more important ones.

\begin{prop}
\label{pro-indep-implies-prodexpect} If \(X\) and \(Y\) are independent,
then for any functions \(u\) and \(v\),
\begin{equation}
\mathbb{E}\left(u(X)v(Y)\right)=\left(\mathbb{E} u(X)\right)\left(\mathbb{E} v(Y)\right).
\end{equation}
\end{prop}

\begin{proof}
This is straightforward from the definition.
\begin{eqnarray*}
\mathbb{E}\left(u(X)v(Y)\right) & = & \iint\, u(x)v(y)\, f_{X,Y}(x,y)\,\mathrm{d} x\mathrm{d} y\\
 & = & \iint\, u(x)v(y)\, f_{X}(x)\, f_{Y}(y)\,\mathrm{d} x\mathrm{d} y\\
 & = & \int u(x)\, f_{X}(x)\,\mathrm{d} x\ \int v(y)\, f_{Y}(y)\,\mathrm{d} y
\end{eqnarray*}
and this last quantity is exactly \(\left(\mathbb{E} u(X)\right)\left(\mathbb{E} v(Y)\right)\). 
\end{proof}

Now that we have Proposition \ref{pro-indep-implies-prodexpect} we mention a
corollary that will help us later to quickly identify those random
variables which are \emph{not} independent.

\begin{cor}
\label{cor-indep-implies-uncorr} If \(X\) and \(Y\) are independent, then
\(\mbox{Cov}(X,Y)=0\), and consequently, \(\mbox{Corr}(X,Y)=0\).
\end{cor}

\begin{proof}
When \(X\) and \(Y\) are independent then \(\mathbb{E} XY=\mathbb{E}
X\,\mathbb{E} Y\). And when the covariance is zero the numerator of
the correlation is 0.
\end{proof}

\begin{rem}
\label{rem-cov0-not-imply-indep} Unfortunately, the converse of Corollary
\ref{cor-indep-implies-uncorr} is not true. That is, there are many random
variables which are dependent yet their covariance and correlation is
zero.
\end{rem}

Proposition \ref{pro-indep-implies-prodexpect} is useful to us and we will
receive mileage out of it, but there is another fact which will play
an even more important role.

\begin{fact}
\label{fac-indep-then-function-indep} If \(X\) and \(Y\) are independent,
then \(u(X)\) and \(v(Y)\) are independent for any functions \(u\) and
\(v\).
\end{fact}

\subsection{Combining Independent Random Variables}
\label{sec-7-4-2}

Another important corollary of Proposition
\ref{pro-indep-implies-prodexpect} will allow us to find the distribution of
sums of random variables.

\begin{cor}
Let \(X\sim\mathsf{binom}(\mathtt{size}=n_{1},\,\mathtt{prob}=p)\) and
\(Y\sim\mathsf{binom}(\mathtt{size}=n_{2},\,\mathtt{prob}=p)\) be
independent.
Then
\(X+Y\sim\mathsf{binom}(\mathtt{size}=n_{1}+n_{2},\,\mathtt{prob}=p)\).
\end{cor}



\begin{cor}
Let
\(X\sim\mathsf{norm}(\mathtt{mean}=\mu_{1},\,\mathtt{sd}=\sigma_{1})\)
and
\(Y\sim\mathsf{norm}(\mathtt{mean}=\mu_{2},\,\mathtt{sd}=\sigma_{2})\)
be independent.
Then \(X+Y\) has a
\(\mathsf{norm}\left(\mathtt{mean}=\mu_{1}+\mu_{2},\,\mathtt{sd}=\sqrt{\sigma_{1}^{2}+\sigma_{2}^{2}}\right)\)
distribution.
\end{cor}


Even when we cannot identify the exact
distribution of a linear combination of random variables, we can still
say something about its mean and variance.

\begin{prop}
\label{pro-mean-sd-lin-comb-two} Let \(X_{1}\) and \(X_{2}\) be
independent with respective population means \(\mu_{1}\) and
\(\mu_{2}\) and population standard deviations \(\sigma_{1}\) and
\(\sigma_{2}\). For given constants \(a_{1}\) and \(a_{2}\), define
\(Y=a_{1}X_{1}+a_{2}X_{2}\). Then the mean and standard deviation of
\(Y\) are given by the formulas
\begin{equation}
\mu_{Y}=a_{1}\mu_{1}+a_{2}\mu_{2},\quad \sigma_{Y}=\left(a_{1}^{2}\sigma_{1}^{2}+a_{2}^{2}\sigma_{2}^{2}\right)^{1/2}.
\end{equation}
\end{prop}

\begin{proof}
We see \[ \mathbb{E}
Y=\mathbb{E}\left(a_{1}X_{1}+a_{2}X_{2}\right)=a_{1}\mathbb{E}
X_{1}+a_{2}\mathbb{E} X_{2}=a_{1}\mu_{1}+a_{2}\mu_{2}.  \] For the
standard deviation, we will find the variance and take the square root
at the end. And to calculate the variance we will first compute
\(\mathbb{E} Y^{2}\) with an eye toward using the identity
\(\sigma_{Y}^{2}=\mathbb{E} Y^{2}-\left(\mathbb{E} Y\right)^{2}\) as a
final step.  \[ \mathbb{E}
Y^{2}=\mathbb{E}\left(a_{1}X_{1}+a_{2}X_{2}\right)^{2}=\mathbb{E}\left(a_{1}^{2}X_{1}^{2}+a_{2}^{2}X_{2}^{2}+2a_{1}a_{2}X_{1}X_{2}\right).
\] Using linearity of expectation the \(\mathbb{E}\) distributes
through the sum. Now \(\mathbb{E}
X_{i}^{2}=\sigma_{i}^{2}+\mu_{i}^{2}\), for \(i=1\) and 2 and
\(\mathbb{E} X_{1}X_{2}=\mathbb{E} X_{1}\mathbb{E}
X_{2}=\mu_{1}\mu_{2}\) because of independence. Thus
\begin{eqnarray*}
\mathbb{E} Y^{2} & = & a_{1}^{2}(\sigma_{1}^{2}+\mu_{1}^{2})+a_{2}^{2}(\sigma_{2}^{2}+\mu_{2}^{2})+2a_{1}a_{2}\mu_{1}\mu_{2},\\
 & = & a_{1}^{2}\sigma_{1}^{2}+a_{2}^{2}\sigma_{2}^{2}+\left(a_{1}^{2}\mu_{1}^{2}+a_{2}^{2}\mu_{2}^{2}+2a_{1}a_{2}\mu_{1}\mu_{2}\right).
\end{eqnarray*}
But notice that the expression in the parentheses is exactly
\(\left(a_{1}\mu_{1}+a_{2}\mu_{2}\right)^{2}=\left(\mathbb{E}
Y\right)^{2}\), so the proof is complete.
\end{proof}

\section{Remarks for the Multivariate Case}
\label{sec-7-8}

There is nothing spooky about \(n\geq3\) random variables. We just
have a whole bunch of them: \(X_{1}\), \(X_{2}\),\ldots{}, \(X_{n}\), which
we can shorten to
\(\mathbf{X}=(X_{1},X_{2},\ldots,X_{n})^{\mathrm{T}}\) to make the
formulas prettier.
For \(\mathbf{X}\) supported on the set
\(S_{\mathbf{X}}\), the joint PDF \(f_{\mathbf{X}}\) (if it exists)
satisfies
\begin{equation}
f_{\mathbf{X}}(\mathbf{x})>0,\quad \mbox{for }\mathbf{x}\in S_{\mathbf{X}},
\end{equation}
and
\begin{equation}
\int\!\!\!\int\cdots\int f_{\mathbf{X}}(\mathbf{x})\,\mathrm{d} x_{1}\mathrm{d} x_{2}\cdots\mathrm{d} x_{n}=1,
\end{equation}
or even shorter: \(\int
f_{\mathbf{X}}(\mathbf{x})\,\mathrm{d}\mathbf{x}=1\). The joint CDF
\(F_{\mathbf{X}}\) is defined by
\begin{equation}
F_{\mathbf{X}}(\mathbf{x})=\mathbb{P}(X_{1}\leq x_{1},\, X_{2}\leq x_{2},\ldots,\, X_{n}\leq x_{n}),
\end{equation}
for \(\mathbf{x}\in\mathbb{R}^{n}\). The expectation of a function
\(g(\mathbf{X})\) is defined just as we would imagine:
\begin{equation}
\mathbb{E} g(\mathbf{X})=\int g(\mathbf{x})\, f_{\mathbf{X}}(\mathbf{x})\,\mathrm{d}\mathbf{x}.
\end{equation}
provided the integral exists and is finite.
The only difference in
any of the above for the discrete case is that integrals are replaced
by sums.

Marginal distributions are obtained by integrating out remaining
variables from the joint distribution. And even if we are given all of
the univariate marginals it is not enough to determine the joint
distribution uniquely.

We say that \(X_{1}\), \(X_{2}\), \ldots{}, \(X_{n}\) are \emph{mutually
independent} if their joint PDF factors into the product of the
marginals
\begin{equation}
f_{\mathbf{X}}(\mathbf{x})=f_{X_{1}}(x_{1})\, f_{X_{2}}(x_{2})\,\cdots\, f_{X_{n}}(x_{n}),
\end{equation}
for every \(\mathbf{x}\) in their joint support \(S_{\mathbf{X}}\).

\begin{prop}
\label{pro-mean-sd-lin-comb} Let \(X_{1}\), \(X_{2}\), \ldots{}, \(X_{n}\) be
independent with respective population means \(\mu_{1}\), \(\mu_{2}\),
\ldots{}, \(\mu_{n}\) and standard deviations \(\sigma_{1}\),
\(\sigma_{2}\), \ldots{}, \(\sigma_{n}\). For given constants \(a_{1}\),
\(a_{2}\), \ldots{},\(a_{n}\) define \(Y=\sum_{i=1}^{n}a_{i}X_{i}\). Then
the mean and standard deviation of \(Y\) are given by the formulas
\begin{equation}
\mu_{Y}=\sum_{i=1}^{n}a_{i}\mu_{i},\quad \sigma_{Y}=\left(\sum_{i=1}^{n}a_{i}^{2}\sigma_{i}^{2}\right)^{1/2}.
\end{equation}
\end{prop}

\begin{proof}
The mean is easy: \[ \mathbb{E}
Y=\mathbb{E}\left(\sum_{i=1}^{n}a_{i}X_{i}\right)=\sum_{i=1}^{n}a_{i}\mathbb{E}
X_{i}=\sum_{i=1}^{n}a_{i}\mu_{i}.  \] The variance is not too
difficult to compute either. As an intermediate step, we calculate
\(\mathbb{E} Y^{2}\).  \[ \mathbb{E}
Y^{2}=\mathbb{E}\left(\sum_{i=1}^{n}a_{i}X_{i}\right)^{2}=\mathbb{E}\left(\sum_{i=1}^{n}a_{i}^{2}X_{i}^{2}+2\sum_{i=1}^{n-1}\sum_{j=i+1}^{n}a_{i}a_{j}X_{i}X_{j}\right).
\] Using linearity of expectation the \(\mathbb{E}\) distributes
through the sums. Now \(\mathbb{E}
X_{i}^{2}=\sigma_{i}^{2}+\mu_{i}^{2}\) and \(\mathbb{E}
X_{i}X_{j}=\mathbb{E} X_{i}\mathbb{E} X_{j}=\mu_{i}\mu_{j}\) when
\(i\neq j\) because of independence. Thus
\begin{eqnarray*}
\mathbb{E} Y^{2} & = & \sum_{i=1}^{n}a_{i}^{2}(\sigma_{i}^{2}+\mu_{i}^{2})+2\sum_{i=1}^{n-1}\sum_{j=i+1}^{n}a_{i}a_{j}\mu_{i}\mu_{j}
= \sum_{i=1}^{n}a_{i}^{2}\sigma_{i}^{2}+\left(\sum_{i=1}^{n}a_{i}^{2}\mu_{i}^{2}+2\sum_{i=1}^{n-1}\sum_{j=i+1}^{n}a_{i}a_{j}\mu_{i}\mu_{j}\right)
\end{eqnarray*}
To complete the proof, note that the expression in the parentheses is
exactly \(\left(\mathbb{E} Y\right)^{2}\), and recall the identity
\(\sigma_{Y}^{2}=\mathbb{E} Y^{2}-\left(\mathbb{E} Y\right)^{2}\).
\end{proof}

There is a corresponding statement of Fact
\ref{fac-indep-then-function-indep} for the multivariate case. The proof is
also omitted here.

\begin{fact}
If \(\mathbf{X}\) and \(\mathbf{Y}\) are mutually independent random
vectors, then \(u(\mathbf{X})\) and \(v(\mathbf{Y})\) are independent
for any functions \(u\) and \(v\).
\end{fact}

\newpage{}

\section{Exercises}
\label{sec-7-10}

\setcounter{thm}{0}

\begin{xca}
\label{xca-Prove-cov-shortcut} Prove that \(
\mbox{Cov}(X,Y)=\mathbb{E}(XY)-(\mathbb{E} X)(\mathbb{E} Y). \)
\end{xca}
