\chapter{Continuous Distributions}
\label{sec-6}

\noindent
The focus of the last chapter was on random variables whose support
can be written down in a list of values (finite or countably
infinite), such as the number of successes in a sequence of Bernoulli
trials. Now we move to random variables whose support is a whole range
of values, say, an interval \((a,b)\). It is shown in later classes
that it is impossible to write all of the numbers down in a list;
there are simply too many of them.

This chapter begins with continuous random variables and the
associated PDFs and CDFs.
The continuous uniform distribution is
highlighted, along with the Gaussian, or normal, distribution. Some
mathematical details pave the way for a catalogue of models.

\textbf{Highlights:}
\begin{itemize}
\item how to choose a reasonable continuous model under a variety of
circumstances
\item basic correspondence between continuous versus discrete random
variables
\item some details on a couple of continuous models, and exposure to a
bunch of other ones
\item how to make new continuous random variables from old ones
\end{itemize}

\section{Continuous Random Variables}
\label{sec-6-1}

\subsection{Probability Density Functions}
\label{sec-6-1-1}

Continuous random variables have supports that look like
\begin{equation}
S_{X}=[a,b]\mbox{ or }(a,b),
\end{equation}
or unions of intervals of the above form. Examples of random variables
that are often taken to be continuous are:

\begin{itemize}
\item the height or weight of an individual,
\item other physical measurements such as the length or size of an object,
and
\item durations of time (usually).
\end{itemize}

Every continuous random variable \(X\) has a \emph{probability density
function} (PDF) denoted \(f_{X}\) associated with itthat
satisfies three basic properties:
\begin{enumerate}
\item \(f_{X}(x)>0\) for \(x\in S_{X}\),
\item \(\int_{x\in S_{X}}f_{X}(x)\,\mathrm{d} x=1\), and
\item \label{enu-contrvcond3} \(\mathbb{P}(X\in A)=\int_{x\in
   A}f_{X}(x)\:\mathrm{d} x\), for an event \(A\subset S_{X}\).
\end{enumerate}

\subsection{Expectation of Continuous Random Variables}
\label{sec-6-1-2}

For a continuous random variable \(X\) the expected value of \(g(X)\)
is
\begin{equation}
\mathbb{E} g(X)=\int_{x\in S}g(x)f_{X}(x)\:\mathrm{d} x,
\end{equation}
provided the (potentially improper) integral \(\int_{S}|g(x)|\,
f(x)\mathrm{d} x\) is convergent. One important example is the mean
\(\mu\), also known as \(\mathbb{E} X\):
\begin{equation}
\mu=\mathbb{E} X=\int_{x\in S}xf_{X}(x)\:\mathrm{d} x,
\end{equation}
provided \(\int_{S}|x|f(x)\mathrm{d} x\) is finite. Also there is the variance
\begin{equation}
\sigma^{2}=\mathbb{E}(X-\mu)^{2}=\int_{x\in S}(x-\mu)^{2}f_{X}(x)\,\mathrm{d} x,
\end{equation}
which can be computed with the alternate formula
\(\sigma^{2}=\mathbb{E} X^{2}-(\mathbb{E} X)^{2}\). In addition, there
is the standard deviation \(\sigma=\sqrt{\sigma^{2}}\).

\label{exa-cont-pdf3x2} Let the continuous random variable \(X\) have PDF
\[ f_{X}(x)=3x^{2},\quad 0\leq x\leq 1. \]
It is easy to
see that \(\int_{-\infty}^{\infty}f(x)\mathrm{d} x=1\).
\begin{align*}
\int_{-\infty}^{\infty}f_{X}(x)\mathrm{d} x & =\int_{0}^{1}3x^{2}\:\mathrm{d} x\\
 & =\left.x^{3}\right|_{x=0}^{1}\\
 & =1^{3}-0^{3}\\
 & =1.
\end{align*}
This being said, we may find \(\mathbb{P}(0.14\leq X<0.71)\).
\begin{align*}
\mathbb{P}(0.14\leq X<0.71) & =\int_{0.14}^{0.71}3x^{2}\mathrm{d} x,\\
 & =\left.x^{3}\right|_{x=0.14}^{0.71}\\
 & =0.71^{3}-0.14^{3}\\
 & \approx0.355167.
\end{align*}
We can find the mean and variance in an identical manner.
\begin{align*}
\mu=\int_{-\infty}^{\infty}xf_{X}(x)\mathrm{d} x & =\int_{0}^{1}x\cdot3x^{2}\:\mathrm{d} x,\\
 & =\frac{3}{4}x^{4}|_{x=0}^{1},\\
 & =\frac{3}{4}.
\end{align*}
It would perhaps be best to calculate the variance with the shortcut
formula \(\sigma^{2}=\mathbb{E} X^{2}-\mu^{2}\):
\begin{align*}
\mathbb{E} X^{2}=\int_{-\infty}^{\infty}x^{2}f_{X}(x)\mathrm{d} x & =\int_{0}^{1}x^{2}\cdot3x^{2}\:\mathrm{d} x\\
 & =\left.\frac{3}{5}x^{5}\right|_{x=0}^{1}\\
 & =3/5.
\end{align*}
which gives \(\sigma^{2}=3/5-(3/4)^{2}=3/80\).

\subsection{How to do it with \(\mathsf{R}\)}
\label{sec-6-1-2-1}

Let \(X\) have PDF \(f(x)=3x^{2}\), \(0<x<1\) and find
\(\mathbb{P}(0.14\leq X\leq0.71)\).

\begin{Verbatim}
f <- function(x) 3*x^2
integrate(f, lower = 0.14, upper = 0.71)
\end{Verbatim}

\begin{verbatim}
: 0.355167 with absolute error < 3.9e-15
\end{verbatim}

Compare this to the answer we found in Example \ref{exa-cont-pdf3x2}. We could
integrate the function \(x \cdot f(x)=\) \texttt{3*x\textasciicircum{}3} from zero to one to
get the mean, and use the shortcut \(\sigma^{2}=\mathbb{E}
X^{2}-\left(\mathbb{E} X\right)^{2}\) for the variance.

\section{The Continuous Uniform Distribution}
\label{sec-6-2}

A random variable \(X\) with the continuous uniform distribution on
the interval \((a,b)\) has PDF
\begin{equation}
f_{X}(x)=\frac{1}{b-a}, \quad a < x < b.
\end{equation}
The associated \(\mathsf{R}\) function is
\(\mathsf{dunif}(\mathtt{min}=a,\,\mathtt{max}=b)\). We write
\(X\sim\mathsf{unif}(\mathtt{min}=a,\,\mathtt{max}=b)\). Due to the
particularly simple form of this PDF we can also write down explicitly
a formula for the CDF \(F_{X}\): \begin{equation} \label{eq-unif-cdf} F_{X}(t) = \begin{cases} 0, & t < 0,\\ \frac{t-a}{b-a}, & a\leq t < b,\\ 1, & t \geq b. \end{cases} \end{equation}

The continuous uniform distribution is the continuous analogue of the
discrete uniform distribution; it is used to model experiments whose
outcome is an interval of numbers that are "equally likely" in the
sense that any two intervals of equal length in the support have the
same probability associated with them.


Choose a number in \( [0,1] \) at random, and let \(X\) be the number
chosen. Then \(X\sim\mathsf{unif}(\mathtt{min}=0,\,\mathtt{max}=1)\).
The mean of \(X\sim\mathsf{unif}(\mathtt{min}=a,\,\mathtt{max}=b)\) is
relatively simple to calculate:
\begin{align*}
\mu=\mathbb{E} X & =\int_{-\infty}^{\infty}x\, f_{X}(x)\,\mathrm{d} x,\\
 & =\int_{a}^{b}x\ \frac{1}{b-a}\ \mathrm{d} x,\\
 & =\left.\frac{1}{b-a}\ \frac{x^{2}}{2}\ \right|_{x=a}^{b},\\
 & =\frac{1}{b-a}\ \frac{b^{2}-a^{2}}{2},\\
 & =\frac{b+a}{2},
\end{align*}
using the popular formula for the difference of squares. The variance
is left to Exercise \ref{xca-variance-dunif}.

\section{The Normal Distribution}
\label{sec-6-3}

We say that \(X\) has a \emph{normal distribution} if it has PDF
\begin{equation}
f_{X}(x)=\frac{1}{\sigma\sqrt{2\pi}}\exp \left\{ \frac{-(x-\mu)^{2}}{2\sigma^{2}} \right\},\quad -\infty < x < \infty.
\end{equation}
We write
\(X\sim\mathsf{norm}(\mathtt{mean}=\mu,\,\mathtt{sd}=\sigma)\), and
the associated \(\mathsf{R}\) function is \texttt{dnorm(x, mean = $\mu$, sd = $\sigma$)}.

The familiar bell-shaped curve, the normal distribution is also known
as the \emph{Gaussian distribution} because the German mathematician
C. F. Gauss largely contributed to its mathematical development. This
distribution is by far the most important distribution, continuous or
discrete. The normal model appears in the theory of all sorts of
natural phenomena, from to the way particles of smoke dissipate in a
closed room, to the journey of a bottle in the ocean to the white
noise of cosmic background radiation.

When \(\mu=0\) and \(\sigma=1\) we say that the random variable has a
\emph{standard normal} distribution and we typically write
\(Z\sim\mathsf{norm}(\mathtt{mean}=0,\,\mathtt{sd}=1)\). The lowercase
Greek letter phi (\(\phi\)) is used to denote the standard normal PDF
and the capital Greek letter phi \(\Phi\) is used to denote the
standard normal CDF: for \(-\infty<z<\infty\),
\begin{equation}
\phi(z)=\frac{1}{\sqrt{2\pi}}\,\mathrm{e}^{-z^{2}/2}\mbox{ and }\Phi(t)=\int_{-\infty}^{t}\phi(z)\,\mathrm{d} z.
\end{equation}

\begin{prop}
If \(X\sim\mathsf{norm}(\mathtt{mean}=\mu,\,\mathtt{sd}=\sigma)\) then
\begin{equation}
Z=\frac{X-\mu}{\sigma}\sim\mathsf{norm}(\mathtt{mean}=0,\,\mathtt{sd}=1).
\end{equation}
\end{prop}

\subsubsection{The 68-95-99.7 Rule} 
When an
empirical distribution is approximately bell shaped there are specific
proportions of the observations which fall at varying distances from
the (sample) mean.
We can obtain precise proportions for normal distributions:


\begin{Verbatim}
pnorm(1:3) - pnorm(-(1:3))
\end{Verbatim}

\begin{verbatim}
: [1] 0.6826895 0.9544997 0.9973002
\end{verbatim}

\emph{Problem}:
\label{exa-iq-model} Let the random experiment consist of a person taking
an IQ test, and let \(X\) be the score on the test. The scores on such
a test are typically standardized to have a mean of 100 and a standard
deviation of 15, and IQ tests have (approximately and notoriously) a
bell-shaped distribution. What is \(\mathbb{P}(85\leq X\leq115)\)?

\emph{Solution}: this one is easy because the limits 85 and 115 fall
exactly one standard deviation (below and above, respectively) from
the mean of 100. The answer is therefore approximately 68\%.

\subsection{Normal Quantiles and the Quantile Function}
\label{sec-6-3-1}

Until now we have been given two values and our task has been to find
the area under the PDF between those values. In this section, we go in
reverse: we are given an area, and we would like to find the value(s)
that correspond to that area.

\label{exa-iq-quantile-state-problem} Assuming the IQ model of Example
\ref{exa-iq-model}, what is the lowest possible IQ score that a person can
have and still be in the top 1\% of all IQ scores?  \emph{Solution}: If a
person is in the top 1\%, then that means that 99\% of the people have
lower IQ scores. So, in other words, we are looking for a value \(x\)
such that \(F(x)=\mathbb{P}(X\leq x)\) satisfies \(F(x)=0.99\), or yet
another way to say it is that we would like to solve the equation
\(F(x)-0.99=0\). For the sake of argument, let us see how to do this
the long way. We define the function \(g(x)=F(x)-0.99\), and then look
for the root of \(g\) with the \texttt{uniroot} function. It uses numerical
procedures to find the root so we need to give it an interval of \(x\)
values in which to search for the root. We can get an educated guess
from the 68-95-99.7 Rule;
the root should be
somewhere between two and three standard deviations (15 each) above
the mean (which is 100).
\begin{Verbatim}
g <- function(x) pnorm(x, mean = 100, sd = 15) - 0.99
uniroot(g, interval = c(130, 145))
\end{Verbatim}

\begin{verbatim}
$root
[1] 134.8952

$f.root
[1] -4.873083e-09

$iter
[1] 6

$init.it
[1] NA

$estim.prec
[1] 6.103516e-05
\end{verbatim}

The answer is shown in \texttt{\$root} which is approximately 
134.8952, that is, a person with this IQ score or higher falls in the
top 1\% of all IQ scores.


The discussion in Example \ref{exa-iq-quantile-state-problem} was centered on the search for a
value \(x\) that solved an equation \(F(x)=p\), for some given
probability \(p\), or in mathematical parlance, the search for
\(F^{-1}\), the inverse of the CDF of \(X\), evaluated at \(p\). This
is so important that it merits a definition all its own.

\begin{defn}
The \emph{quantile function} of a random variable \(X\) is
the inverse of its cumulative distribution function:
\begin{equation}
Q_{X}(p)=\min\left\{ x:\ F_{X}(x)\geq p\right\} ,\quad 0 < p <1.
\end{equation}
\end{defn}

Quantile functions are defined for all of the base distributions with
the \texttt{q} prefix to the distribution name.
Back to Example \ref{exa-iq-quantile-state-problem}, we are looking for \(Q_{X}(0.99)\), where
\(X\sim\mathsf{norm}(\mathtt{mean}=100,\,\mathtt{sd}=15)\). It could
not be easier to do with \(\mathsf{R}\).

\begin{Verbatim}
qnorm(0.99, mean = 100, sd = 15)
\end{Verbatim}

\begin{verbatim}
: [1] 134.8952
\end{verbatim}

Compare this answer to the one obtained earlier with \texttt{uniroot}.

\section{Functions of Continuous Random Variables}
\label{sec-6-4}

The goal of this section is to determine the distribution of
\(U=g(X)\) based on the distribution of \(X\). In the discrete case
all we needed to do was back substitute for \(x=g^{-1}(u)\) in the PMF
of \(X\) (sometimes accumulating probability mass along the way). In
the continuous case, however, we need more sophisticated tools.

Using the CDF method, starting from the equation
\(F_{Y}(y)=\mathbb{P}(Y\leq y)\), we may substitute \(g(X)\) for
\(Y\), then solve for \(X\) to obtain \(\mathbb{P}[X\leq g^{-1}(y)]\),
which is just another way to write
\(F_{X}[g^{-1}(y)]\). Differentiating this last quantity with respect
to \(y\) will yield the PDF of \(Y\).


\emph{Problem}:
Suppose \(X\sim\mathsf{unif}(\mathtt{min}=0,\,\mathtt{max}=1)\) and
suppose that we let \(Y=-\ln\, X\). What is the PDF of \(Y\)?

The support set of \(X\) is \((0,1),\) and \(y\) traverses
\((0,\infty)\) as \(x\) ranges from \(0\) to \(1\), so the support set
of \(Y\) is \(S_{Y}=(0,\infty)\). For any \(y>0\), we consider \[
F_{Y}(y)=\mathbb{P}(Y\leq y)=\mathbb{P}(-\ln\, X\leq
y)=\mathbb{P}(X\geq\mathrm{e}^{-y})=1-\mathbb{P}(X<\mathrm{e}^{-y}),
\] where the next to last equality follows because the exponential
function is \emph{monotone} (this point will be revisited later). Now since
\(X\) is continuous the two probabilities
\(\mathbb{P}(X<\mathrm{e}^{-y})\) and
\(\mathbb{P}(X\leq\mathrm{e}^{-y})\) are equal; thus \[ 1-\mathbb{P}(X
<
\mathrm{e}^{-y})=1-\mathbb{P}(X\leq\mathrm{e}^{-y})=1-F_{X}(\mathrm{e}^{-y}).
\] Now recalling that the CDF of a
\(\mathsf{unif}(\mathtt{min}=0,\,\mathtt{max}=1)\) random variable
satisfies \(F(u)=u\) (see Equation \eqref{eq-unif-cdf}), we can say \[
F_{Y}(y)=1-F_{X}(\mathrm{e}^{-y})=1-\mathrm{e}^{-y},\quad \mbox{for
}y>0.  \] We have consequently found the formula for the CDF of \(Y\);
to obtain the PDF \(f_{Y}\) we need only differentiate \(F_{Y}\): \[
f_{Y}(y)=\frac{\mathrm{d}}{\mathrm{d}
y}\left(1-\mathrm{e}^{-y}\right)=0-\mathrm{e}^{-y}(-1), \] or
\(f_{Y}(y)=\mathrm{e}^{-y}\) for \(y>0\). This turns out to be a
member of the exponential family of distributions, see Section
\ref{sec-6-5}.

\emph{Problem}:
\label{exa-distn-of-z-squared} Let
\(Z\sim\mathsf{norm}(\mathtt{mean}=0,\,\mathtt{sd}=1)\) and let
\(U=Z^{2}\).
What is the PDF of \(U\)?

Notice first that
\(Z^{2}\geq0\), and thus the support of \(U\) is \([0,\infty)\). And
for any \(u\geq0\), \[ F_{U}(u)=\mathbb{P}(U\leq
u)=\mathbb{P}(Z^{2}\leq u).  \] But \(Z^{2}\leq u\) occurs if and only
if \(-\sqrt{u}\leq Z\leq\sqrt{u}\). The last probability above is
simply the area under the standard normal PDF from \(-\sqrt{u}\) to
\(\sqrt{u}\), and since \(\phi\) is symmetric about 0, we have \[
\mathbb{P}(Z^{2}\leq u)=2\mathbb{P}(0\leq
Z\leq\sqrt{u})=2\left[F_{Z}(\sqrt{u})-F_{Z}(0)\right]=2\Phi(\sqrt{u})-1,
\] because \(\Phi(0)=1/2\). To find the PDF of \(U\) we differentiate
the CDF recalling that \(\Phi'= \phi\).  \[
f_{U}(u)=\left(2\Phi(\sqrt{u})-1\right)'=2\phi(\sqrt{u})\cdot\frac{1}{2\sqrt{u}}=u^{-1/2}\phi(\sqrt{u}).
\] Substituting, \[ f_{U}(u) =
u^{-1/2}\frac{1}{\sqrt{2\pi}}\,\mathrm{e}^{-(\sqrt{u})^{2}/2}=(2\pi
u)^{-1/2}\mathrm{e}^{-u/2},\quad u > 0.  \] This is what we will later
call a \emph{chi-square distribution with 1 degree of freedom}. See Section
\ref{sec-6-5}.

\begin{fact}
\label{fac-lin-trans-norm-is-norm}
If \(X\sim\mathsf{norm}(\mathtt{mean}=\mu,\,\mathtt{sd}=\sigma)\) and if \(Y=a+bX\) for constants \(a\) and \(b\), with \(b\neq0\), then \(Y\sim\mathsf{norm}(\mathtt{mean}=a+b\mu,\,\mathtt{sd}=|b|\sigma)\).
\end{fact}

\begin{proof}
Please exercise.
\end{proof}

\section{Other Continuous Distributions}
\label{sec-6-5}

\subsection{Waiting Time Distributions}
\label{sec-6-5-1}

In some experiments, the random variable being measured is the time
until a certain event occurs. For example, a quality control
specialist may be testing a manufactured product to see how long it
takes until it fails. An efficiency expert may be recording the
customer traffic at a retail store to streamline scheduling of staff.

\subsubsection{The Exponential Distribution}
\label{sec-6-5-1-1}

We say that \(X\) has an \emph{exponential distribution} and write
\(X\sim\mathsf{exp}(\mathtt{rate}=\lambda)\).
\begin{equation}
f_{X}(x)=\lambda\mathrm{e}^{-\lambda x},\quad x>0
\end{equation}
The associated \(\mathsf{R}\) functions are \texttt{dexp(x, rate = 1)},
\texttt{pexp}, \texttt{qexp}, and \texttt{rexp}, which give the PDF, CDF, quantile
function, and simulate random variates, respectively.

The parameter \(\lambda\) measures the rate of arrivals (to be
described later) and must be positive. The CDF is given by the formula
\begin{equation}
F_{X}(t)=1-\mathrm{e}^{-\lambda t},\quad t>0.
\end{equation}
The mean is \(\mu=1/\lambda\) and the variance is
\(\sigma^{2}=1/\lambda^{2}\).

The exponential distribution is closely related to the Poisson
distribution. If customers arrive at a store according to a Poisson
process with rate \(\lambda\) and if \(Y\) counts the number of
customers that arrive in the time interval \([0,t)\), then we know that \( Y \sim
\mathsf{pois}(\mathtt{lambda}=\lambda t). \) Now consider a different
question: let us start our clock at time 0 and stop the clock when the
first customer arrives. Let \(X\) be the length of this random time
interval. Then \(X\sim\mathsf{exp}(\mathtt{rate}=\lambda)\). Observe
the following string of equalities:
\begin{align*}
\mathbb{P}(X>t) & =\mathbb{P}(\mbox{first arrival after time \emph{t}}),\\
 & =\mathbb{P}(\mbox{no events in [0,\emph{t})}),\\
 & =\mathbb{P}(Y=0),\\
 & =\mathrm{e}^{-\lambda t},
\end{align*}
where the last line is the PMF of \(Y\) evaluated at \(y=0\). In other
words, \(\mathbb{P}(X\leq t)=1-\mathrm{e}^{-\lambda t}\), which is
exactly the CDF of an \(\mathsf{exp}(\mathtt{rate}=\lambda)\)
distribution.

The exponential distribution is said to be \emph{memoryless} because
exponential random variables "forget" how old they are at every
instant. That is, the probability that we must wait an additional five
hours for a customer to arrive, given that we have already waited
seven hours, is exactly the probability that we needed to wait five
hours for a customer in the first place. In mathematical symbols, for
any \(s,\, t>0\),
\begin{equation}
\mathbb{P}(X>s+t\,|\, X>t)=\mathbb{P}(X>s).
\end{equation}
See Exercise \ref{xca-prove-the-memoryless}.

\subsubsection{The Gamma Distribution}
\label{sec-6-5-1-2}

This is a generalization of the exponential distribution. We say that
\(X\) has a gamma distribution and write
\(X\sim\mathsf{gamma}(\mathtt{shape}=\alpha,\,\mathtt{rate}=\lambda)\). It
has PDF
\begin{equation}
f_{X}(x)=\frac{\lambda^{\alpha}}{\Gamma(\alpha)}\: x^{\alpha-1}\mathrm{e}^{-\lambda x},\quad x>0.
\end{equation}

The associated \(\mathsf{R}\) functions are \texttt{dgamma(x, shape, rate =
1)}, \texttt{pgamma}, \texttt{qgamma}, and \texttt{rgamma}, which give the PDF, CDF,
quantile function, and simulate random variates, respectively. If
\(\alpha=1\) then \(X\sim\mathsf{exp}(\mathtt{rate}=\lambda)\). The
mean is \(\mu=\alpha/\lambda\) and the variance is
\(\sigma^{2}=\alpha/\lambda^{2}\).

To motivate the gamma distribution recall that if \(X\) measures the
length of time until the first event occurs in a Poisson process with
rate \(\lambda\) then \(X\sim\mathsf{exp}(\mathtt{rate}=\lambda)\). If
we let \(Y\) measure the length of time until the
\(\alpha^{\mathrm{th}}\) event occurs then
\(Y\sim\mathsf{gamma}(\mathtt{shape}=\alpha,\,\mathtt{rate}=\lambda)\). When
\(\alpha\) is an integer this distribution is also known as the
\emph{Erlang} distribution.


At a car wash, two customers arrive per hour on the average. We decide
to measure how long it takes until the third customer arrives. If
\(Y\) denotes this random time then
\(Y\sim\mathsf{gamma}(\mathtt{shape}=3,\,\mathtt{rate}=1/2)\).

\subsection{The Chi square, Student's \(t\), and Snedecor's \(F\) Distributions}
\label{sec-6-5-2}

\subsubsection{The Chi square Distribution}
\label{sec-6-5-2-1}

A random variable \(X\) with PDF
\begin{equation}
f_{X}(x)=\frac{1}{\Gamma(p/2)2^{p/2}}x^{p/2-1}\mathrm{e}^{-x/2},\quad x>0,
\end{equation}
is said to have a \emph{chi-square distribution} with \(p\) \emph{degrees of
freedom}. We write \(X\sim\mathsf{chisq}(\mathtt{df}=p)\). The
associated \(\mathsf{R}\) functions are \texttt{dchisq(x, df)}, \texttt{pchisq},
\texttt{qchisq}, and \texttt{rchisq}, which give the PDF, CDF, quantile function,
and simulate random variates, respectively. See Figure
\ref{fig-chisq-dist-vary-df}, which is produced by:

\begin{Verbatim}
curve(dchisq(x, df = 3), from = 0, to = 20, ylab = "y")
ind <- c(4, 5, 10, 15)
for (i in ind) curve(dchisq(x, df = i), 0, 20, add = TRUE)
\end{Verbatim}

\begin{figure}[ht!]
\centering
\includegraphics[width=0.9\textwidth]{fig/contdist-chisq-dist-vary-df.ps}
\caption[Chi square distribution for various degrees of freedom]{\label{fig-chisq-dist-vary-df}\small The chi square distribution for various degrees of freedom.}
\end{figure}

\begin{rem}
Here are some useful things to know about the chi-square distribution.
\begin{enumerate}
\item If \(Z\sim\mathtt{norm}(\mathtt{mean}=0,\,\mathtt{sd}=1)\), then
\(Z^{2}\sim\mathsf{chisq}(\mathtt{df}=1)\). We saw this in Example
\ref{exa-distn-of-z-squared}, and the fact is important when it
comes time to find the distribution of the sample variance,
\(S^{2}\).
\item The chi-square distribution is supported on the positive
\(x\)-axis, with a right-skewed distribution.
\item The \(\mathsf{chisq}(\mathtt{df}=p)\) distribution is the same as a
\(\mathsf{gamma}(\mathtt{shape}=p/2,\,\mathtt{rate}=1/2)\)
distribution.
\end{enumerate}
\end{rem}

\subsubsection{Student's \(t\) distribution}
\label{sec-6-5-2-2}

This distribution was first published by W. S. Gosset (1900) under the pseudonym Student, and the distribution has consequently come to be known as Student's \(t\) distribution.

A random variable \(X\) with PDF
\begin{equation}
f_{X}(x) = \frac{\Gamma\left[ (r+1)/2\right] }{\sqrt{r\pi}\,\Gamma(r/2)}\left( 1 + \frac{x^{2}}{r} \right)^{-(r+1)/2},\quad -\infty < x < \infty
\end{equation}
is said to have \emph{Student's} \(t\) distribution with \(r\) \emph{degrees of
freedom}, and we write \(X\sim\mathsf{t}(\mathtt{df}=r)\). The
associated \(\mathsf{R}\) functions are \texttt{dt(x, df)}, \texttt{pt}, \texttt{qt}, and \texttt{rt},
which give the PDF, CDF, quantile function, and simulate random
variates, respectively.

The shape of the Student's \(t\) distribution is similar to the normal, but the tails are considerably heavier.
See Figure \ref{fig-Students-t-dist-vary-df}.
The code to produce Figure \ref{fig-Students-t-dist-vary-df} is

\begin{Verbatim}
curve(dt(x, df = 30), from = -3, to = 3, lwd = 3, ylab = "y")
ind <- c(1, 2, 3, 5, 10)
for (i in ind) curve(dt(x, df = i), -3, 3, add = TRUE)
\end{Verbatim}

\begin{figure}[ht!]
\centering
\includegraphics[width=0.9\textwidth]{fig/sampdist-Students-t-dist-vary-df.ps}
\caption[Student's \(t\) distribution for various degrees of freedom]
{
\label{fig-Students-t-dist-vary-df}
\small
A plot of Student's \(t\) distribution for various degrees of freedom.
}
\end{figure}

\begin{rem}
There are a few things to note about the \(\mathtt{t}(\mathtt{df}=r)\) distribution.
\begin{enumerate}
\item
The \(\mathtt{t}(\mathtt{df}=r)\) distribution can be defined as the distribution of the random variable:
\[ T=\frac{Z}{\sqrt{V/r}}, \]
where $Z\sim\mathsf{norm}(\mathtt{mean}=0,\,\mathtt{sd}=1)$ and $V\sim\mathsf{chisq}(\mathtt{df}=r)$ are independent.
\item
The standard deviation of \(\mathsf{t}(\mathtt{df}=r)\) is undefined (that is, infinite) unless \(r>2\).
When \(r\) is more than 2, the standard deviation is always bigger than one, but decreases to 1 as \(r\to\infty\).
\item
As \(r\to\infty\), the \(\mathtt{t}(\mathtt{df}=r)\) distribution approaches the \(\mathsf{norm}(\mathtt{mean}=0,\,\mathtt{sd}=1)\) distribution.
\end{enumerate}
\end{rem}

\subsubsection{Snedecor's \(F\) distribution}
\label{sec-6-5-2-3}

A random variable \(X\) with PDF
\begin{equation}
f_{X}(x)=\frac{\Gamma[(m+n)/2]}{\Gamma(m/2)\Gamma(n/2)}\left(\frac{m}{n}\right)^{m/2}x^{m/2-1}\left(1+\frac{m}{n}x\right)^{-(m+n)/2},\quad x>0.
\end{equation}
is said to have an \(F\) distribution with \((m,n)\) degrees of
freedom. We write
\(X\sim\mathsf{f}(\mathtt{df1}=m,\,\mathtt{df2}=n)\). The associated
\(\mathsf{R}\) functions are \texttt{df(x, df1, df2)}, \texttt{pf}, \texttt{qf}, and \texttt{rf},
which give the PDF, CDF, quantile function, and simulate random
variates, respectively.

\begin{rem}
Here are some notes about the \(F\) distribution.
\begin{enumerate}
\item
A random variable of the $F$ distribution with parameters $\mathtt{df1}=m$ and $\mathtt{df2}=n$ arises as the ratio of two appropriately scaled chi-squared variates:
\[ X=\frac{U/m}{V/n} \]
where $U$ and $V$ have independent chi-square distributions with $m$ and $n$ degrees of freedom, respectively.
\item If \(X\sim\mathsf{f}(\mathtt{df1}=m,\,\mathtt{df2}=n)\) and
\(Y=1/X\), then
\(Y\sim\mathsf{f}(\mathtt{df1}=n,\,\mathtt{df2}=m)\). Historically,
this fact was especially convenient. In the old days, statisticians
used printed tables for their statistical calculations. Since the
\(F\) tables were symmetric in \(m\) and \(n\), it meant that
publishers could cut the size of their printed tables in half. It
plays less of a role today now that personal computers are
widespread.
\item If \(X\sim\mathsf{t}(\mathtt{df}=r)\), then
\(X^{2}\sim\mathsf{f}(\mathtt{df1}=1,\,\mathtt{df2}=r)\).
\end{enumerate}
\end{rem}

\newpage{}

\section{Exercises}
\label{sec-6-6}
\setcounter{thm}{0}

\begin{xca}
Find \(C\) so that function $f(x) = Cx^{n}$ is a valid PDF of a random variable $X\in(0, 1)$.
\end{xca}

\begin{xca}
For the following random experiments, decide what the distribution of
\(X\) should be. In nearly every case, there are additional
assumptions that should be made for the distribution to apply;
identify those assumptions (which may or may not strictly hold in
practice).
\begin{enumerate}
\item We throw a dart at a dart board. Let \(X\) denote the squared
linear distance from the bulls-eye to the where the dart landed.
\item We randomly choose a textbook from the shelf at the bookstore and
let \(P\) denote the proportion of the total pages of the book
devoted to exercises.
\item We measure the time it takes for the water to completely drain out
of the kitchen sink.
\item We randomly sample strangers at the grocery store and ask them how
long it will take them to drive home.
\end{enumerate}
\end{xca}

\begin{xca}
If \(Z\) is \(\mathsf{norm}(\mathtt{mean} = 0,\,\mathtt{sd} = 1)\), find 
\begin{enumerate}
\item \(\mathbb{P}(Z > 2.64)\)
\item \(\mathbb{P}(0 \leq Z < 0.87)\)
\item \(\mathbb{P}(|Z| > 1.39)\) (Hint: draw a picture!)
\end{enumerate}
\end{xca}

\begin{xca}
\label{xca-variance-dunif} Calculate the variance of
\(X\sim\mathsf{unif}(\mathtt{min}=a,\,\mathtt{max}=b)\). \emph{Hint:} First
calculate \(\mathbb{E} X^{2}\).
\end{xca}

\begin{xca}
\label{xca-prove-the-memoryless} Prove the memoryless property for
exponential random variables. That is, for \(X \sim
\mathsf{exp}(\mathtt{rate} = \lambda)\) show that for any \(s,t > 0\),
\[ \mathbb{P}(X > s + t\,|\, X > t) = \mathbb{P}(X > s).  \]
\end{xca}
