\chapter{Discrete Distributions}
\label{sec-5}

\noindent
In this chapter we introduce discrete random variables, those who take
values in a finite or countably infinite support set. We discuss
probability mass functions and some special expectations, namely, the
mean, variance and standard deviation. Some of the more important
discrete distributions are explored in detail.
There are some comments on simulation, and we mention transformations
of random variables in the discrete case.

\textbf{Highlights:}
\begin{itemize}
\item how to choose a reasonable discrete model under a variety of
real-world circumstances
\item the notion, calculation, and properties of probability expectation
\item some details on a couple of discrete models, and exposure to a bunch
of other ones
\item how to make new discrete random variables from old ones
\end{itemize}

\section{Discrete Random Variables}
\label{sec-5-1}

\subsection{Probability Mass Functions}
\label{sec-5-1-1}

Discrete random variables are characterized by their supports which
take the form
\begin{equation}
S_{X}=\{u_{1},u_{2},\ldots,u_{k}\}\mbox{ or }S_{X}=\{u_{1},u_{2},u_{3}\ldots\}.
\end{equation}

Every discrete random variable \(X\) has associated with it a
probability mass function (PMF) \(f_{X}:S_{X}\to[0,1]\) defined by
\begin{equation}
f_{X}(x)=\mathbb{P}(X=x),\quad x\in S_{X}.
\end{equation}

Since values of the PMF represent probabilities, PMFs enjoy certain properties:
\begin{enumerate}
\item \(f_{X}(x)>0\) for \(x\in S\),
\item \(\sum_{x\in S}f_{X}(x)=1\), and
\item \(\mathbb{P}(X\in A)=\sum_{x\in A}f_{X}(x)\), for any event
\(A\subset S\).
\end{enumerate}

\subsubsection{Example: toss a coin 3 times}
\label{exa-Toss-a-coin}
If we toss a coin 3 times, the sample space would be \[
S=\{ HHH,\ HTH,\ THH,\ TTH,\ HHT,\ HTT,\ THT,\ TTT\}.  \]

Now let \(X\) be the number of Heads observed. Then \(X\) has support
\(S_{X}=\{ 0,1,2,3\} \). Assuming that the coin is fair and was tossed
in exactly the same way each time, it is not unreasonable to suppose
that the outcomes in the sample space are all equally likely.

What is the PMF of \(X\)? Notice that \(X\) is zero exactly when the
outcome \(TTT\) occurs, and this event has probability
\(1/8\). Therefore, \(f_{X}(0)=1/8\), and the same reasoning shows
that \(f_{X}(3)=1/8\). Exactly three outcomes result in \(X=1\), thus,
\(f_{X}(1)=3/8\) and \(f_{X}(2)\) holds the remaining \(3/8\)
probability (the total is 1). We can represent the PMF with a table:

\begin{table}[htb]
\caption[Flipping a coin three times]{\label{tab-pmf-flip-coin-three}Flipping a coin three times: the PMF.}
\centering
\begin{tabular}{lrrrrr}
\(x\in S_{X}\) & 0 & 1 & 2 & 3 & Total\\
\hline
\(f_{X}(x)=\mathbb{P}(X=x)\) & 1/8 & 3/8 & 3/8 & 1/8 & 1\\
\end{tabular}
\end{table}

\subsection{Mean, Variance, and Standard Deviation}
\label{sec-5-1-2}

There are numbers associated with PMFs. One important example is the
mean \(\mu\), also known as \(\mathbb{E} X\) (which we will discuss
later):
\begin{equation}
\mu=\mathbb{E} X=\sum_{x\in S}xf_{X}(x),
\end{equation}
provided the (potentially infinite) series \(\sum|x|f_{X}(x)\) is convergent. Another important number is the variance:
\begin{equation}
\sigma^{2}=\mathbb{E}(X-\mu)^2=\sum_{x\in S}(x-\mu)^{2}f_{X}(x),
\end{equation}
which can be computed with the alternate formula
\[ \sigma^{2}=\mathbb{E}X^2-\mu^2=\sum x{}^{2}f_{X}(x)-\mu^{2} .\]

Directly defined from the variance is the
standard deviation \(\sigma=\sqrt{\sigma^{2}}\).

\subsubsection{Exercise: show $\mathbb{E}(X-\mathbb{E}X)^2=\mathbb{E}X^2-(\mathbb{E}X)^2$}

\subsubsection{Example: toss a coin 3 times}

\label{exa-disc-pmf-mean} We will calculate the mean of \(X\) in Example
\ref{exa-Toss-a-coin}.  \[ \mu = \sum_{x = 0}^{3}xf_{X}(x) = 0 \cdot
\frac{1}{8} + 1 \cdot \frac{3}{8} + 2 \cdot
\frac{3}{8}+3\cdot\frac{1}{8} = 1.5. \] We interpret \(\mu = 1.5\) by
reasoning that if we were to repeat the random experiment many times,
independently each time, observe many corresponding outcomes of the
random variable \(X\), and take the sample mean of the observations,
then the calculated value would fall close to 1.5. The approximation
would get better as we observe more and more values of \(X\) (another
form of the Law of Large Numbers).
Another way it is commonly stated is
that \(X\) is 1.5 "on the average" or "in the long run".


\begin{rem}
Note that although we say \(X\) is 1.5 on the average, we must keep in mind that our \(X\) never actually equals 1.5.
By definition, it is impossible for \(X\) to equal 1.5.
\end{rem}

\subsection{Cumulative Distribution Functions}

Related to the probability mass function \(f_{X}(x)=\mathbb{P}(X=x)\)
is another important function called the \emph{cumulative distribution
function} (CDF), \(F_{X}\). It is defined by the formula
\begin{equation}
F_{X}(t)=\mathbb{P}(X\leq t),\quad -\infty < t < \infty.
\end{equation}

We know that all PMFs satisfy certain properties, and a similar
statement may be made for CDFs. In particular, any CDF \(F_{X}\)
satisfies
\begin{itemize}
\item \(F_{X}\) is nondecreasing (\(t_{1}\leq t_{2}\) implies
\(F_{X}(t_{1})\leq F_{X}(t_{2})\)).
\item \(F_{X}\) is right-continuous (\(\lim_{t\to
  a^{+}}F_{X}(t)=F_{X}(a)\) for all \(a\in\mathbb{R}\)).
\item \(\lim_{t\to-\infty}F_{X}(t)=0\) and
\(\lim_{t\to\infty}F_{X}(t)=1\).
\end{itemize}
We say that \(X\) has the distribution \(F_{X}\) and we write \(X\sim
F_{X}\). In an abuse of notation we will also write \(X\sim f_{X}\)
and for the named distributions the PMF or CDF will be identified by
the family name instead of the defining formula.

\subsection{How to do it with \(\mathsf{R}\)}
\label{sec-5-1-2-1}

The mean and variance of a discrete random variable is easy to compute
at the console. Let's return to Example \ref{exa-disc-pmf-mean}. We will start
by defining a vector \texttt{x} containing the support of \(X\), and a vector
\texttt{f} to contain the values of \(f_{X}\) at the respective outcomes in
\texttt{x}:

\begin{Verbatim}
x <- c(0,1,2,3)
f <- c(1/8, 3/8, 3/8, 1/8)
\end{Verbatim}

To calculate the mean \(\mu\), we need to multiply the corresponding
values of \texttt{x} and \texttt{f} and add them. This is easily accomplished in
\(\mathsf{R}\) since operations on vectors are performed
\emph{element-wise}:

\begin{Verbatim}
mu <- sum(x * f)
mu
\end{Verbatim}

\begin{verbatim}
: [1] 1.5
\end{verbatim}

To compute the variance \(\sigma^{2}\), we subtract the value of \texttt{mu}
from each entry in \texttt{x}, square the answers, multiply by \texttt{f}, and
\texttt{sum}.

\begin{Verbatim}
sigma2 <- sum((x-mu)^2 * f)
sigma2
\end{Verbatim}

\begin{verbatim}
: [1] 0.75
\end{verbatim}

The standard deviation \(\sigma\) is simply the square root of \(\sigma^{2}\).

\begin{Verbatim}
sigma <- sqrt(sigma2)
sigma
\end{Verbatim}

\begin{verbatim}
: [1] 0.8660254
\end{verbatim}

Finally, we may find the values of the CDF \(F_{X}\) on the support by
accumulating the probabilities in \(f_{X}\) with the \texttt{cumsum}
function.

\begin{Verbatim}
F <- cumsum(f)
F
\end{Verbatim}

\begin{verbatim}
: [1] 0.125 0.500 0.875 1.000
\end{verbatim}

\section{The Binomial Distribution}
\label{sec-5-3}

We have seen the basic building blocks of discrete distributions and
we now study particular models that statisticians often encounter in
the field.
Perhaps the most fundamental of all is the \emph{binomial} distribution.

The binomial distribution is based on a \emph{Bernoulli trial}, which is a
random experiment in which there are only two possible outcomes:
success (\(S\)) and failure (\(F\)). We conduct the Bernoulli trial
and let \begin{equation} X = \begin{cases} 1 & \mbox{if the outcome is $S$},\\ 0 & \mbox{if the outcome is $F$}. \end{cases} \end{equation}
If the probability of success is \(p\) then the probability of failure
must be \(1-p=q\) and the PMF of \(X\) is
\begin{equation}
f_{X}(x)=p^{x}(1-p)^{1-x},\quad x=0,1.
\end{equation}
It is easy to calculate \(\mu=\mathbb{E} X=p\) and \(\mathbb{E}
X^{2}=p\) so that \(\sigma^{2}=p-p^{2}=p(1-p)\).

\subsection{The Binomial Model}
\label{sec-5-3-1}

The Binomial model has three defining properties:
\begin{itemize}
\item Bernoulli trials are conducted \(n\) times,
\item the trials are independent,
\item the probability of success \(p\) does not change between trials.
\end{itemize}
If \(X\) counts the number of successes in the \(n\) independent
trials, then the PMF of \(X\) is 
\begin{equation}
f_{X}(x)={n \choose x}p^{x}(1-p)^{n-x},\quad x=0,1,2,\ldots,n.
\end{equation}

We say that \(X\) has a \emph{binomial distribution} and we write
\(X\sim\mathsf{binom}(\mathtt{size}=n,\,\mathtt{prob}=p)\). It is
clear that \(f_{X}(x)\geq0\) for all \(x\) in the support because the
value is the product of nonnegative numbers. We next check that \(\sum
f(x)=1\): \[ \sum_{x = 0}^{n}{n \choose x} p^{x} (1 - p)^{n - x} =
[p + (1 - p)]^{n} = 1^{n} = 1.  \] We next find the mean:
\begin{alignat*}{1}
\mu= & \sum_{x=0}^{n}x\,{n \choose x}p^{x}(1-p)^{n-x},\\
= & \sum_{x=1}^{n}x\,\frac{n!}{x!(n-x)!}p^{x}q^{n-x},\\
= & n\cdot p\sum_{x=1}^{n}\frac{(n-1)!}{(x-1)!(n-x)!}p^{x-1}q^{n-x},\\
= & np\,\sum_{x-1=0}^{n-1}{n-1 \choose x-1}p^{(x-1)}(1-p)^{(n-1)-(x-1)},\\
= & np.
\end{alignat*}
A similar argument shows that \(\mathbb{E} X(X - 1) = n(n - 1)p^{2}\).
Therefore
\begin{alignat*}{1}
\sigma^{2}= & \mathbb{E} X(X-1)+\mathbb{E} X-[\mathbb{E} X]^{2},\\
= & n(n-1)p^{2}+np-(np)^{2},\\
= & n^{2}p^{2}-np^{2}+np-n^{2}p^{2},\\
= & np-np^{2}=np(1-p).
\end{alignat*}


\subsubsection{Example: a four-child family}
In a four-child family, each child may be either a boy (\(B\)) or a girl
(\(G\)). For simplicity we suppose that
\(\mathbb{P}(B)=\mathbb{P}(G)=1/2\) and that the genders of the
children are determined independently. If we let \(X\) count the
number of \(B\)'s, then
\(X\sim\mathsf{binom}(\mathtt{size}=4,\,\mathtt{prob}=1/2)\). Further,
\(\mathbb{P}(X=2)\) is
\[
f_{X}(2)={4 \choose 2}(1/2)^{2}(1/2)^{2}=\frac{6}{2^{4}}.
\]
The mean number of boys is \(4(1/2)=2\) and the variance of \(X\) is
\(4(1/2)(1/2)=1\).

\subsection{How to do it with \(\mathsf{R}\)}
\label{sec-5-3-1-1}

The corresponding \(\mathsf{R}\) function for the PMF and CDF are
\texttt{dbinom} and \texttt{pbinom}, respectively. We demonstrate their use in the
following examples.

Roll 12 dice simultaneously, and let \(X\) denote the number of 6's
that appear. We wish to find the probability of getting seven, eight,
or nine 6's. If we let \(S=\{ \mbox{get a 6 on one roll} \} \), then
\(\mathbb{P}(S)=1/6\) and the rolls constitute Bernoulli trials; thus
\(X\sim\mathsf{binom}(\mathtt{size}=12,\ \mathtt{prob}=1/6)\) and our
task is to find \(\mathbb{P}(7\leq X\leq9)\). This is just
\[ 
\mathbb{P}(7\leq X\leq9)=\sum_{x=7}^{9}{12 \choose x}(1/6)^{x}(5/6)^{12-x}.
\]
Again, one method to solve this problem would be to generate a
probability mass table and add up the relevant rows. However, an
alternative method is to notice that \(\mathbb{P}(7\leq
X\leq9)=\mathbb{P}(X\leq9)-\mathbb{P}(X\leq6)=F_{X}(9)-F_{X}(6)\), so
we could get the same answer by:

\begin{Verbatim}
pbinom(9, size=12, prob=1/6) - pbinom(6, size=12, prob=1/6)
diff(pbinom(c(6,9), size = 12, prob = 1/6))  # same thing
sum(dbinom(7:9, size = 12, prob = 1/6))      # same thing
\end{Verbatim}

\begin{verbatim}
: [1] 0.001291758
\end{verbatim}



\label{exa-toss-coin-3-withR} Toss a coin three times and let \(X\) be the
number of Heads observed. We know from before that
\(X\sim\mathsf{binom}(\mathtt{size}=3,\,\mathtt{prob}=1/2)\) which
implies the following PMF:

\begin{table}[htb]
\caption[Flipping a coin thrice: PMF]{\label{tab-flip-coin-thrice}Flipping a coin three times: the PMF.}
\centering
\begin{tabular}{lrrrrr}
\(x=\mbox{num. of Heads}\) & 0 & 1 & 2 & 3 & Total\\
\hline
\(f(x) = \mathbb{P}(X = x)\) & 1/8 & 3/8 & 3/8 & 1/8 & 1\\
\end{tabular}
\end{table}

Our next goal is to write down the CDF of \(X\) explicitly. The first
case is easy: it is impossible for \(X\) to be negative, so if \(x<0\)
then we should have \(\mathbb{P}(X\leq x)=0\). Now choose a value
\(x\) satisfying \(0\leq x<1\), say, \(x=0.3\). The only way that
\(X\leq x\) could happen would be if \(X=0\), therefore,
\(\mathbb{P}(X\leq x)\) should equal \(\mathbb{P}(X=0)\), and the same
is true for any \(0\leq x<1\). Similarly, for any \(1\leq x<2\), say,
\(x=1.73\), the event \(\{ X\leq x \}\) is exactly the event \(\{
X=0\mbox{ or }X=1 \}\). Consequently, \(\mathbb{P}(X\leq x)\) should
equal \(\mathbb{P}(X=0\mbox{ or
}X=1)=\mathbb{P}(X=0)+\mathbb{P}(X=1)\). Continuing in this fashion,
we may figure out the values of \(F_{X}(x)\) for all possible inputs
\(-\infty<x<\infty\), and we may summarize our observations with the
following piecewise defined function: \[ F_{X}(x)=\mathbb{P}(X\leq x) = \begin{cases} 0, & x < 0,\\ \frac{1}{8}, & 0\leq x < 1,\\ \frac{1}{8} + \frac{3}{8} = \frac{4}{8}, & 1\leq x < 2,\\ \frac{4}{8} + \frac{3}{8} = \frac{7}{8}, & 2\leq x < 3,\\ 1, & x \geq 3. \end{cases} \]
In particular, the CDF of \(X\) is defined for the entire real line,
\(\mathbb{R}\). The CDF is right continuous and nondecreasing. A graph
of the \(\mathsf{binom}(\mathtt{size}=3,\,\mathtt{prob}=1/2)\) CDF is
shown in Figure \ref{fig-binom-cdf-base}.

\begin{figure}[ht!]
\centering
\includegraphics[width=0.9\textwidth]{fig/discdist-binom-cdf-base.ps}
\caption[Graph of the \(\mathsf{binom}(\mathtt{size}=3,\,\mathtt{prob}=1/2)\) CDF]{\label{fig-binom-cdf-base}\small A graph of the \(\mathsf{binom}(\mathtt{size}=3,\,\mathtt{prob}=1/2)\) CDF.}
\end{figure}

\section{The Empirical and Uniform Distribution}
\label{sec-5-5}

Do an experiment \(n\) times and observe \(n\) values \(x_{1}\),
\(x_{2}\), \ldots{}, \(x_{n}\) of a random variable \(X\). For simplicity
in most of the discussion that follows it will be convenient to
imagine that the observed values are distinct, but the remarks are
valid even when the observed values are repeated.

\subsection{The Empirical Distribution}

\begin{defn}
The \emph{empirical distribution} \index{Empirical distribution} is the probability distribution
that places probability mass \(1/n\) on each of the values \(x_{1}\),
\(x_{2}\), \ldots{}, \(x_{n}\). The empirical PMF takes the form
\begin{equation} 
f_{X}(x)=\frac{1}{n},\quad x\in \{ x_{1},x_{2},...,x_{n} \}.
\end{equation}
If the value \(x_{i}\) is repeated \(k\) times, the mass at \(x_{i}\) is accumulated to \(k/n\).
\end{defn}

The mean of the empirical distribution is
\begin{equation}
\mu=\sum_{x\in S}xf_{X}(x)=\sum_{i=1}^{n}x_{i}\cdot\frac{1}{n}
\end{equation}
and we recognize this last quantity to be the sample mean, \(\overline{x}\). The variance of the empirical distribution is
\begin{equation}
\sigma^{2}=\sum_{x\in S}(x-\mu)^{2}f_{X}(x)=\sum_{i=1}^{n}(x_{i}-\overline{x})^{2}\cdot\frac{1}{n}
\end{equation}
and this last quantity looks very close to what we already know to be the sample variance.
\begin{equation}
s^{2}=\frac{1}{n-1}\sum_{i=1}^{n}(x_{i}-\overline{x})^{2}.
\end{equation}

\subsection{The Uniform Distribution}

The discrete uniform distribution is a special instance of the empirical distribution.
A random variable \(X\) with the discrete uniform distribution on the
integers \(1,2,\ldots,m\) has PMF
\begin{equation}
f_{X}(x)=\frac{1}{m},\quad x=1,2,\ldots,m.
\end{equation}

We write \(X\sim\mathsf{disunif}(m)\). A random experiment where this
distribution occurs is the choice of an integer at random between 1
and 100, inclusive. Let \(X\) be the number chosen. Then
\(X\sim\mathsf{disunif}(m=100)\) and
\[
\mathbb{P}(X=x)=\frac{1}{100},\quad x=1,\ldots,100.
\]
We find a direct formula for the mean of \(X\sim\mathsf{disunif}(m)\):
\begin{equation}
\mu = \sum_{x = 1}^{m}xf_{X}(x) = \sum_{x = 1}^{m}x \cdot \frac{1}{m} = \frac{1}{m}(1 + 2 + \cdots + m) = \frac{m + 1}{2},
\end{equation}
where we have used the famous identity \(1 + 2 + \cdots + m = m(m +
1)/2\). That is, if we repeatedly choose integers at random from 1 to
\(m\) then, on the average, we expect to get \((m+1)/2\). To get the
variance we first calculate \[ \sum_{x = 1}^{m} x^{2} f_{X}(x) =
\frac{1}{m} \sum_{x = 1}^{m} x^{2} = \frac{1}{m}\frac{m(m + 1)(2m +
1)}{6} = \frac{(m + 1)(2m + 1)}{6}, \] and finally,
\begin{equation}
\sigma^{2} = \sum_{x = 1}^{m} x^{2} f_{X}(x) - \mu^{2} = \frac{(m + 1)(2m + 1)}{6} - \left(\frac{m + 1}{2}\right)^{2} = \cdots = \frac{m^{2} - 1}{12}.
\end{equation}

Roll a die and  let \(X\) be the upward face showing.  Then \(m = 6\),
\(\mu = 7/2 = 3.5\), and \(\sigma^{2} = (6^{2} - 1)/12 = 35/12\).

\subsection{How to do it with \(\mathsf{R}\)}
\label{sec-5-5-1}

The empirical distribution is not directly available as a distribution
in the same way that the other base probability distributions are, but
there are plenty of resources available:

\begin{Verbatim}
x <- c(4, 7, 9, 11, 12)
mean(x)
mean((x-mean(x))^2)                 # empirical variance
mean(x^2)-mean(x)^2                 # empirical variance
var(x)                              # sample variance
sd(x)                               # sample standard deviation
\end{Verbatim}

\begin{verbatim}
[1] 8.6
[1] 8.24
[1] 8.24
[1] 10.3
[1] 3.209361
\end{verbatim}

One can use \texttt{sample} function for empirical simulation with the general syntax
\texttt{sample(x, size, replace = TRUE)}.
The argument \texttt{x} identifies the set from which to randomly
sample. If \texttt{x} is a number, then sampling is done from 1 to \texttt{x}. The
argument \texttt{size} tells how big the sample size should be, and \texttt{replace}
tells whether or not numbers should be replaced in the urn after
having been sampled. The default option is \texttt{replace = FALSE} but for
empirical simulation the sampled values should be replaced.

\begin{Verbatim}
# flip a fair coin 1000 times
sample(c("H","T"), size = 1000, replace = TRUE)

# roll a fair die 3000 times
sample(6, size = 3000, replace = TRUE)

# choose 27 random numbers from 30 to 70
sample(30:70, size = 27, replace = TRUE)
\end{Verbatim}

\section{Other Discrete Distributions}
\label{sec-5-6}

The binomial and discrete uniform distributions are popular, and
rightly so; they are simple and form the foundation for many other
more complicated distributions. But the particular uniform and
binomial models only apply to a limited range of problems. In this
section we introduce situations for which we need more than what the
uniform and binomial offer.

\subsection{Waiting Time Distributions}
\label{sec-5-6-2}

An important class of problems is associated with the amount of
time it takes for a specified event of interest to occur. For example,
we could flip a coin repeatedly until we observe Heads. We could toss
a piece of paper repeatedly until we make it in the trash can.

\subsubsection{The Geometric Distribution}
\label{sec-5-6-2-1}

Suppose that we conduct Bernoulli trials repeatedly, noting the
successes and failures. Let \(X\) be the number of failures before a
success. If \(\mathbb{P}(S)=p\) then \(X\) has PMF
\begin{equation}
f_{X}(x)=p(1-p)^{x},\quad x=0,1,2,\ldots
\end{equation}
(Why?) We say that \(X\) has a \emph{Geometric distribution} and we write
\(X\sim\mathsf{geom}(\mathtt{prob}=p)\). The associated \(\mathsf{R}\)
functions are \texttt{dgeom(x, prob)}, \texttt{pgeom}, \texttt{qgeom}, and \texttt{rhyper}, which
give the PMF, CDF, quantile function, and simulate random variates,
respectively.

Again it is clear that \(f(x)\geq0\) and we check that \(\sum f(x)=1\)
:
\begin{alignat*}{1}
\sum_{x=0}^{\infty}p(1-p)^{x}= & p\sum_{x=0}^{\infty}q^{x}=p\,\frac{1}{1-q}=1.
\end{alignat*}
Similarly, we can find that the mean and variance are
\begin{equation}
\mu=\frac{1-p}{p}=\frac{q}{p}\mbox{ and }\sigma^{2}=\frac{q}{p^{2}}.
\end{equation}


\emph{Problem}:
The Pittsburgh Steelers place kicker, Jeff Reed, made 81.2\% of his
attempted field goals in his career up to 2006. Assuming that his
successive field goal attempts are approximately Bernoulli trials,
find the probability that Jeff misses at least 5 field goals before
his first successful goal.

\emph{Solution}: If \(X=\) the number of missed goals until Jeff's first
success, then \(X\sim\mathsf{geom}(\mathtt{prob}=0.812)\) and we want
\(\mathbb{P}(X\geq5)=\mathbb{P}(X>4)\). We can find this in
\(\mathsf{R}\) with

\begin{Verbatim}
pgeom(4, prob = 0.812, lower.tail = FALSE)
\end{Verbatim}

\begin{verbatim}
: [1] 0.0002348493
\end{verbatim}



\begin{note}
Some books use a slightly different definition of the geometric
distribution. They consider Bernoulli trials and let \(Y\) count
instead the number of trials until a success, so that \(Y\) has PMF
\begin{equation}
f_{Y}(y)=p(1-p)^{y-1},\quad y=1,2,3,\ldots
\end{equation}
When they say "geometric distribution", this is what they mean. It
is not hard to see that the two definitions are related. In fact, if
\(X\) denotes our geometric and \(Y\) theirs, then
\(Y=X+1\). Consequently, they have \(\mu_{Y}=\mu_{X}+1\) and
\(\sigma_{Y}^{2}=\sigma_{X}^{2}\).
\end{note}

\subsubsection{The Negative Binomial Distribution}
\label{sec-5-6-2-2}

We may generalize the problem and consider the case where we wait for
\emph{more} than one success. Suppose that we conduct Bernoulli trials
repeatedly, noting the respective successes and failures. Let \(X\)
count the number of failures before \(r\) successes. If
\(\mathbb{P}(S)=p\) then \(X\) has PMF
\begin{equation}
f_{X}(x)={r+x-1 \choose r-1}\, p^{r}(1-p)^{x},\quad x=0,1,2,\ldots
\end{equation}

We say that \(X\) has a \emph{Negative Binomial distribution} and write
\(X\sim\mathsf{nbinom}(\mathtt{size}=r,\,\mathtt{prob}=p)\). The
associated \(\mathsf{R}\) functions are \texttt{dnbinom(x, size, prob)},
\texttt{pnbinom}, \texttt{qnbinom}, and \texttt{rnbinom}, which give the PMF, CDF, quantile
function, and simulate random variates, respectively.

As usual it should be clear that \(f_{X}(x)\geq 0\) and the fact that
\(\sum f_{X}(x)=1\) follows from a generalization of the geometric
series by means of a Maclaurin's series expansion:
\begin{alignat}{1}
\frac{1}{1-t}= & \sum_{k=0}^{\infty}t^{k},\quad \mbox{for \(-1 < t < 1\)},\mbox{ and}\\
\frac{1}{(1-t)^{r}}= & \sum_{k=0}^{\infty}{r+k-1 \choose r-1}\, t^{k},\quad \mbox{for \(-1 < t < 1\)}.
\end{alignat}
Therefore
\begin{equation}
\sum_{x=0}^{\infty}f_{X}(x)=p^{r}\sum_{x=0}^{\infty}{r+x-1 \choose r-1}\, q^{x}=p^{r}(1-q)^{-r}=1,
\end{equation}
since \(|q|=|1-p|<1\). 

Similarly, we can find that the mean and variance are
\begin{equation}
\mu=r\frac{1-p}{p}=r\frac{q}{p}\mbox{ and }\sigma^{2}=r\frac{q}{p^{2}}.
\end{equation}

\emph{Problem}:
We flip a coin repeatedly and let \(X\) count the number of Tails
until we get seven Heads. What is \(\mathbb{P}(X=5)?\)

\emph{Solution}: We
know that
\(X\sim\mathsf{nbinom}(\mathtt{size}=7,\,\mathtt{prob}=1/2)\).  
\[
\mathbb{P}(X=5)=f_{X}(5)={7+5-1 \choose 7-1}(1/2)^{7}(1/2)^{5}={11
\choose 6}2^{-12} 
\]
and we can get this in \(\mathsf{R}\) with

\begin{Verbatim}
dnbinom(5, size = 7, prob = 0.5)
\end{Verbatim}

\begin{verbatim}
: [1] 0.112793
\end{verbatim}

\begin{note}
As with the Geometric distribution, some books use a slightly
different definition of the Negative Binomial distribution. They
consider Bernoulli trials and let \(Y\) be the number of trials until
\(r\) successes, so that \(Y\) has PMF
\begin{equation}
f_{Y}(y)={y-1 \choose r-1}p^{r}(1-p)^{y-r},\quad y=r,r+1,r+2,\ldots
\end{equation}
It is again not hard to see that if \(X\) denotes our Negative
Binomial and \(Y\) theirs, then \(Y=X+r\). Consequently, they have
\(\mu_{Y}=\mu_{X}+r\) and \(\sigma_{Y}^{2}=\sigma_{X}^{2}\).
\end{note}

\subsection{Arrival Processes}
\label{sec-5-6-3}

\subsubsection{The Poisson Distribution}
\label{sec-5-6-3-1}

This is a distribution associated with "rare events", for reasons
which will become clear in a moment. The events might be:
traffic accidents,
typing errors, or
customers arriving in a bank.

Let \(\lambda\) be the average number of events in the time interval
\([0,1]\). Let the random variable \(X\) count the number of events
occurring in the interval. Then under certain reasonable conditions it
can be shown that
\begin{equation}
f_{X}(x)=\mathbb{P}(X=x)=\mathrm{e}^{-\lambda}\frac{\lambda^{x}}{x!},\quad x=0,1,2,\ldots
\end{equation}
We use the notation
\(X\sim\mathsf{pois}(\mathtt{lambda}=\lambda)\). The associated
\(\mathsf{R}\) functions are \texttt{dpois(x, lambda)}, \texttt{ppois}, \texttt{qpois}, and
\texttt{rpois}, which give the PMF, CDF, quantile function, and simulate
random variates, respectively.

Both mean and variance of \(X\sim\mathsf{pois}(\mathtt{lambda}=\lambda)\) are $\lambda$.

\subsubsection{What are the reasonable conditions?}
\label{sec-5-6-3-2}

Divide \([0,1]\) into subintervals of length \(1/n\). A \emph{Poisson
process} \index{Poisson process} satisfies the following conditions:
\begin{itemize}
\item the probability of an event occurring in a particular subinterval is
\(\approx\lambda/n\).
\item the probability of two or more events occurring in any subinterval
is \(\approx 0\).
\item occurrences in disjoint subintervals are independent.
\end{itemize}

\begin{rem}
\label{rem-poisson-process} If \(X\) counts the number of events in the
interval \([0,t]\) and \(\lambda\) is the average number that occur in
unit time, then \(X\sim\mathsf{pois}(\mathtt{lambda}=\lambda t)\),
that is,
\begin{equation}
\mathbb{P}(X=x)=\mathrm{e}^{-\lambda t}\frac{(\lambda t)^{x}}{x!},\quad x=0,1,2,3\ldots
\end{equation}
\end{rem}

\emph{Problem}:
On the average, five cars arrive at a particular car wash every
hour. Let \(X\) count the number of cars that arrive from 10AM to
11AM. Then \(X\sim\mathsf{pois}(\mathtt{lambda}=5)\). Also,
\(\mu=\sigma^{2}=5\). What is the probability that no car arrives
during this period?

\emph{Solution}: The probability that no car arrives
is \[
\mathbb{P}(X=0)=\mathrm{e}^{-5}\frac{5^{0}}{0!}=\mathrm{e}^{-5}\approx0.0067.
\]



\emph{Problem}:
Suppose the car wash above is in operation from 8AM to 6PM, and we let
\(Y\) be the number of customers that appear in this period. Since
this period covers a total of 10 hours, from Remark \ref{rem-poisson-process} we
get that \(Y\sim\mathsf{pois}(\mathtt{lambda}=5\ast10=50)\). What is
the probability that there are between 48 and 50 customers, inclusive?

\emph{Solution}: We want \(\mathbb{P}(48\leq
Y\leq50)=\mathbb{P}(X\leq50)-\mathbb{P}(X\leq47)\).

\begin{Verbatim}
diff(ppois(c(47, 50), lambda = 50))
\end{Verbatim}

\begin{verbatim}
: [1] 0.1678485
\end{verbatim}

\section{Functions of Discrete Random Variables}
\label{sec-5-7}

We have built a large catalogue of discrete distributions, but the
tools of this section will give us the ability to consider infinitely
many more. Given a random variable \(X\) and a given function \(h\),
we may consider \(Y=h(X)\). Since the values of \(X\) are determined
by chance, so are the values of \(Y\). The question is, what is the
PMF of the random variable \(Y\)? The answer, of course, depends on
\(h\).


Let \(X\sim\mathsf{nbinom}(\mathtt{size}=r,\,\mathtt{prob}=p)\). We
saw in \ref{sec-5-6} that \(X\) represents the number
of failures until \(r\) successes in a sequence of Bernoulli
trials. Suppose now that instead we were interested in counting the
number of trials (successes and failures) until the
\(r^{\mathrm{th}}\) success occurs, which we will denote by \(Y\). In
a given performance of the experiment, the number of failures (\(X\))
and the number of successes (\(r\)) together will comprise the total
number of trials (\(Y\)), or in other words, \(X+r=Y\). We may let
\(h\) be defined by \(h(x)=x+r\) so that \(Y=h(X)\), and we notice
that \(h\) is linear and hence one-to-one. Finally, \(X\) takes values
\(0,\ 1,\ 2,\ldots\) implying that the support of \(Y\) would be \(\{
r,\ r+1,\ r+2,\ldots \}\). Solving for \(X\) we get
\(X=Y-r\). Examining the PMF of \(X\)
\begin{equation}
f_{X}(x)={r+x-1 \choose r-1}\, p^{r}(1-p)^{x},
\end{equation}
we can substitute \( x = y - r \) to get
\begin{eqnarray*}
f_{Y}(y) & = & f_{X}(y-r),\\
 & = & {r+(y-r)-1 \choose r-1}\, p^{r}(1-p)^{y-r},\\
 & = & {y-1 \choose r-1}\, p^{r}(1-p)^{y-r},\quad y=r,\, r+1,\ldots
\end{eqnarray*}

Even when the function \(h\) is not one-to-one, we may still find the
PMF of \(Y\) simply by accumulating, for each \(y\), the probability
of all the \(x\)'s that are mapped to that \(y\).
\begin{prop}
Let \(X\) be a discrete random variable with PMF \(f_{X}\) supported
on the set \(S_{X}\). Let \(Y=h(X)\) for some function \(h\). Then
\(Y\) has PMF \(f_{Y}\) defined by
\begin{equation}
f_{Y}(y)=\sum_{\{x\in S_{X}|\, h(x)=y\}}f_{X}(x)
\end{equation}
\end{prop}


Let \(X\sim\mathsf{binom}(\mathtt{size}=4,\,\mathtt{prob}=1/2)\), and let \(Y=(X-1)^{2}\). Consider the following table:

\begin{table}[htb]
\caption[Transform discrete random variable]{\label{tab-disc-transf}Transforming a discrete random variable.}
\centering
\begin{tabular}{lrrrrr}
x & 0 & 1 & 2 & 3 & 4\\
\hline
\(f_{X}(x)\) & 1/16 & 1/4 & 6/16 & 1/4 & 1/16\\
\hline
\(y=(x-1)^{2}\) & 1 & 0 & 1 & 4 & 9\\
\end{tabular}
\end{table}

From this we see that \(Y\) has support \(S_{Y}=\{0,1,4,9\}\). We also
see that \(h(x)=(x-1)^{2}\) is not one-to-one on the support of \(X\),
because both \(x=0\) and \(x=2\) are mapped by \(h\) to
\(y=1\). Nevertheless, we see that \(Y=0\) only when \(X=1\), which
has probability \(1/4\); therefore, \(f_{Y}(0)\) should equal
\(1/4\). A similar approach works for \(y=4\) and \(y=9\). And \(Y=1\)
exactly when \(X=0\) or \(X=2\), which has total probability
\(7/16\). In summary, the PMF of \(Y\) may be written:

\begin{table}[htb]
\caption[Transforming discrete random variable: PMF]{\label{tab-disc-transf-pmf}Transforming a discrete random variable, its PMF.}
\centering
\begin{tabular}{lrrrr}
y & 0 & 1 & 4 & 9\\
\hline
\(f_{Y}(y)\) & 1/4 & 7/16 & 1/4 & 1/16\\
\end{tabular}
\end{table}

There is not a special name for the distribution of \(Y\), it is just
an example of what to do when the transformation of a random variable
is not one-to-one. The method is the same for more complicated
problems.


\begin{prop}
If \(X\) is a random variable with \(\mathbb{E} X=\mu\) and \(\mbox{Var}(X)=\sigma^{2}\), then the mean and variance of \(Y=mX+b\) is
\begin{equation}
\mu_{Y}=m\mu+b,\quad \sigma_{Y}^{2}=m^{2}\sigma^{2},\quad \sigma_{Y}=|m|\sigma.
\end{equation}
\end{prop}

\newpage{}

\section{Exercises}
\label{sec-5-8}
\setcounter{thm}{0}

\begin{xca}
A recent national study showed that approximately 44.7\% of college
students have used Wikipedia as a source in at least one of their term
papers. Let \(X\) equal the number of students in a random sample of
size \(n=31\) who have used Wikipedia as a source.
\begin{itemize}
\item How is \(X\) distributed?
\item Sketch the probability mass function (roughly).
\item Sketch the cumulative distribution function (roughly).
\item Find the probability that \(X\) is equal to 17.
\item Find the probability that \(X\) is at most 13.
\item Find the probability that \(X\) is bigger than 11.
\item Find the probability that \(X\) is at least 15.
\item Find the probability that \(X\) is between 16 and 19, inclusive.
\item Give the mean of \(X\), denoted \(\mathbb{E} X\).
\item Give the variance of \(X\).
\item Give the standard deviation of \(X\).
\item Find \(\mathbb{E}(4X + 51.324)\).
\end{itemize}
\end{xca}

\begin{xca}
For the following situations, decide what the distribution of \(X\)
should be. In nearly every case, there are additional assumptions that
should be made for the distribution to apply; identify those
assumptions (which may or may not hold in practice.)
\begin{itemize}
\item We shoot basketballs at a basketball hoop, and count the number of
shots until we make a goal. Let \(X\) denote the number of missed
shots. On a normal day we would typically make about 37\% of the
shots.
\item We drop a Styrofoam cup to the floor twenty times, each time
recording whether the cup comes to rest perfectly right side up, or
not. Let \(X\) be the number of times the cup lands perfectly right
side up.
\item We toss a piece of trash at the garbage can from across the room. If
we miss the trash can, we retrieve the trash and try again,
continuing to toss until we make the shot. Let \(X\) denote the
number of missed shots.
\item We count the number of pats on a baby's back until (s)he burps.
\end{itemize}
\end{xca}
